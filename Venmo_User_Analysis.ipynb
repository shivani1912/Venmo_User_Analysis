{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Venmo_User_Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shivani1912/Venmo_User_Analysis/blob/master/Venmo_User_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzGbKpvN_agJ",
        "colab_type": "text"
      },
      "source": [
        "## Background\n",
        "Venmo is a peer-to-peer (P2P) mobile payment app owned by PayPal. The Venmo app allows its users to exchange money with just a click of a button. In the fourth quarter of 2019, Venmo’s net payment volume amounted to 29 billion U.S. dollars, representing a 56 percent year-on-year growth, and its user-base had more than 40 million active accounts . 1 What made Venmo so popular in the US is its social flavor; users are required to accompany their transactions with a message describing what the transaction was about. This social twist has allowed Venmo to\n",
        "transform financial transactions into sharing experiences.\n",
        "\n",
        "## Objective\n",
        "In this project, we analyze the user's social network and spending behaviors to predict the total transactions transacted by the user in the first year akin to Customer Lifetime Value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1B3Rxv2R_hmt",
        "colab_type": "text"
      },
      "source": [
        "# Text Analytics\n",
        "Venmo data offers a unique opportunity to see what people are spending their money on. However, the most challenging part in figuring out such a thing is to have the right “lexicon” (most of them are proprietary and for a reason, as you will hopefully realise). Venmo messages can be 1) emoji, 2) text and 3) a combination of emoji and text.\n",
        "\n",
        "For emoji, we have an emoji classification dictionary that includes all emoji and their categories (By scraping emojipedia.com).\n",
        "\n",
        "Text, however, is harder to classify. We have created a word classification dictionary.\n",
        " In this dictionary, words are divided into 9 topics: 1) People (usually, words that indicate emotions towards others), 2) Food, 3) Event, 4) Activity, 5) Travel (note that Travel is a subset of Activity, but in my opinion required to be a different topic), 6) Transportation, 7) Utility, 8) Cash and 9) Illegal/Sarcasm (as you will notice, there are many bad words in there, and it’s hard to distinguish if they are used as sarcasm or if they are used for illegal activities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zlZwUPuak0d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3e1714e4-4832-4952-9711-2fe9952cecd3"
      },
      "source": [
        "# Install all neccessary packages for our analysis\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!pip install -q findspark\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.0.0-preview2/spark-3.0.0-preview2-bin-hadoop2.7.tgz\n",
        "!tar xvf spark-3.0.0-preview2-bin-hadoop2.7.tgz\n",
        "!pip install emoji"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "spark-3.0.0-preview2-bin-hadoop2.7/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/data/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/data/streaming/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/data/streaming/AFINN-111.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/data/mllib/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/data/mllib/sample_binary_classification_data.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/data/mllib/sample_kmeans_data.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/data/mllib/sample_multiclass_classification_data.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/data/mllib/sample_lda_libsvm_data.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/data/mllib/iris_libsvm.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/data/mllib/pagerank_data.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/data/mllib/sample_linear_regression_data.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/data/mllib/pic_data.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/data/mllib/als/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/data/mllib/als/test.data\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/data/mllib/als/sample_movielens_ratings.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/data/mllib/ridge-data/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/data/mllib/ridge-data/lpsa.data\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/data/mllib/sample_movielens_data.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/data/mllib/sample_isotonic_regression_libsvm_data.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/data/mllib/sample_svm_data.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/data/mllib/kmeans_data.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/data/mllib/sample_libsvm_data.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/data/mllib/images/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/data/mllib/images/origin/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/data/mllib/images/origin/multi-channel/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/data/mllib/images/origin/multi-channel/grayscale.jpg\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/data/mllib/images/origin/multi-channel/BGRA.png\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/data/mllib/images/origin/multi-channel/chr30.4.184.jpg\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/data/mllib/images/origin/multi-channel/BGRA_alpha_60.png\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/data/mllib/images/origin/kittens/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/data/mllib/images/origin/kittens/not-image.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/data/mllib/images/origin/kittens/54893.jpg\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/data/mllib/images/origin/kittens/DP153539.jpg\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/data/mllib/images/origin/kittens/29.5.a_b_EGDP022204.jpg\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/data/mllib/images/origin/kittens/DP802813.jpg\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/data/mllib/images/origin/license.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/data/mllib/images/partitioned/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/data/mllib/images/partitioned/cls=multichannel/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/data/mllib/images/partitioned/cls=multichannel/date=2018-01/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/data/mllib/images/partitioned/cls=multichannel/date=2018-01/BGRA.png\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/data/mllib/images/partitioned/cls=multichannel/date=2018-01/BGRA_alpha_60.png\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/data/mllib/images/partitioned/cls=multichannel/date=2018-02/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/data/mllib/images/partitioned/cls=multichannel/date=2018-02/grayscale.jpg\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/data/mllib/images/partitioned/cls=multichannel/date=2018-02/chr30.4.184.jpg\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/data/mllib/images/partitioned/cls=kittens/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/data/mllib/images/partitioned/cls=kittens/date=2018-01/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/data/mllib/images/partitioned/cls=kittens/date=2018-01/not-image.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/data/mllib/images/partitioned/cls=kittens/date=2018-01/29.5.a_b_EGDP022204.jpg\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/data/mllib/images/partitioned/cls=kittens/date=2018-02/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/data/mllib/images/partitioned/cls=kittens/date=2018-02/54893.jpg\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/data/mllib/images/partitioned/cls=kittens/date=2018-02/DP153539.jpg\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/data/mllib/images/partitioned/cls=kittens/date=2018-02/DP802813.jpg\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/data/mllib/images/license.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/data/mllib/streaming_kmeans_data_test.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/data/mllib/sample_lda_data.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/data/mllib/gmm_data.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/data/mllib/sample_fpgrowth.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/data/graphx/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/data/graphx/followers.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/data/graphx/users.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/licenses/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/licenses/LICENSE-javassist.html\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/licenses/LICENSE-protobuf.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/licenses/LICENSE-jakarta.xml.bind-api.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/licenses/LICENSE-dnsjava.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/licenses/LICENSE-CC0.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/licenses/LICENSE-leveldbjni.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/licenses/LICENSE-jakarta-ws-rs-api\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/licenses/LICENSE-re2j.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/licenses/LICENSE-vis.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/licenses/LICENSE-d3.min.js.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/licenses/LICENSE-dagre-d3.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/licenses/LICENSE-jaxb-runtime.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/licenses/LICENSE-matchMedia-polyfill.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/licenses/LICENSE-mustache.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/licenses/LICENSE-arpack.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/licenses/LICENSE-pmml-model.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/licenses/LICENSE-sorttable.js.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/licenses/LICENSE-pyrolite.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/licenses/LICENSE-javolution.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/licenses/LICENSE-javax-transaction-transaction-api.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/licenses/LICENSE-JTransforms.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/licenses/LICENSE-join.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/licenses/LICENSE-machinist.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/licenses/LICENSE-py4j.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/licenses/LICENSE-slf4j.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/licenses/LICENSE-AnchorJS.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/licenses/LICENSE-heapq.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/licenses/LICENSE-istack-commons-runtime.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/licenses/LICENSE-scopt.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/licenses/LICENSE-scala.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/licenses/LICENSE-jakarta.activation-api.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/licenses/LICENSE-netlib.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/licenses/LICENSE-json-formatter.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/licenses/LICENSE-jline.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/licenses/LICENSE-jquery.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/licenses/LICENSE-modernizr.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/licenses/LICENSE-jodd.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/licenses/LICENSE-jakarta-annotation-api\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/licenses/LICENSE-automaton.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/licenses/LICENSE-minlog.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/licenses/LICENSE-zstd-jni.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/licenses/LICENSE-antlr.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/licenses/LICENSE-spire.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/licenses/LICENSE-graphlib-dot.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/licenses/LICENSE-f2j.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/licenses/LICENSE-cloudpickle.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/licenses/LICENSE-reflectasm.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/licenses/LICENSE-janino.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/licenses/LICENSE-sbt-launch-lib.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/licenses/LICENSE-zstd.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/licenses/LICENSE-paranamer.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/licenses/LICENSE-datatables.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/licenses/LICENSE-jsp-api.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/licenses/LICENSE-bootstrap.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/licenses/LICENSE-JLargeArrays.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/licenses/LICENSE-kryo.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/licenses/LICENSE-xmlenc.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/licenses/LICENSE-respond.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/jars/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/jars/spark-examples_2.12-3.0.0-preview2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/jars/scopt_2.12-3.7.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/resources/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/resources/users.orc\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/resources/people.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/resources/employees.json\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/resources/people.json\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/resources/people.csv\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/resources/users.parquet\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/resources/kv1.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/resources/partitioned_users.orc/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/resources/partitioned_users.orc/favorite_color=red/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/resources/partitioned_users.orc/favorite_color=red/users.orc\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/resources/partitioned_users.orc/favorite_color=__HIVE_DEFAULT_PARTITION__/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/resources/partitioned_users.orc/favorite_color=__HIVE_DEFAULT_PARTITION__/users.orc\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/resources/partitioned_users.orc/do_not_read_this.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/resources/full_user.avsc\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/resources/user.avsc\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/resources/users.avro\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scripts/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scripts/getGpusResources.sh\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaKMeansExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaCorrelationExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaModelSelectionViaTrainValidationSplitExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaPrefixSpanExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorSizeHintExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaOneHotEncoderExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaQuantileDiscretizerExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaElementwiseProductExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaBucketedRandomProjectionLSHExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaGradientBoostedTreeRegressorExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaCountVectorizerExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaInteractionExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaRandomForestClassifierExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaDocument.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaOneVsRestExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaBucketizerExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLogisticRegressionSummaryExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaRandomForestRegressorExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLinearRegressionWithElasticNetExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorAssemblerExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaDecisionTreeClassificationExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMinMaxScalerExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMultilayerPerceptronClassifierExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMulticlassLogisticRegressionWithElasticNetExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaWord2VecExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaRFormulaExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaGaussianMixtureExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaGeneralizedLinearRegressionExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaChiSquareTestExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMaxAbsScalerExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaModelSelectionViaCrossValidationExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaRobustScalerExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaSQLTransformerExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaBinarizerExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLDAExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaTokenizerExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaFPGrowthExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaChiSqSelectorExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaTfIdfExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaPolynomialExpansionExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaALSExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaStandardScalerExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaPipelineExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaPCAExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaDecisionTreeRegressionExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLabeledDocument.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaNaiveBayesExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaBisectingKMeansExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMinHashLSHExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaSummarizerExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaNormalizerExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLinearSVCExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaStringIndexerExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaImputerExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorSlicerExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaFeatureHasherExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorIndexerExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaIsotonicRegressionExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaPowerIterationClusteringExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLogisticRegressionWithElasticNetExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaGradientBoostedTreeClassifierExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaIndexToStringExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaEstimatorTransformerParamExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaNGramExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaStopWordsRemoverExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaDCTExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaAFTSurvivalRegressionExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaLogQuery.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaNetworkWordCount.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaRecoverableNetworkWordCount.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaDirectKerberizedKafkaWordCount.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaRecord.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaDirectKafkaWordCount.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaStatefulNetworkWordCount.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaCustomReceiver.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaSqlNetworkWordCount.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaQueueStream.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaHdfsLR.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaStatusTrackerDemo.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaPageRank.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaSparkPi.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredKerberizedKafkaWordCount.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredSessionization.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredNetworkWordCountWindowed.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredNetworkWordCount.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredKafkaWordCount.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedTypedAggregation.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedUntypedAggregation.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/hive/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/hive/JavaSparkHiveExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaWordCount.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaKMeansExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaPrefixSpanExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaGradientBoostingRegressionExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaElementwiseProductExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaKernelDensityEstimationExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRankingMetricsExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaALS.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaHypothesisTestingExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaSummaryStatisticsExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaStratifiedSamplingExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaDecisionTreeClassificationExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRandomForestRegressionExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaLogisticRegressionWithLBFGSExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaMulticlassClassificationMetricsExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaSVMWithSGDExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaLBFGSExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaStreamingTestExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaGaussianMixtureExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaAssociationRulesExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRecommendationExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaMultiLabelClassificationMetricsExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaChiSqSelectorExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaPCAExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaDecisionTreeRegressionExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaSVDExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaLatentDirichletAllocationExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaNaiveBayesExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaBisectingKMeansExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaHypothesisTestingKolmogorovSmirnovTestExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRandomForestClassificationExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaSimpleFPGrowth.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaBinaryClassificationMetricsExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaGradientBoostingClassificationExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaIsotonicRegressionExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaCorrelationsExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaPowerIterationClusteringExample.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaTC.java\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/ml/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/ml/correlation_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/ml/word2vec_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/ml/gaussian_mixture_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/ml/multiclass_logistic_regression_with_elastic_net.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/ml/bisecting_k_means_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/ml/max_abs_scaler_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/ml/tf_idf_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/ml/index_to_string_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/ml/linearsvc.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/ml/random_forest_classifier_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/ml/train_validation_split.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/ml/chi_square_test_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/ml/vector_size_hint_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/ml/aft_survival_regression.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/ml/string_indexer_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/ml/estimator_transformer_param_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/ml/binarizer_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/ml/bucketed_random_projection_lsh_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/ml/multilayer_perceptron_classification.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/ml/stopwords_remover_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/ml/vector_indexer_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/ml/normalizer_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/ml/cross_validator.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/ml/logistic_regression_summary_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/ml/decision_tree_regression_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/ml/generalized_linear_regression_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/ml/random_forest_regressor_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/ml/dct_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/ml/vector_assembler_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/ml/elementwise_product_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/ml/lda_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/ml/linear_regression_with_elastic_net.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/ml/standard_scaler_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/ml/polynomial_expansion_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/ml/n_gram_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/ml/one_vs_rest_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/ml/bucketizer_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/ml/als_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/ml/isotonic_regression_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/ml/chisq_selector_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/ml/sql_transformer.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/ml/dataframe_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/ml/feature_hasher_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/ml/imputer_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/ml/tokenizer_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/ml/logistic_regression_with_elastic_net.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/ml/pipeline_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/ml/gradient_boosted_tree_regressor_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/ml/summarizer_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/ml/min_max_scaler_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/ml/gradient_boosted_tree_classifier_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/ml/quantile_discretizer_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/ml/power_iteration_clustering_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/ml/onehot_encoder_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/ml/rformula_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/ml/vector_slicer_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/ml/interaction_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/ml/pca_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/ml/count_vectorizer_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/ml/decision_tree_classification_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/ml/kmeans_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/ml/naive_bayes_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/ml/min_hash_lsh_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/ml/robust_scaler_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/ml/prefixspan_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/ml/fpgrowth_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/streaming/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/streaming/queue_stream.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/streaming/network_wordjoinsentiments.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/streaming/hdfs_wordcount.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/streaming/network_wordcount.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/streaming/sql_network_wordcount.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/streaming/recoverable_network_wordcount.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/streaming/stateful_network_wordcount.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/wordcount.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/transitive_closure.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/status_api_demo.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/kmeans.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/pi.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/sql/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/sql/streaming/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/sql/streaming/structured_network_wordcount.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/sql/streaming/structured_network_wordcount_windowed.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/sql/streaming/structured_kafka_wordcount.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/sql/hive.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/sql/basic.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/sql/arrow.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/sql/datasource.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/avro_inputformat.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/sort.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/pagerank.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/mllib/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/mllib/word2vec_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/mllib/multi_label_metrics_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/mllib/gaussian_mixture_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/mllib/bisecting_k_means_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/mllib/tf_idf_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/mllib/logistic_regression_with_lbfgs_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/mllib/hypothesis_testing_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/mllib/gradient_boosting_classification_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/mllib/regression_metrics_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/mllib/normalizer_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/mllib/decision_tree_regression_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/mllib/kmeans.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/mllib/hypothesis_testing_kolmogorov_smirnov_test_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/mllib/elementwise_product_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/mllib/random_rdd_generation.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/mllib/latent_dirichlet_allocation_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/mllib/word2vec.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/mllib/binary_classification_metrics_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/mllib/stratified_sampling_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/mllib/random_forest_regression_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/mllib/correlations.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/mllib/standard_scaler_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/mllib/correlations_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/mllib/svm_with_sgd_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/mllib/k_means_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/mllib/isotonic_regression_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/mllib/summary_statistics_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/mllib/streaming_linear_regression_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/mllib/recommendation_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/mllib/multi_class_metrics_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/mllib/streaming_k_means_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/mllib/gradient_boosting_regression_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/mllib/logistic_regression.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/mllib/pca_rowmatrix_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/mllib/power_iteration_clustering_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/mllib/linear_regression_with_sgd_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/mllib/sampled_rdds.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/mllib/random_forest_classification_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/mllib/svd_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/mllib/decision_tree_classification_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/mllib/naive_bayes_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/mllib/kernel_density_estimation_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/mllib/fpgrowth_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/mllib/gaussian_mixture_model.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/mllib/ranking_metrics_example.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/logistic_regression.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/parquet_inputformat.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/python/als.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkRemoteFileTest.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/PolynomialExpansionExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LDAExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/BisectingKMeansExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GBTExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/FeatureHasherExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/BucketizerExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/NormalizerExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/PCAExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/PowerIterationClusteringExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/StringIndexerExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/SummarizerExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/RFormulaExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/VectorAssemblerExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ImputerExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/CountVectorizerExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MinHashLSHExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DCTExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestRegressorExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/StopWordsRemoverExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ModelSelectionViaCrossValidationExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/PrefixSpanExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ChiSquareTestExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionSummaryExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MultilayerPerceptronClassifierExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/Word2VecExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/StandardScalerExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/BucketedRandomProjectionLSHExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GradientBoostedTreeClassifierExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/VectorSlicerExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/BinarizerExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/OneHotEncoderExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/InteractionExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/IsotonicRegressionExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/SQLTransformerExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/NaiveBayesExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/PipelineExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/TokenizerExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ALSExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/KMeansExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MulticlassLogisticRegressionWithElasticNetExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/OneVsRestExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DataFrameExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/AFTSurvivalRegressionExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestClassifierExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/VectorIndexerExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/EstimatorTransformerParamExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ElementwiseProductExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeClassificationExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/FPGrowthExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GaussianMixtureExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LinearSVCExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/NGramExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/VectorSizeHintExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionWithElasticNetExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/UnaryTransformerExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DeveloperApiExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/QuantileDiscretizerExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LinearRegressionExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/RobustScalerExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ChiSqSelectorExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MinMaxScalerExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MaxAbsScalerExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/TfIdfExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GradientBoostedTreeRegressorExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GeneralizedLinearRegressionExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ModelSelectionViaTrainValidationSplitExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/CorrelationExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeRegressionExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/IndexToStringExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LinearRegressionWithElasticNetExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/SqlNetworkWordCount.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/StreamingExamples.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/QueueStream.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/HdfsWordCount.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/PageViewStream.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/PageViewGenerator.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/RecoverableNetworkWordCount.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/CustomReceiver.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/RawNetworkGrep.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/DirectKafkaWordCount.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/StatefulNetworkWordCount.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/NetworkWordCount.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/DirectKerberizedKafkaWordCount.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/DriverSubmissionTest.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/DFSReadWriteTest.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalFileLR.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/MultiBroadcastTest.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SkewedGroupByTest.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/AccumulatorMetricsTest.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkTC.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LogQuery.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkPageRank.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkPi.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalKMeans.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/BroadcastTest.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalLR.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalPi.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredKafkaWordCount.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredSessionization.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredNetworkWordCount.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredNetworkWordCountWindowed.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredKerberizedKafkaWordCount.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedTypedAggregation.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/RDDRelation.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/hive/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/hive/SparkHiveExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedUntypedAggregation.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/SimpleTypedAggregator.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkHdfsLR.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LDAExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/BisectingKMeansExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/TallSkinnySVD.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SampledRDDs.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RandomRDDGeneration.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/NormalizerExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostingClassificationExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PowerIterationClusteringExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingTestExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/MultiLabelMetricsExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LogisticRegressionWithLBFGSExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/MultivariateSummarizer.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LBFGSExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/TFIDFExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RandomForestRegressionExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PCAOnSourceVectorExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SVDExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PrefixSpanExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RandomForestClassificationExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/Word2VecExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StandardScalerExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/MulticlassMetricsExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostingRegressionExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/BinaryClassification.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/BinaryClassificationMetricsExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/IsotonicRegressionExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PMMLModelExportExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/CosineSimilarity.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/DenseKMeans.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/NaiveBayesExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/TallSkinnyPCA.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/KMeansExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingLinearRegressionExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RankingMetricsExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/HypothesisTestingExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostedTreesRunner.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/HypothesisTestingKolmogorovSmirnovTestExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SimpleFPGrowth.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/AbstractParams.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SVMWithSGDExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/ElementwiseProductExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeClassificationExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/FPGrowthExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/GaussianMixtureExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingLogisticRegression.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RecommendationExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/ChiSqSelectorExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SummaryStatisticsExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SparseNaiveBayes.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/MovieLensALS.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PCAOnRowMatrixExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/Correlations.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/CorrelationsExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingKMeansExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/AssociationRulesExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StratifiedSamplingExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/KernelDensityEstimationExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeRunner.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeRegressionExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LatentDirichletAllocationExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/GroupByTest.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/pythonconverters/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/pythonconverters/AvroConverters.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/LiveJournalPageRank.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/Analytics.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/ConnectedComponentsExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/AggregateMessagesExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/SynthBenchmark.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/PageRankExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/SSSPExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/TriangleCountingExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/ComprehensiveExample.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkKMeans.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalALS.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkALS.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SimpleSkewedGroupByTest.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ExceptionHandlingTest.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/HdfsTest.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkLR.scala\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/r/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/r/ml/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/r/ml/bisectingKmeans.R\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/r/ml/fpm.R\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/r/ml/isoreg.R\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/r/ml/decisionTree.R\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/r/ml/powerIterationClustering.R\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/r/ml/naiveBayes.R\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/r/ml/prefixSpan.R\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/r/ml/gaussianMixture.R\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/r/ml/kmeans.R\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/r/ml/glm.R\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/r/ml/mlp.R\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/r/ml/lda.R\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/r/ml/survreg.R\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/r/ml/ml.R\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/r/ml/gbt.R\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/r/ml/svmLinear.R\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/r/ml/als.R\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/r/ml/logit.R\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/r/ml/kstest.R\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/r/ml/randomForest.R\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/r/streaming/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/r/streaming/structured_network_wordcount.R\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/r/data-manipulation.R\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/r/dataframe.R\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/examples/src/main/r/RSparkSQLExample.R\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/conf/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/conf/slaves.template\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/conf/spark-env.sh.template\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/conf/log4j.properties.template\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/conf/spark-defaults.conf.template\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/conf/fairscheduler.xml.template\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/conf/metrics.properties.template\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/datanucleus-core-4.1.17.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/jpam-1.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/guice-servlet-3.0.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/hive-shims-0.23-2.3.6.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/okapi-shade-0.4.2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/httpclient-4.5.6.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/jaxb-api-2.2.2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/avro-1.8.2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/hadoop-mapreduce-client-app-2.7.4.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/jsr305-3.0.0.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/slf4j-api-1.7.16.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/JTransforms-3.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/orc-core-1.5.8.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/spark-mllib-local_2.12-3.0.0-preview2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/jersey-server-2.29.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/scala-library-2.12.10.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/jackson-dataformat-yaml-2.10.0.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/parquet-common-1.10.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/zstd-jni-1.4.4-3.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/hive-serde-2.3.6.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/ST4-4.0.4.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/libthrift-0.12.0.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/spark-hive-thriftserver_2.12-3.0.0-preview2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/hive-shims-scheduler-2.3.6.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/javassist-3.22.0-CR2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/stax-api-1.0.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/json4s-ast_2.12-3.6.6.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/snappy-java-1.1.7.3.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/spark-graph_2.12-3.0.0-preview2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/hive-common-2.3.6.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/pyrolite-4.30.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/metrics-json-4.1.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/hive-exec-2.3.6-core.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/super-csv-2.2.0.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/json4s-scalap_2.12-3.6.6.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/jackson-databind-2.10.0.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/log4j-1.2.17.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/javolution-5.5.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/spire_2.12-0.17.0-M1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/spark-graphx_2.12-3.0.0-preview2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/zookeeper-3.4.14.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/velocity-1.5.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/jetty-sslengine-6.1.26.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/spark-tags_2.12-3.0.0-preview2-tests.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/jakarta.activation-api-1.2.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/spark-core_2.12-3.0.0-preview2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/scala-parser-combinators_2.12-1.1.2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/paranamer-2.8.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/jackson-jaxrs-1.9.13.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/api-util-1.0.0-M20.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/spark-unsafe_2.12-3.0.0-preview2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/commons-lang-2.6.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/spark-kubernetes_2.12-3.0.0-preview2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/joda-time-2.10.5.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/jackson-module-paranamer-2.10.0.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/mesos-1.4.0-shaded-protobuf.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/breeze_2.12-1.0.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/spark-sql_2.12-3.0.0-preview2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/metrics-jvm-4.1.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/jersey-media-jaxb-2.29.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/activation-1.1.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/spark-hive_2.12-3.0.0-preview2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/jersey-hk2-2.29.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/commons-net-3.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/xmlenc-0.52.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/spark-mesos_2.12-3.0.0-preview2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/avro-mapred-1.8.2-hadoop2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/transaction-api-1.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/spark-yarn_2.12-3.0.0-preview2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/scala-collection-compat_2.12-2.1.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/metrics-core-4.1.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/protobuf-java-2.5.0.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/py4j-0.10.8.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/commons-compress-1.8.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/curator-recipes-2.7.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/kubernetes-client-4.6.4.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/spark-sketch_2.12-3.0.0-preview2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/zjsonpatch-0.3.0.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/kubernetes-model-common-4.6.4.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/spire-macros_2.12-0.17.0-M1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/spark-mllib_2.12-3.0.0-preview2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/commons-compiler-3.0.15.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/univocity-parsers-2.8.3.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/json4s-jackson_2.12-3.6.6.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/arrow-vector-0.15.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/spark-catalyst_2.12-3.0.0-preview2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/okhttp-3.12.6.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/jersey-container-servlet-2.29.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/aopalliance-1.0.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/hk2-locator-2.6.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/jta-1.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/commons-crypto-1.0.0.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/hadoop-client-2.7.4.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/cats-kernel_2.12-2.0.0-M4.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/jetty-util-6.1.26.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/spire-platform_2.12-0.17.0-M1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/jcl-over-slf4j-1.7.16.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/hadoop-yarn-server-web-proxy-2.7.4.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/jaxb-runtime-2.3.2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/jetty-6.1.26.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/hadoop-mapreduce-client-common-2.7.4.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/json-1.8.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/osgi-resource-locator-1.0.3.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/hk2-utils-2.6.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/hadoop-yarn-common-2.7.4.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/algebra_2.12-2.0.0-M2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/jackson-core-asl-1.9.13.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/json4s-core_2.12-3.6.6.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/parquet-hadoop-1.10.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/jersey-common-2.29.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/chill-java-0.9.3.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/guice-3.0.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/stax-api-1.0-2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/hadoop-yarn-api-2.7.4.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/minlog-1.3.0.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/shims-0.7.45.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/hadoop-mapreduce-client-jobclient-2.7.4.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/commons-cli-1.2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/jackson-module-jaxb-annotations-2.10.0.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/jackson-xc-1.9.13.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/logging-interceptor-3.12.6.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/curator-framework-2.7.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/scala-reflect-2.12.10.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/scala-xml_2.12-1.2.0.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/api-asn1-api-1.0.0-M20.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/hadoop-auth-2.7.4.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/flatbuffers-java-1.9.0.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/commons-configuration-1.6.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/jackson-core-2.10.0.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/HikariCP-2.5.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/libfb303-0.9.3.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/audience-annotations-0.5.0.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/objenesis-2.5.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/arrow-memory-0.15.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/jakarta.validation-api-2.0.2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/spark-network-common_2.12-3.0.0-preview2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/commons-io-2.4.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/hk2-api-2.6.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/breeze-macros_2.12-1.0.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/hive-vector-code-gen-2.3.6.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/hive-cli-2.3.6.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/aopalliance-repackaged-2.6.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/hadoop-yarn-client-2.7.4.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/jackson-mapper-asl-1.9.13.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/commons-collections-3.2.2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/commons-digester-1.8.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/hive-service-rpc-2.3.6.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/hadoop-hdfs-2.7.4.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/ivy-2.4.0.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/hadoop-annotations-2.7.4.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/istack-commons-runtime-3.0.8.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/netty-all-4.1.42.Final.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/spire-util_2.12-0.17.0-M1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/compress-lzf-1.0.3.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/RoaringBitmap-0.7.45.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/javax.jdo-3.2.0-m3.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/hive-metastore-2.3.6.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/jul-to-slf4j-1.7.16.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/hadoop-common-2.7.4.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/spark-tags_2.12-3.0.0-preview2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/spark-graph-api_2.12-3.0.0-preview2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/parquet-format-2.4.0.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/httpcore-4.4.12.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/parquet-column-1.10.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/hadoop-mapreduce-client-shuffle-2.7.4.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/kubernetes-model-4.6.4.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/slf4j-log4j12-1.7.16.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/kryo-shaded-4.0.2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/antlr-runtime-3.5.2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/xercesImpl-2.9.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/metrics-jmx-4.1.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/machinist_2.12-0.6.8.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/hive-llap-common-2.3.6.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/okio-1.15.0.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/derby-10.12.1.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/jakarta.xml.bind-api-2.3.2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/spark-repl_2.12-3.0.0-preview2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/jackson-module-scala_2.12-2.10.0.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/bonecp-0.8.0.RELEASE.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/curator-client-2.7.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/snakeyaml-1.24.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/jersey-container-servlet-core-2.29.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/macro-compat_2.12-1.1.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/commons-math3-3.4.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/oro-2.0.8.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/lz4-java-1.7.0.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/orc-mapreduce-1.5.8.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/jackson-annotations-2.10.0.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/chill_2.12-0.9.3.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/jersey-client-2.29.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/opencsv-2.3.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/spark-kvstore_2.12-3.0.0-preview2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/scala-compiler-2.12.10.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/metrics-graphite-4.1.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/apacheds-kerberos-codec-2.0.0-M15.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/arrow-format-0.15.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/parquet-jackson-1.10.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/generex-1.0.2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/spark-streaming_2.12-3.0.0-preview2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/parquet-encoding-1.10.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/commons-pool-1.5.4.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/javax.servlet-api-3.1.0.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/avro-ipc-1.8.2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/jakarta.ws.rs-api-2.1.6.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/jsp-api-2.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/jakarta.annotation-api-1.3.5.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/stream-2.9.6.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/javax.inject-1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/JLargeArrays-1.5.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/xz-1.5.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/datanucleus-api-jdo-4.2.4.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/orc-shims-1.5.8.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/core-1.1.2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/commons-codec-1.10.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/commons-logging-1.1.3.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/aircompressor-0.10.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/shapeless_2.12-2.3.3.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/hadoop-yarn-server-common-2.7.4.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/jline-2.14.6.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/leveldbjni-all-1.8.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/hive-shims-2.3.6.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/commons-lang3-3.9.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/spark-network-shuffle_2.12-3.0.0-preview2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/xbean-asm7-shaded-4.15.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/jdo-api-3.0.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/spark-cypher_2.12-3.0.0-preview2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/hive-jdbc-2.3.6.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/commons-dbcp-1.4.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/gson-2.2.4.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/arpack_combined_all-0.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/janino-3.0.15.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/hive-shims-common-2.3.6.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/apacheds-i18n-2.0.0-M15.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/jodd-core-3.5.2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/antlr4-runtime-4.7.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/htrace-core-3.1.0-incubating.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/datanucleus-rdbms-4.1.19.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/hive-beeline-2.3.6.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/jakarta.inject-2.6.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/automaton-1.11-8.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/spark-launcher_2.12-3.0.0-preview2.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/guava-14.0.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/commons-text-1.6.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/hadoop-mapreduce-client-core-2.7.4.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/hive-storage-api-2.6.0.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/commons-beanutils-1.9.4.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/jars/commons-httpclient-3.1.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/NOTICE\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/.gitignore\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/heapq3.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/ml/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/ml/tuning.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/ml/clustering.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/ml/wrapper.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/ml/util.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/ml/tests/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/ml/tests/test_stat.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/ml/tests/test_persistence.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/ml/tests/__init__.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/ml/tests/test_linalg.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/ml/tests/test_training_summary.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/ml/tests/test_param.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/ml/tests/test_feature.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/ml/tests/test_image.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/ml/tests/test_evaluation.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/ml/tests/test_wrapper.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/ml/tests/test_pipeline.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/ml/tests/test_base.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/ml/tests/test_algorithms.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/ml/tests/test_tuning.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/ml/tree.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/ml/fpm.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/ml/__init__.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/ml/linalg/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/ml/linalg/__init__.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/ml/base.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/ml/common.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/ml/evaluation.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/ml/classification.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/ml/param/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/ml/param/shared.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/ml/param/__init__.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/ml/param/_shared_params_code_gen.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/ml/feature.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/ml/image.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/ml/pipeline.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/ml/recommendation.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/ml/stat.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/ml/regression.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/shuffle.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/streaming/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/streaming/util.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/streaming/context.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/streaming/tests/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/streaming/tests/test_dstream.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/streaming/tests/__init__.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/streaming/tests/test_context.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/streaming/tests/test_kinesis.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/streaming/tests/test_listener.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/streaming/kinesis.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/streaming/dstream.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/streaming/__init__.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/streaming/listener.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/util.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/rddsampler.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/context.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/rdd.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/tests/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/tests/test_serializers.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/tests/test_profiler.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/tests/test_pin_thread.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/tests/test_util.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/tests/__init__.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/tests/test_join.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/tests/test_taskcontext.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/tests/test_rdd.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/tests/test_appsubmit.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/tests/test_context.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/tests/test_broadcast.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/tests/test_rddbarrier.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/tests/test_readwrite.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/tests/test_daemon.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/tests/test_worker.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/tests/test_conf.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/tests/test_shuffle.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/_globals.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/resultiterable.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/__init__.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/resourceinformation.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/find_spark_home.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/version.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/profiler.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/cloudpickle.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/statcounter.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/status.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/python/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/python/pyspark/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/python/pyspark/shell.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/sql/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/sql/context.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/sql/functions.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/sql/tests/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/sql/tests/test_udf.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/sql/tests/test_pandas_udf_cogrouped_map.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/sql/tests/test_pandas_udf_window.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/sql/tests/__init__.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/sql/tests/test_arrow.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/sql/tests/test_dataframe.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/sql/tests/test_functions.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/sql/tests/test_pandas_udf.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/sql/tests/test_pandas_udf_iter.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/sql/tests/test_context.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/sql/tests/test_group.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/sql/tests/test_datasources.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/sql/tests/test_catalog.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/sql/tests/test_serde.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/sql/tests/test_readwriter.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/sql/tests/test_pandas_udf_scalar.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/sql/tests/test_types.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/sql/tests/test_streaming.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/sql/tests/test_conf.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/sql/tests/test_utils.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/sql/tests/test_pandas_udf_grouped_agg.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/sql/tests/test_session.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/sql/tests/test_column.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/sql/tests/test_pandas_udf_grouped_map.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/sql/dataframe.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/sql/group.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/sql/utils.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/sql/__init__.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/sql/types.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/sql/catalog.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/sql/streaming.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/sql/window.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/sql/avro/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/sql/avro/functions.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/sql/avro/__init__.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/sql/readwriter.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/sql/conf.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/sql/column.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/sql/udf.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/sql/session.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/sql/cogroup.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/mllib/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/mllib/clustering.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/mllib/util.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/mllib/tests/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/mllib/tests/test_stat.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/mllib/tests/test_util.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/mllib/tests/__init__.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/mllib/tests/test_linalg.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/mllib/tests/test_feature.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/mllib/tests/test_streaming_algorithms.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/mllib/tests/test_algorithms.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/mllib/tree.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/mllib/fpm.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/mllib/__init__.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/mllib/linalg/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/mllib/linalg/__init__.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/mllib/linalg/distributed.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/mllib/common.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/mllib/random.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/mllib/evaluation.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/mllib/classification.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/mllib/feature.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/mllib/recommendation.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/mllib/stat/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/mllib/stat/KernelDensity.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/mllib/stat/__init__.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/mllib/stat/test.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/mllib/stat/distribution.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/mllib/stat/_statistics.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/mllib/regression.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/taskcontext.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/serializers.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/conf.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/traceback_utils.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/storagelevel.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/worker.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/testing/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/testing/sqlutils.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/testing/streamingutils.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/testing/utils.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/testing/__init__.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/testing/mlutils.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/testing/mllibutils.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/java_gateway.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/broadcast.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/join.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/files.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/accumulators.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/daemon.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/shell.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/lib/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/lib/pyspark.zip\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/lib/py4j-0.10.8.1-src.zip\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/lib/PY4J_LICENSE.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/test_support/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/test_support/hello/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/test_support/hello/hello.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/test_support/hello/sub_hello/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/test_support/hello/sub_hello/sub_hello.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/test_support/userlib-0.1.zip\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/test_support/sql/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/test_support/sql/streaming/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/test_support/sql/streaming/text-test.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/test_support/sql/people.json\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/test_support/sql/people_array_utf16le.json\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/test_support/sql/ages.csv\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/test_support/sql/ages_newlines.csv\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/test_support/sql/people1.json\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/test_support/sql/people_array.json\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/test_support/sql/text-test.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/_SUCCESS\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/_metadata\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/month=9/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/month=9/day=1/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/month=9/day=1/part-r-00008.gz.parquet\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/month=9/day=1/.part-r-00008.gz.parquet.crc\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/_common_metadata\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/part-r-00002.gz.parquet\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/part-r-00004.gz.parquet\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/.part-r-00004.gz.parquet.crc\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/.part-r-00002.gz.parquet.crc\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=26/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=26/.part-r-00005.gz.parquet.crc\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=26/part-r-00005.gz.parquet\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=9/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=9/day=1/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=9/day=1/part-r-00007.gz.parquet\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=9/day=1/.part-r-00007.gz.parquet.crc\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/test_support/sql/orc_partitioned/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=1/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=1/c=1/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=1/c=1/.part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc.crc\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=1/c=1/part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/test_support/sql/orc_partitioned/_SUCCESS\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=0/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=0/c=0/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=0/c=0/.part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc.crc\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=0/c=0/part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/test_support/userlibrary.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/test_support/SimpleHTTPServer.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/test_coverage/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/test_coverage/conf/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/test_coverage/conf/spark-defaults.conf\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/test_coverage/sitecustomize.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/test_coverage/coverage_daemon.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark.egg-info/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark.egg-info/requires.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark.egg-info/SOURCES.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark.egg-info/dependency_links.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark.egg-info/PKG-INFO\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark.egg-info/top_level.txt\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/run-tests\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/setup.cfg\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/run-tests.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/pylintrc\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/run-tests-with-coverage\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/README.md\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/.coveragerc\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/docs/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/docs/_static/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/docs/_static/copybutton.js\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/docs/_static/pyspark.js\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/docs/_static/pyspark.css\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/docs/pyspark.ml.rst\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/docs/_templates/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/docs/_templates/layout.html\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/docs/pyspark.sql.rst\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/docs/make.bat\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/docs/Makefile\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/docs/conf.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/docs/make2.bat\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/docs/index.rst\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/docs/pyspark.mllib.rst\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/docs/pyspark.streaming.rst\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/docs/pyspark.rst\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/MANIFEST.in\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/setup.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/python/dist/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/bin/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/bin/spark-submit.cmd\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/bin/pyspark.cmd\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/bin/spark-sql\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/bin/beeline\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/bin/pyspark\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/bin/load-spark-env.sh\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/bin/load-spark-env.cmd\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/bin/spark-submit\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/bin/run-example.cmd\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/bin/find-spark-home.cmd\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/bin/spark-shell2.cmd\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/bin/docker-image-tool.sh\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/bin/sparkR2.cmd\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/bin/sparkR\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/bin/find-spark-home\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/bin/spark-shell.cmd\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/bin/spark-submit2.cmd\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/bin/spark-class2.cmd\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/bin/spark-sql2.cmd\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/bin/spark-class\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/bin/spark-sql.cmd\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/bin/pyspark2.cmd\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/bin/beeline.cmd\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/bin/run-example\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/bin/spark-shell\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/bin/spark-class.cmd\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/bin/sparkR.cmd\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/sbin/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/sbin/stop-slave.sh\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/sbin/start-history-server.sh\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/sbin/stop-slaves.sh\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/sbin/stop-history-server.sh\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/sbin/start-thriftserver.sh\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/sbin/start-mesos-shuffle-service.sh\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/sbin/start-master.sh\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/sbin/stop-master.sh\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/sbin/stop-mesos-dispatcher.sh\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/sbin/start-slave.sh\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/sbin/start-mesos-dispatcher.sh\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/sbin/start-slaves.sh\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/sbin/stop-mesos-shuffle-service.sh\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/sbin/stop-all.sh\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/sbin/slaves.sh\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/sbin/spark-daemons.sh\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/sbin/start-all.sh\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/sbin/spark-daemon.sh\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/sbin/stop-thriftserver.sh\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/sbin/spark-config.sh\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/RELEASE\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/README.md\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/yarn/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/yarn/spark-3.0.0-preview2-yarn-shuffle.jar\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/LICENSE\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/R/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/R/lib/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/R/lib/SparkR/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/R/lib/SparkR/tests/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/R/lib/SparkR/tests/testthat/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/R/lib/SparkR/tests/testthat/test_basic.R\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/R/lib/SparkR/INDEX\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/R/lib/SparkR/help/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/R/lib/SparkR/help/SparkR.rdb\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/R/lib/SparkR/help/SparkR.rdx\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/R/lib/SparkR/help/paths.rds\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/R/lib/SparkR/help/aliases.rds\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/R/lib/SparkR/help/AnIndex\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/R/lib/SparkR/worker/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/R/lib/SparkR/worker/daemon.R\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/R/lib/SparkR/worker/worker.R\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/R/lib/SparkR/html/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/R/lib/SparkR/html/00Index.html\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/R/lib/SparkR/html/R.css\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/R/lib/SparkR/Meta/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/R/lib/SparkR/Meta/hsearch.rds\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/R/lib/SparkR/Meta/package.rds\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/R/lib/SparkR/Meta/nsInfo.rds\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/R/lib/SparkR/Meta/links.rds\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/R/lib/SparkR/Meta/features.rds\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/R/lib/SparkR/Meta/vignette.rds\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/R/lib/SparkR/Meta/Rd.rds\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/R/lib/SparkR/NAMESPACE\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/R/lib/SparkR/profile/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/R/lib/SparkR/profile/general.R\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/R/lib/SparkR/profile/shell.R\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/R/lib/SparkR/DESCRIPTION\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/R/lib/SparkR/R/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/R/lib/SparkR/R/SparkR.rdb\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/R/lib/SparkR/R/SparkR\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/R/lib/SparkR/R/SparkR.rdx\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/R/lib/SparkR/doc/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/R/lib/SparkR/doc/sparkr-vignettes.R\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/R/lib/SparkR/doc/sparkr-vignettes.html\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/R/lib/SparkR/doc/sparkr-vignettes.Rmd\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/R/lib/SparkR/doc/index.html\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/R/lib/sparkr.zip\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/kubernetes/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/kubernetes/tests/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/kubernetes/tests/worker_memory_check.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/kubernetes/tests/py_container_checks.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/kubernetes/tests/pyfiles.py\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/kubernetes/dockerfiles/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/kubernetes/dockerfiles/spark/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/kubernetes/dockerfiles/spark/entrypoint.sh\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/kubernetes/dockerfiles/spark/bindings/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/kubernetes/dockerfiles/spark/bindings/python/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/kubernetes/dockerfiles/spark/bindings/python/Dockerfile\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/kubernetes/dockerfiles/spark/bindings/R/\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/kubernetes/dockerfiles/spark/bindings/R/Dockerfile\n",
            "spark-3.0.0-preview2-bin-hadoop2.7/kubernetes/dockerfiles/spark/Dockerfile\n",
            "Collecting emoji\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/8d/521be7f0091fe0f2ae690cc044faf43e3445e0ff33c574eae752dd7e39fa/emoji-0.5.4.tar.gz (43kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 1.9MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-0.5.4-cp36-none-any.whl size=42176 sha256=c044a4a3a0b30a1329cba7bc6b891bd25ef3c003b80a409d3863400bfa64f91d\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/a9/0a/4f8e8cce8074232aba240caca3fade315bb49fac68808d1a9c\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-0.5.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAO5THcU2VMk",
        "colab_type": "text"
      },
      "source": [
        "# Loading data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRebdpuRdDmg",
        "colab_type": "text"
      },
      "source": [
        "Installing spark & Importing all packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-QI4SxSYY2C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-preview2-bin-hadoop2.7\"\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1VSh_iqYfqW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "from google.colab import files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5-PFFQLBWq8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession, Window\n",
        "from google.colab import files\n",
        "from pyspark.sql import functions as f\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import ArrayType, StructType, StructField, StringType, IntegerType, MapType\n",
        "from emoji import UNICODE_EMOJI\n",
        "import emoji\n",
        "from collections import Counter\n",
        "from itertools import chain\n",
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StopWordsRemover"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSa2nukxZK_u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#upload_f=files.upload()\n",
        "# initialise sparkContext\n",
        "spark = SparkSession\\\n",
        "            .builder\\\n",
        "            .appName('Venmo Classification')\\\n",
        "            .getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xH5FJx4anu9z",
        "colab_type": "text"
      },
      "source": [
        "Mounting drive and loading transaction data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3asPgueb-k9i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "3789c3de-5cc8-4c08-e8bb-7fa65bd5b306"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0kRQguhq961",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For windows, change \"drive\" to \"gdrive\"\n",
        "path='/content/drive/My Drive/VenmoSample.snappy.parquet'\n",
        "input_parq= spark\\\n",
        "                  .read\\\n",
        "                  .option('header','true')\\\n",
        "                  .option('inferSchema','true')\\\n",
        "                  .parquet(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4Sn22zaYkT6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "28ed024f-229d-4d9e-947a-8ed25aab4db9"
      },
      "source": [
        "input_parq.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- user1: integer (nullable = true)\n",
            " |-- user2: integer (nullable = true)\n",
            " |-- transaction_type: string (nullable = true)\n",
            " |-- datetime: timestamp (nullable = true)\n",
            " |-- description: string (nullable = true)\n",
            " |-- is_business: boolean (nullable = true)\n",
            " |-- story_id: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChgXmkkFDede",
        "colab_type": "text"
      },
      "source": [
        "## Classifying Venmo’s transactions\n",
        "Use the text dictionary and the emoji dictionary to classify Venmo’s transactions in your sample dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AL8Qqwur1MG5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We will work with a subset of 50k rows\n",
        "input_parq = spark.createDataFrame(input_parq.take(50000))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxvkPeoF6ZzA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "outputId": "7cc256b2-f5e1-49f2-fa58-d15642414869"
      },
      "source": [
        "input_parq.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+-------+----------------+-------------------+--------------------+-----------+--------------------+\n",
            "|   user1|  user2|transaction_type|           datetime|         description|is_business|            story_id|\n",
            "+--------+-------+----------------+-------------------+--------------------+-----------+--------------------+\n",
            "| 1218774|1528945|         payment|2015-11-27 10:48:19|                Uber|      false|5657c473cd03c9af2...|\n",
            "| 5109483|4782303|         payment|2015-06-17 11:37:04|              Costco|      false|5580f9702b64f70ab...|\n",
            "| 4322148|3392963|         payment|2015-06-19 07:05:31|        Sweaty balls|      false|55835ccb1a624b14a...|\n",
            "|  469894|1333620|          charge|2016-06-03 23:34:13|                  🎥|      false|5751b185cd03c9af2...|\n",
            "| 2960727|3442373|         payment|2016-05-29 23:23:42|                   ⚡|      false|574b178ecd03c9af2...|\n",
            "| 3977544|2709470|         payment|2016-09-29 22:12:07|          Chipotlaid|      false|57ed2f4723e064eac...|\n",
            "| 3766386|4209061|         payment|2016-05-20 10:31:15|     kitchen counter|      false|573e8503cd03c9af2...|\n",
            "|  730075| 804466|         payment|2016-05-26 04:46:45|                Food|      false|57461d46cd03c9af2...|\n",
            "| 5221751|4993533|         payment|2016-07-14 22:53:49|               Zaxby|      false|5787b58d23e064eac...|\n",
            "| 6843582|7308338|         payment|2016-08-31 10:32:46|           Fan sucks|      false|57c64fdf23e064eac...|\n",
            "| 5317324|3942984|         payment|2016-01-04 09:11:25|                  👠|      false|5689c6bdcd03c9af2...|\n",
            "| 1134661|1556430|         payment|2015-10-09 01:53:52|         Thanks babe|      false|5616bbc0cd03c9af2...|\n",
            "| 4238868|4879587|         payment|2015-10-04 08:28:01|                  🍺|      false|561080a1cd03c9af2...|\n",
            "|11719500|8702716|         payment|2016-07-07 21:40:39|                   ⛽|      false|577e69e723e064eac...|\n",
            "| 3625798|5692302|         payment|2016-10-16 14:43:41|Hey man  it's bee...|      false|58032fad23e064eac...|\n",
            "|  613908|3045405|          charge|2016-05-07 06:42:17|         Getaway car|      false|572d2bd9cd03c9af2...|\n",
            "| 4682257|1870271|         payment|2016-02-24 09:14:12|     🔮 gypsy things|      false|56cd03e4cd03c9af2...|\n",
            "| 9414481|2869012|         payment|2016-04-09 09:19:46|                  🔴|      false|570866c2cd03c9af2...|\n",
            "|  241386|2580543|         payment|2015-05-17 06:00:19|           Furniture|      false|5557cc0407f81c33e...|\n",
            "|  656477| 656214|          charge|2013-12-14 22:43:27|bed bath mostly b...|      false|52ac6e93d56b6bac5...|\n",
            "+--------+-------+----------------+-------------------+--------------------+-----------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cd3QDW--bkxk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "dd086a3d-3bcb-48ed-aa9a-3cfc41660494"
      },
      "source": [
        "input_parq.select('story_id').distinct().count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7113104"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVy0PBsZgeaY",
        "colab_type": "text"
      },
      "source": [
        "## Cleaning description data\n",
        "Splitting & Tokenization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "js0ic-qUf0Sc",
        "colab_type": "text"
      },
      "source": [
        "Extracting all words to one column\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbYia9A2fiiY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentenceData=input_parq.withColumn('clean_text',regexp_replace('description',\"\\\\p{So}+\",''))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRaMHgS8gIYJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "d0d49d7f-7c5f-458b-8ee1-5fd98b48bdd9"
      },
      "source": [
        "sentenceData.select([c for c in sentenceData.columns if c in ['description','clean_text']]).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+\n",
            "|         description|          clean_text|\n",
            "+--------------------+--------------------+\n",
            "|                Uber|                Uber|\n",
            "|              Costco|              Costco|\n",
            "|        Sweaty balls|        Sweaty balls|\n",
            "|                  🎥|                    |\n",
            "|                   ⚡|                    |\n",
            "|          Chipotlaid|          Chipotlaid|\n",
            "|     kitchen counter|     kitchen counter|\n",
            "|                Food|                Food|\n",
            "|               Zaxby|               Zaxby|\n",
            "|           Fan sucks|           Fan sucks|\n",
            "|                  👠|                    |\n",
            "|         Thanks babe|         Thanks babe|\n",
            "|                  🍺|                    |\n",
            "|                   ⛽|                    |\n",
            "|Hey man  it's bee...|Hey man  it's bee...|\n",
            "|         Getaway car|         Getaway car|\n",
            "|     🔮 gypsy things|        gypsy things|\n",
            "|                  🔴|                    |\n",
            "|           Furniture|           Furniture|\n",
            "|bed bath mostly b...|bed bath mostly b...|\n",
            "+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uraRxxIjlCaK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = Tokenizer(inputCol=\"clean_text\", outputCol=\"words\")\n",
        "wordsData = tokenizer.transform(sentenceData)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPhNFpFclvNM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "be8f53e4-e236-4808-8a40-7388b4f73101"
      },
      "source": [
        "wordsData.show(15)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+-------+----------------+-------------------+--------------------+-----------+--------------------+--------------------+--------------------+\n",
            "|   user1|  user2|transaction_type|           datetime|         description|is_business|            story_id|          clean_text|               words|\n",
            "+--------+-------+----------------+-------------------+--------------------+-----------+--------------------+--------------------+--------------------+\n",
            "| 1218774|1528945|         payment|2015-11-27 10:48:19|                Uber|      false|5657c473cd03c9af2...|                Uber|              [uber]|\n",
            "| 5109483|4782303|         payment|2015-06-17 11:37:04|              Costco|      false|5580f9702b64f70ab...|              Costco|            [costco]|\n",
            "| 4322148|3392963|         payment|2015-06-19 07:05:31|        Sweaty balls|      false|55835ccb1a624b14a...|        Sweaty balls|     [sweaty, balls]|\n",
            "|  469894|1333620|          charge|2016-06-03 23:34:13|                  🎥|      false|5751b185cd03c9af2...|                    |                  []|\n",
            "| 2960727|3442373|         payment|2016-05-29 23:23:42|                   ⚡|      false|574b178ecd03c9af2...|                    |                  []|\n",
            "| 3977544|2709470|         payment|2016-09-29 22:12:07|          Chipotlaid|      false|57ed2f4723e064eac...|          Chipotlaid|        [chipotlaid]|\n",
            "| 3766386|4209061|         payment|2016-05-20 10:31:15|     kitchen counter|      false|573e8503cd03c9af2...|     kitchen counter|  [kitchen, counter]|\n",
            "|  730075| 804466|         payment|2016-05-26 04:46:45|                Food|      false|57461d46cd03c9af2...|                Food|              [food]|\n",
            "| 5221751|4993533|         payment|2016-07-14 22:53:49|               Zaxby|      false|5787b58d23e064eac...|               Zaxby|             [zaxby]|\n",
            "| 6843582|7308338|         payment|2016-08-31 10:32:46|           Fan sucks|      false|57c64fdf23e064eac...|           Fan sucks|        [fan, sucks]|\n",
            "| 5317324|3942984|         payment|2016-01-04 09:11:25|                  👠|      false|5689c6bdcd03c9af2...|                    |                  []|\n",
            "| 1134661|1556430|         payment|2015-10-09 01:53:52|         Thanks babe|      false|5616bbc0cd03c9af2...|         Thanks babe|      [thanks, babe]|\n",
            "| 4238868|4879587|         payment|2015-10-04 08:28:01|                  🍺|      false|561080a1cd03c9af2...|                    |                  []|\n",
            "|11719500|8702716|         payment|2016-07-07 21:40:39|                   ⛽|      false|577e69e723e064eac...|                    |                  []|\n",
            "| 3625798|5692302|         payment|2016-10-16 14:43:41|Hey man  it's bee...|      false|58032fad23e064eac...|Hey man  it's bee...|[hey, man, , it's...|\n",
            "+--------+-------+----------------+-------------------+--------------------+-----------+--------------------+--------------------+--------------------+\n",
            "only showing top 15 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2mg63BVgz4r",
        "colab_type": "text"
      },
      "source": [
        "Extracting all emojis to one column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtG67Fiug3wH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TokenizedData = wordsData.withColumn('Emojis',regexp_extract(col('description'),\"\\\\p{So}+\",0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMNShtIcgut1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "c45e079b-32cb-4c44-a5aa-616caff12c60"
      },
      "source": [
        "TokenizedData.show(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+-------+----------------+-------------------+---------------+-----------+--------------------+---------------+------------------+------+\n",
            "|  user1|  user2|transaction_type|           datetime|    description|is_business|            story_id|     clean_text|             words|Emojis|\n",
            "+-------+-------+----------------+-------------------+---------------+-----------+--------------------+---------------+------------------+------+\n",
            "|1218774|1528945|         payment|2015-11-27 10:48:19|           Uber|      false|5657c473cd03c9af2...|           Uber|            [uber]|      |\n",
            "|5109483|4782303|         payment|2015-06-17 11:37:04|         Costco|      false|5580f9702b64f70ab...|         Costco|          [costco]|      |\n",
            "|4322148|3392963|         payment|2015-06-19 07:05:31|   Sweaty balls|      false|55835ccb1a624b14a...|   Sweaty balls|   [sweaty, balls]|      |\n",
            "| 469894|1333620|          charge|2016-06-03 23:34:13|             🎥|      false|5751b185cd03c9af2...|               |                []|    🎥|\n",
            "|2960727|3442373|         payment|2016-05-29 23:23:42|              ⚡|      false|574b178ecd03c9af2...|               |                []|     ⚡|\n",
            "|3977544|2709470|         payment|2016-09-29 22:12:07|     Chipotlaid|      false|57ed2f4723e064eac...|     Chipotlaid|      [chipotlaid]|      |\n",
            "|3766386|4209061|         payment|2016-05-20 10:31:15|kitchen counter|      false|573e8503cd03c9af2...|kitchen counter|[kitchen, counter]|      |\n",
            "| 730075| 804466|         payment|2016-05-26 04:46:45|           Food|      false|57461d46cd03c9af2...|           Food|            [food]|      |\n",
            "|5221751|4993533|         payment|2016-07-14 22:53:49|          Zaxby|      false|5787b58d23e064eac...|          Zaxby|           [zaxby]|      |\n",
            "|6843582|7308338|         payment|2016-08-31 10:32:46|      Fan sucks|      false|57c64fdf23e064eac...|      Fan sucks|      [fan, sucks]|      |\n",
            "+-------+-------+----------------+-------------------+---------------+-----------+--------------------+---------------+------------------+------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heOcCQsEh-6I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create function to separate split emojis in a string to arrays\n",
        "def emoji_splitter(text):\n",
        "    new_string = \"\"\n",
        "    for char in text:\n",
        "        if char in emoji.UNICODE_EMOJI:\n",
        "            new_string += \" {} \".format(char)\n",
        "        else:\n",
        "            new_string += char\n",
        "    return [v for v in map(lambda x: x.strip(), new_string.split(\" \")) if v != \"\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMRUtPeFiANi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# UDF-ize for Spark\n",
        "cool_udf = udf(lambda z: emoji_splitter(z), StringType())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNycUI3ViIVd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a new column that splits the emojis\n",
        "TokenizedData= TokenizedData.withColumn(\"split_emojis\",cool_udf(TokenizedData.Emojis))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epreDpifmFUT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "4c32d306-fade-4a0d-8d5e-fff88b6df86e"
      },
      "source": [
        "TokenizedData.show(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+-------+----------------+-------------------+---------------+-----------+--------------------+---------------+------------------+------+------------+\n",
            "|  user1|  user2|transaction_type|           datetime|    description|is_business|            story_id|     clean_text|             words|Emojis|split_emojis|\n",
            "+-------+-------+----------------+-------------------+---------------+-----------+--------------------+---------------+------------------+------+------------+\n",
            "|1218774|1528945|         payment|2015-11-27 10:48:19|           Uber|      false|5657c473cd03c9af2...|           Uber|            [uber]|      |          []|\n",
            "|5109483|4782303|         payment|2015-06-17 11:37:04|         Costco|      false|5580f9702b64f70ab...|         Costco|          [costco]|      |          []|\n",
            "|4322148|3392963|         payment|2015-06-19 07:05:31|   Sweaty balls|      false|55835ccb1a624b14a...|   Sweaty balls|   [sweaty, balls]|      |          []|\n",
            "| 469894|1333620|          charge|2016-06-03 23:34:13|             🎥|      false|5751b185cd03c9af2...|               |                []|    🎥|        [🎥]|\n",
            "|2960727|3442373|         payment|2016-05-29 23:23:42|              ⚡|      false|574b178ecd03c9af2...|               |                []|     ⚡|         [⚡]|\n",
            "|3977544|2709470|         payment|2016-09-29 22:12:07|     Chipotlaid|      false|57ed2f4723e064eac...|     Chipotlaid|      [chipotlaid]|      |          []|\n",
            "|3766386|4209061|         payment|2016-05-20 10:31:15|kitchen counter|      false|573e8503cd03c9af2...|kitchen counter|[kitchen, counter]|      |          []|\n",
            "| 730075| 804466|         payment|2016-05-26 04:46:45|           Food|      false|57461d46cd03c9af2...|           Food|            [food]|      |          []|\n",
            "|5221751|4993533|         payment|2016-07-14 22:53:49|          Zaxby|      false|5787b58d23e064eac...|          Zaxby|           [zaxby]|      |          []|\n",
            "|6843582|7308338|         payment|2016-08-31 10:32:46|      Fan sucks|      false|57c64fdf23e064eac...|      Fan sucks|      [fan, sucks]|      |          []|\n",
            "+-------+-------+----------------+-------------------+---------------+-----------+--------------------+---------------+------------------+------+------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgA9pm85owKu",
        "colab_type": "text"
      },
      "source": [
        "Delete Stop words (StopWordsRemover)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6smwPgJoum3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "dca0fd8f-6107-49f4-c24e-7d9584301d18"
      },
      "source": [
        "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
        "wordsData=remover.transform(TokenizedData)\n",
        "wordsData.show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+-------+----------------+-------------------+------------+-----------+--------------------+------------+---------------+------+------------+---------------+\n",
            "|  user1|  user2|transaction_type|           datetime| description|is_business|            story_id|  clean_text|          words|Emojis|split_emojis|       filtered|\n",
            "+-------+-------+----------------+-------------------+------------+-----------+--------------------+------------+---------------+------+------------+---------------+\n",
            "|1218774|1528945|         payment|2015-11-27 10:48:19|        Uber|      false|5657c473cd03c9af2...|        Uber|         [uber]|      |          []|         [uber]|\n",
            "|5109483|4782303|         payment|2015-06-17 11:37:04|      Costco|      false|5580f9702b64f70ab...|      Costco|       [costco]|      |          []|       [costco]|\n",
            "|4322148|3392963|         payment|2015-06-19 07:05:31|Sweaty balls|      false|55835ccb1a624b14a...|Sweaty balls|[sweaty, balls]|      |          []|[sweaty, balls]|\n",
            "| 469894|1333620|          charge|2016-06-03 23:34:13|          🎥|      false|5751b185cd03c9af2...|            |             []|    🎥|        [🎥]|             []|\n",
            "|2960727|3442373|         payment|2016-05-29 23:23:42|           ⚡|      false|574b178ecd03c9af2...|            |             []|     ⚡|         [⚡]|             []|\n",
            "+-------+-------+----------------+-------------------+------------+-----------+--------------------+------------+---------------+------+------------+---------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuMrhdQX-a3s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For windows, change \"drive\" to \"gdrive\"\n",
        "dict_path='/content/drive/My Drive/Colab Notebooks/Bigdata_Venmo/Venmo_Emoji_Classification_Dictionary.csv'\n",
        "input_emoji= spark\\\n",
        "                  .read\\\n",
        "                  .option('header','true')\\\n",
        "                  .option('inferSchema','true')\\\n",
        "                  .csv(dict_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqvqCklTxi71",
        "colab_type": "text"
      },
      "source": [
        "## Uploading Emoji Dictionary\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9IXTZACMP9w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "emoji_df = pd.read_csv(dict_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDfQZDy8abJU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "outputId": "eff95cfd-1029-4002-ad71-305ad394ceac"
      },
      "source": [
        "emoji_df.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Event</th>\n",
              "      <th>Travel</th>\n",
              "      <th>Food</th>\n",
              "      <th>Activity</th>\n",
              "      <th>Transportation</th>\n",
              "      <th>People</th>\n",
              "      <th>Utility</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>🇦🇺</td>\n",
              "      <td>🏔</td>\n",
              "      <td>🍇</td>\n",
              "      <td>👾</td>\n",
              "      <td>🚄</td>\n",
              "      <td>😀</td>\n",
              "      <td>⚡</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>🇫🇷</td>\n",
              "      <td>⛰</td>\n",
              "      <td>🍈</td>\n",
              "      <td>🕴</td>\n",
              "      <td>🚅</td>\n",
              "      <td>😃</td>\n",
              "      <td>💡</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>🎂</td>\n",
              "      <td>🌋</td>\n",
              "      <td>🍉</td>\n",
              "      <td>🎪</td>\n",
              "      <td>🚆</td>\n",
              "      <td>😄</td>\n",
              "      <td>🔌</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>🛍</td>\n",
              "      <td>🗻</td>\n",
              "      <td>🍊</td>\n",
              "      <td>🎭</td>\n",
              "      <td>🚇</td>\n",
              "      <td>😁</td>\n",
              "      <td>📺</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>🇨🇦</td>\n",
              "      <td>🏕</td>\n",
              "      <td>🍋</td>\n",
              "      <td>🎨</td>\n",
              "      <td>🚈</td>\n",
              "      <td>😆</td>\n",
              "      <td>🔌</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>🇧🇷</td>\n",
              "      <td>🏖</td>\n",
              "      <td>🍌</td>\n",
              "      <td>🎰</td>\n",
              "      <td>🚉</td>\n",
              "      <td>😅</td>\n",
              "      <td>⚡</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>🐉</td>\n",
              "      <td>🏜</td>\n",
              "      <td>🍍</td>\n",
              "      <td>🚣</td>\n",
              "      <td>🚊</td>\n",
              "      <td>🤣</td>\n",
              "      <td>💡</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>🎅</td>\n",
              "      <td>🏝</td>\n",
              "      <td>🍎</td>\n",
              "      <td>🛀</td>\n",
              "      <td>🚝</td>\n",
              "      <td>😂</td>\n",
              "      <td>💸</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>🇲🇽</td>\n",
              "      <td>🏞</td>\n",
              "      <td>🍏</td>\n",
              "      <td>🎗</td>\n",
              "      <td>🚞</td>\n",
              "      <td>🙂</td>\n",
              "      <td>💦</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>🇨🇳</td>\n",
              "      <td>🏟</td>\n",
              "      <td>🍐</td>\n",
              "      <td>🎟</td>\n",
              "      <td>🚋</td>\n",
              "      <td>🙃-</td>\n",
              "      <td>💧</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Event Travel Food Activity Transportation People Utility\n",
              "0    🇦🇺      🏔    🍇        👾              🚄      😀       ⚡\n",
              "1    🇫🇷      ⛰    🍈        🕴              🚅      😃       💡\n",
              "2     🎂      🌋    🍉        🎪              🚆      😄       🔌\n",
              "3     🛍      🗻    🍊        🎭              🚇      😁       📺\n",
              "4    🇨🇦      🏕    🍋        🎨              🚈      😆       🔌\n",
              "5    🇧🇷      🏖    🍌        🎰              🚉      😅       ⚡\n",
              "6     🐉      🏜    🍍        🚣              🚊      🤣       💡\n",
              "7     🎅      🏝    🍎        🛀              🚝      😂       💸\n",
              "8    🇲🇽      🏞    🍏        🎗              🚞      🙂       💦\n",
              "9    🇨🇳      🏟    🍐        🎟              🚋     🙃-       💧"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9qKqmGy2SoQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "9d84583a-3286-4c55-91e1-2412674cfb82"
      },
      "source": [
        "Emoji_Dict_stacked=pd.DataFrame(emoji_df.stack())\n",
        "Emoji_Dict_stacked"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
              "      <th>Event</th>\n",
              "      <td>🇦🇺</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Travel</th>\n",
              "      <td>🏔</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Food</th>\n",
              "      <td>🍇</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Activity</th>\n",
              "      <td>👾</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Transportation</th>\n",
              "      <td>🚄</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>380</th>\n",
              "      <th>People</th>\n",
              "      <td>💼</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>381</th>\n",
              "      <th>People</th>\n",
              "      <td>💕</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>382</th>\n",
              "      <th>People</th>\n",
              "      <td>🌚</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>383</th>\n",
              "      <th>People</th>\n",
              "      <td>💜</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>384</th>\n",
              "      <th>People</th>\n",
              "      <td>💛</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>664 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                     0\n",
              "0   Event           🇦🇺\n",
              "    Travel           🏔\n",
              "    Food             🍇\n",
              "    Activity         👾\n",
              "    Transportation   🚄\n",
              "...                 ..\n",
              "380 People           💼\n",
              "381 People           💕\n",
              "382 People           🌚\n",
              "383 People           💜\n",
              "384 People           💛\n",
              "\n",
              "[664 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yCCNcmY2dJX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Emoji_Dict_stacked=Emoji_Dict_stacked.reset_index(level=[0])\n",
        "Emoji_Dict_stacked=Emoji_Dict_stacked.drop('level_0',axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkfHOAWL3jo1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Emoji_Dict_stacked['emoji_category']=Emoji_Dict_stacked.index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBJWtAkg3GO2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "fcf6be6f-90a8-4c22-bcde-5463c2df4186"
      },
      "source": [
        "Emoji_Dict_spark=spark.createDataFrame(Emoji_Dict_stacked.astype(str)) \n",
        "Emoji_Dict_spark=Emoji_Dict_spark.withColumnRenamed(\"0\", \"split_emojis\")\n",
        "Emoji_Dict_spark = Emoji_Dict_spark.withColumn('emoji_category', trim(Emoji_Dict_spark.emoji_category))\n",
        "\n",
        "Emoji_Dict_spark.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------+--------------+\n",
            "|split_emojis|emoji_category|\n",
            "+------------+--------------+\n",
            "|        🇦🇺|         Event|\n",
            "|          🏔|        Travel|\n",
            "|          🍇|          Food|\n",
            "|          👾|      Activity|\n",
            "|          🚄|Transportation|\n",
            "|          😀|        People|\n",
            "|           ⚡|       Utility|\n",
            "|        🇫🇷|         Event|\n",
            "|           ⛰|        Travel|\n",
            "|          🍈|          Food|\n",
            "|          🕴|      Activity|\n",
            "|          🚅|Transportation|\n",
            "|          😃|        People|\n",
            "|          💡|       Utility|\n",
            "|          🎂|         Event|\n",
            "|          🌋|        Travel|\n",
            "|          🍉|          Food|\n",
            "|          🎪|      Activity|\n",
            "|          🚆|Transportation|\n",
            "|          😄|        People|\n",
            "+------------+--------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRCCny2_xs7C",
        "colab_type": "text"
      },
      "source": [
        "### Uploading Word Dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5KemXXKJd07",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# change \"drive\" to \"gdrive\" for windows\n",
        "worddict_path='/content/drive/My Drive/Venmo Word Classification Dictonary BAX-423 - Word_Dict (1).csv'\n",
        "Word_Dict=pd.read_csv(worddict_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAxRUYOMazmN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "outputId": "26c76f15-ee67-4d72-c6f9-05115e50c501"
      },
      "source": [
        "Word_Dict.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>People</th>\n",
              "      <th>Food</th>\n",
              "      <th>Event</th>\n",
              "      <th>Activity</th>\n",
              "      <th>Travel</th>\n",
              "      <th>Transportation</th>\n",
              "      <th>Utility</th>\n",
              "      <th>Cash</th>\n",
              "      <th>Illegal/Sarcasm</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>friend</td>\n",
              "      <td>food</td>\n",
              "      <td>birthday</td>\n",
              "      <td>ball</td>\n",
              "      <td>beach</td>\n",
              "      <td>lyft</td>\n",
              "      <td>bill</td>\n",
              "      <td>atm</td>\n",
              "      <td>addiction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>friendship</td>\n",
              "      <td>bbq</td>\n",
              "      <td>christmas</td>\n",
              "      <td>boat</td>\n",
              "      <td>place</td>\n",
              "      <td>uber</td>\n",
              "      <td>cable</td>\n",
              "      <td>bank</td>\n",
              "      <td>drug</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>baby</td>\n",
              "      <td>bean</td>\n",
              "      <td>happy</td>\n",
              "      <td>bar</td>\n",
              "      <td>la</td>\n",
              "      <td>cab</td>\n",
              "      <td>fee</td>\n",
              "      <td>cash</td>\n",
              "      <td>wangs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>boy</td>\n",
              "      <td>latte</td>\n",
              "      <td>bday</td>\n",
              "      <td>book</td>\n",
              "      <td>world</td>\n",
              "      <td>bus</td>\n",
              "      <td>electric</td>\n",
              "      <td>money</td>\n",
              "      <td>weed</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>girl</td>\n",
              "      <td>breakfast</td>\n",
              "      <td>wedding</td>\n",
              "      <td>club</td>\n",
              "      <td>hotel</td>\n",
              "      <td>car</td>\n",
              "      <td>electricity</td>\n",
              "      <td>buck</td>\n",
              "      <td>anal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>help</td>\n",
              "      <td>brunch</td>\n",
              "      <td>xmas</td>\n",
              "      <td>card</td>\n",
              "      <td>trip</td>\n",
              "      <td>gas</td>\n",
              "      <td>internet</td>\n",
              "      <td>wallet</td>\n",
              "      <td>bj</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>like</td>\n",
              "      <td>burger</td>\n",
              "      <td>holiday</td>\n",
              "      <td>dance</td>\n",
              "      <td>vega</td>\n",
              "      <td>taxi</td>\n",
              "      <td>rent</td>\n",
              "      <td>monies</td>\n",
              "      <td>blowjob</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>love</td>\n",
              "      <td>burrito</td>\n",
              "      <td>hbd</td>\n",
              "      <td>football</td>\n",
              "      <td>tahoe</td>\n",
              "      <td>ride</td>\n",
              "      <td>wifi</td>\n",
              "      <td>tip</td>\n",
              "      <td>boob</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>mom</td>\n",
              "      <td>cake</td>\n",
              "      <td>halloween</td>\n",
              "      <td>fun</td>\n",
              "      <td>nyc</td>\n",
              "      <td>rental</td>\n",
              "      <td>utility</td>\n",
              "      <td>dollar</td>\n",
              "      <td>booty</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>save</td>\n",
              "      <td>cheese</td>\n",
              "      <td>thanksgiving</td>\n",
              "      <td>game</td>\n",
              "      <td>dc</td>\n",
              "      <td>train</td>\n",
              "      <td>tax</td>\n",
              "      <td>payback</td>\n",
              "      <td>blow</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      People       Food         Event   ...     Utility      Cash Illegal/Sarcasm \n",
              "0      friend       food      birthday  ...         bill     atm         addiction\n",
              "1  friendship        bbq     christmas  ...        cable    bank              drug\n",
              "2        baby       bean         happy  ...          fee    cash             wangs\n",
              "3         boy      latte          bday  ...     electric    money             weed\n",
              "4        girl  breakfast       wedding  ...  electricity     buck             anal\n",
              "5        help     brunch          xmas  ...     internet   wallet               bj\n",
              "6        like     burger       holiday  ...         rent   monies          blowjob\n",
              "7        love    burrito           hbd  ...         wifi      tip             boob\n",
              "8         mom       cake     halloween  ...      utility   dollar            booty\n",
              "9        save     cheese  thanksgiving  ...          tax  payback             blow\n",
              "\n",
              "[10 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frf8kvWBwOcl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Word_Dict_stacked=pd.DataFrame(Word_Dict.stack())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAkOwZjUxUFu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "6f614f6e-dd49-40de-d5eb-2b2b52126136"
      },
      "source": [
        "Word_Dict_stacked"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
              "      <th>People</th>\n",
              "      <td>friend</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Food</th>\n",
              "      <td>food</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Event</th>\n",
              "      <td>birthday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Activity</th>\n",
              "      <td>ball</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Travel</th>\n",
              "      <td>beach</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>259</th>\n",
              "      <th>Food</th>\n",
              "      <td>veg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>260</th>\n",
              "      <th>Food</th>\n",
              "      <td>takeout</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>261</th>\n",
              "      <th>Food</th>\n",
              "      <td>beer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>262</th>\n",
              "      <th>Food</th>\n",
              "      <td>Tan-cha</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>263</th>\n",
              "      <th>Food</th>\n",
              "      <td>pho</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>847 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                     0\n",
              "0   People      friend\n",
              "    Food          food\n",
              "    Event     birthday\n",
              "    Activity      ball\n",
              "    Travel       beach\n",
              "...                ...\n",
              "259 Food           veg\n",
              "260 Food       takeout\n",
              "261 Food          beer\n",
              "262 Food       Tan-cha\n",
              "263 Food           pho\n",
              "\n",
              "[847 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhaksSSxwqxh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Word_Dict_stacked=Word_Dict_stacked.reset_index(level=[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cC4xnhDr-9AX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "9c14b13d-5620-4efd-d933-53673c23fce7"
      },
      "source": [
        "Word_Dict_stacked"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>level_0</th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>People</th>\n",
              "      <td>0</td>\n",
              "      <td>friend</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Food</th>\n",
              "      <td>0</td>\n",
              "      <td>food</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Event</th>\n",
              "      <td>0</td>\n",
              "      <td>birthday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Activity</th>\n",
              "      <td>0</td>\n",
              "      <td>ball</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Travel</th>\n",
              "      <td>0</td>\n",
              "      <td>beach</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Food</th>\n",
              "      <td>259</td>\n",
              "      <td>veg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Food</th>\n",
              "      <td>260</td>\n",
              "      <td>takeout</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Food</th>\n",
              "      <td>261</td>\n",
              "      <td>beer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Food</th>\n",
              "      <td>262</td>\n",
              "      <td>Tan-cha</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Food</th>\n",
              "      <td>263</td>\n",
              "      <td>pho</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>847 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          level_0         0\n",
              "People          0    friend\n",
              "Food            0      food\n",
              "Event           0  birthday\n",
              "Activity        0      ball\n",
              "Travel          0     beach\n",
              "...           ...       ...\n",
              "Food          259       veg\n",
              "Food          260   takeout\n",
              "Food          261      beer\n",
              "Food          262   Tan-cha\n",
              "Food          263       pho\n",
              "\n",
              "[847 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyOqUS8Pxog_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Word_Dict_stacked=Word_Dict_stacked.drop('level_0',axis=1)\n",
        "Word_Dict_stacked['word_category']=Word_Dict_stacked.index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1SmkZvtpOZK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Word_Dict_spark=spark.createDataFrame(Word_Dict_stacked.astype(str)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HiNKPhJgrhi2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "ca48ffa1-a945-46b3-8925-280ab567468d"
      },
      "source": [
        "Word_Dict_spark=Word_Dict_spark.withColumnRenamed('0', \"words\")\n",
        "Word_Dict_spark = Word_Dict_spark.withColumn('word_category', trim(Word_Dict_spark.word_category))\n",
        "Word_Dict_spark.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+---------------+\n",
            "|     words|  word_category|\n",
            "+----------+---------------+\n",
            "|    friend|         People|\n",
            "|      food|           Food|\n",
            "|  birthday|          Event|\n",
            "|      ball|       Activity|\n",
            "|     beach|         Travel|\n",
            "|      lyft| Transportation|\n",
            "|      bill|        Utility|\n",
            "|      atm |           Cash|\n",
            "| addiction|Illegal/Sarcasm|\n",
            "|friendship|         People|\n",
            "|       bbq|           Food|\n",
            "| christmas|          Event|\n",
            "|      boat|       Activity|\n",
            "|     place|         Travel|\n",
            "|      uber| Transportation|\n",
            "|     cable|        Utility|\n",
            "|     bank |           Cash|\n",
            "|      drug|Illegal/Sarcasm|\n",
            "|      baby|         People|\n",
            "|      bean|           Food|\n",
            "+----------+---------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbDQEtqSeT8b",
        "colab_type": "text"
      },
      "source": [
        "Splitting words & emojis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1nmLIu0beDQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        },
        "outputId": "50aa23e7-a7b1-4c98-937d-e9c583478130"
      },
      "source": [
        "#Getting all the words in the lists of filtered column in separate rows\n",
        "df=wordsData\n",
        "df2 = df.select(df.story_id,explode(df.filtered))\n",
        "df2 = df2.withColumnRenamed(\"col\", \"words\")\n",
        "df2.printSchema()\n",
        "df2.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- story_id: string (nullable = true)\n",
            " |-- words: string (nullable = true)\n",
            "\n",
            "+--------------------+----------+\n",
            "|            story_id|     words|\n",
            "+--------------------+----------+\n",
            "|5657c473cd03c9af2...|      uber|\n",
            "|5580f9702b64f70ab...|    costco|\n",
            "|55835ccb1a624b14a...|    sweaty|\n",
            "|55835ccb1a624b14a...|     balls|\n",
            "|5751b185cd03c9af2...|          |\n",
            "|574b178ecd03c9af2...|          |\n",
            "|57ed2f4723e064eac...|chipotlaid|\n",
            "|573e8503cd03c9af2...|   kitchen|\n",
            "|573e8503cd03c9af2...|   counter|\n",
            "|57461d46cd03c9af2...|      food|\n",
            "|5787b58d23e064eac...|     zaxby|\n",
            "|57c64fdf23e064eac...|       fan|\n",
            "|57c64fdf23e064eac...|     sucks|\n",
            "|5689c6bdcd03c9af2...|          |\n",
            "|5616bbc0cd03c9af2...|    thanks|\n",
            "|5616bbc0cd03c9af2...|      babe|\n",
            "|561080a1cd03c9af2...|          |\n",
            "|577e69e723e064eac...|          |\n",
            "|58032fad23e064eac...|       hey|\n",
            "|58032fad23e064eac...|       man|\n",
            "+--------------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_k6LL9-kG6A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "7aecac08-b80e-4a6a-fac8-322e8e866555"
      },
      "source": [
        "#Getting all the eemojis in the lists of split-emojis column in separate rows\n",
        "df3=df.withColumn(\n",
        "    \"split_emojis\", \n",
        "    explode(split(regexp_replace(col(\"split_emojis\"), \"(^\\[)|(\\]$)\", \"\"), \", \")))\n",
        "df3=df3.select(df3.story_id,df3.split_emojis)\n",
        "\n",
        "df3.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+------------+\n",
            "|            story_id|split_emojis|\n",
            "+--------------------+------------+\n",
            "|5657c473cd03c9af2...|            |\n",
            "|5580f9702b64f70ab...|            |\n",
            "|55835ccb1a624b14a...|            |\n",
            "|5751b185cd03c9af2...|          🎥|\n",
            "|574b178ecd03c9af2...|           ⚡|\n",
            "|57ed2f4723e064eac...|            |\n",
            "|573e8503cd03c9af2...|            |\n",
            "|57461d46cd03c9af2...|            |\n",
            "|5787b58d23e064eac...|            |\n",
            "|57c64fdf23e064eac...|            |\n",
            "|5689c6bdcd03c9af2...|          👠|\n",
            "|5616bbc0cd03c9af2...|            |\n",
            "|561080a1cd03c9af2...|          🍺|\n",
            "|577e69e723e064eac...|           ⛽|\n",
            "|58032fad23e064eac...|            |\n",
            "|572d2bd9cd03c9af2...|            |\n",
            "|56cd03e4cd03c9af2...|          🔮|\n",
            "|570866c2cd03c9af2...|          🔴|\n",
            "|5557cc0407f81c33e...|            |\n",
            "|52ac6e93d56b6bac5...|            |\n",
            "+--------------------+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yg0QZUIi2CBi",
        "colab_type": "text"
      },
      "source": [
        "Mapping words to the category"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8ln9MaJx955",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "9fe38ff3-b987-4548-f26a-a6df24f1988a"
      },
      "source": [
        "Mapped_words = df2.join(Word_Dict_spark, on=['words'], how='left')\n",
        "Mapped_words.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+--------------------+-------------+\n",
            "|    words|            story_id|word_category|\n",
            "+---------+--------------------+-------------+\n",
            "| #loveyoy|56bc1478cd03c9af2...|         null|\n",
            "|(bachelor|56d73dbacd03c9af2...|         null|\n",
            "|      ...|5695c66bcd03c9af2...|         null|\n",
            "|      ...|538f845cd546b8434...|         null|\n",
            "|      ...|561852decd03c9af2...|         null|\n",
            "|      ...|5659f78acd03c9af2...|         null|\n",
            "|      ...|5546c65fd4aeb66e7...|         null|\n",
            "|      ...|5779fc6423e064eac...|         null|\n",
            "|      ...|5714cfcbcd03c9af2...|         null|\n",
            "|      ...|5607c821cd03c9af2...|         null|\n",
            "|      ...|57bdd2c123e064eac...|         null|\n",
            "|      ...|54d5292891bd05aa9...|         null|\n",
            "|      ...|571a83cccd03c9af2...|         null|\n",
            "|      ...|572e1f7ecd03c9af2...|         null|\n",
            "|      ...|53a9b7217d0b0354e...|         null|\n",
            "|      ...|5759a106cd03c9af2...|         null|\n",
            "|      ...|51a3da437de518fa3...|         null|\n",
            "|      ...|55954ad1b651bb523...|         null|\n",
            "|      ...|554067951a624b189...|         null|\n",
            "|      ...|56932a24cd03c9af2...|         null|\n",
            "+---------+--------------------+-------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVHAr39moeXm",
        "colab_type": "text"
      },
      "source": [
        "Mapping emojis to the category"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQwtc5Zv4UxA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "dae37e53-ca64-4d85-9cb0-9f1d54b1dafc"
      },
      "source": [
        "Mapped_emojis = df3.join(Emoji_Dict_spark, on=['split_emojis'], how='left')\n",
        "Mapped_emojis.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------+--------------------+--------------+\n",
            "|split_emojis|            story_id|emoji_category|\n",
            "+------------+--------------------+--------------+\n",
            "|          🌌|55ccfd6e583c04745...|        Travel|\n",
            "|          🌌|58138b4f23e064eac...|        Travel|\n",
            "|          🌌|54c7d44cca134c404...|        Travel|\n",
            "|          🌌|57c0692323e064eac...|        Travel|\n",
            "|          🌌|566764e3cd03c9af2...|        Travel|\n",
            "|          🌌|580a9f1823e064eac...|        Travel|\n",
            "|          🌟|57d3046523e064eac...|          null|\n",
            "|          🌟|555d79a71a624b0f6...|          null|\n",
            "|          🌟|55aad22a1a3b58701...|          null|\n",
            "|          🌟|551477c5f3fe5a049...|          null|\n",
            "|          🌟|570b293ecd03c9af2...|          null|\n",
            "|          🌟|54c7d44cca134c404...|          null|\n",
            "|          🌟|553008155d6cc86a4...|          null|\n",
            "|          🌟|56d3315bcd03c9af2...|          null|\n",
            "|          🌟|571ba808cd03c9af2...|          null|\n",
            "|          🌟|5633e73acd03c9af2...|          null|\n",
            "|          🌟|5663e143cd03c9af2...|          null|\n",
            "|          🌟|5663e143cd03c9af2...|          null|\n",
            "|          🌟|5663e143cd03c9af2...|          null|\n",
            "|          🌟|5663e143cd03c9af2...|          null|\n",
            "+------------+--------------------+--------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Pe3Jr_ezVMZ",
        "colab_type": "text"
      },
      "source": [
        "Category count"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFDRfLjFzauC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "1d9dc18b-cf98-4afa-ad22-7e1598ba4a18"
      },
      "source": [
        "#emoji category count\n",
        "emoji_count=Mapped_emojis.groupBy(\"story_id\",\"emoji_category\").agg(count(\"*\"))\n",
        "emoji_count=emoji_count.withColumnRenamed(\"count(1)\", \"emoji_count\")\n",
        "emoji_count=emoji_count.withColumnRenamed(\"story_id\", \"story_id_emoji\")\n",
        "emoji_count=emoji_count.na.fill('Unclassified')\n",
        "emoji_count.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------------+-----------+\n",
            "|      story_id_emoji|emoji_category|emoji_count|\n",
            "+--------------------+--------------+-----------+\n",
            "|56a01529cd03c9af2...|          Food|          1|\n",
            "|56d364d5cd03c9af2...|          Food|          1|\n",
            "|55852225b651bb748...|          Food|          1|\n",
            "|5670566fcd03c9af2...|          Food|          1|\n",
            "|56022c0ccd03c9af2...|          Food|          1|\n",
            "|5675de89cd03c9af2...|          Food|          4|\n",
            "|556102eaca8179256...|          Food|          2|\n",
            "|56f6e980cd03c9af2...|      Activity|          1|\n",
            "|53dd1dcb7d0b0354e...|      Activity|          1|\n",
            "|56870a23cd03c9af2...|      Activity|          1|\n",
            "|561539d3cd03c9af2...|  Unclassified|          3|\n",
            "|53223a04f02e4d984...|          Food|          1|\n",
            "|55f5bb72cd03c9af2...|      Activity|          3|\n",
            "|56b0fcdfcd03c9af2...|      Activity|          2|\n",
            "|541252d57d0b0354e...|        People|          5|\n",
            "|5508acf0cd03c9af2...|      Activity|          1|\n",
            "|57b3c41723e064eac...|  Unclassified|          6|\n",
            "|552be43ad4aeb6459...|  Unclassified|          5|\n",
            "|57383611cd03c9af2...|  Unclassified|          1|\n",
            "|5767fae1cd03c9af2...|      Activity|          1|\n",
            "+--------------------+--------------+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UB4xCIwl_xll",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "cc5b55b6-c68d-4075-c91a-bd44b0ddf146"
      },
      "source": [
        "#words category count\n",
        "word_count=Mapped_words.groupBy(\"story_id\",\"word_category\").agg(count(\"*\"))\n",
        "word_count=word_count.withColumnRenamed(\"count(1)\", \"word_count\")\n",
        "word_count=word_count.withColumnRenamed(\"story_id\", \"story_id_word\")\n",
        "word_count=word_count.na.fill('Unclassified')\n",
        "word_count.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+-------------+----------+\n",
            "|       story_id_word|word_category|word_count|\n",
            "+--------------------+-------------+----------+\n",
            "|57531ee1cd03c9af2...| Unclassified|         5|\n",
            "|53361fcbd546b8434...| Unclassified|         1|\n",
            "|57e44dce23e064eac...| Unclassified|         1|\n",
            "|56461499cd03c9af2...| Unclassified|         3|\n",
            "|540906217d0b0354e...| Unclassified|         1|\n",
            "|5724b10bcd03c9af2...| Unclassified|         1|\n",
            "|56672c25cd03c9af2...| Unclassified|         6|\n",
            "|57d31e0323e064eac...| Unclassified|         1|\n",
            "|5511fabdcd03c9af2...| Unclassified|         1|\n",
            "|56f3591acd03c9af2...| Unclassified|         7|\n",
            "|575c4e5acd03c9af2...|     Activity|         1|\n",
            "|552dae721a624b26d...| Unclassified|         1|\n",
            "|5568e6eb2b64f764e...| Unclassified|         1|\n",
            "|5636a269cd03c9af2...| Unclassified|         5|\n",
            "|57712e3a23e064eac...| Unclassified|         3|\n",
            "|569aaf89cd03c9af2...|         Food|         1|\n",
            "|57c9a9d123e064eac...| Unclassified|         1|\n",
            "|567899ddcd03c9af2...| Unclassified|         1|\n",
            "|560238c2cd03c9af2...| Unclassified|         2|\n",
            "|575a007dcd03c9af2...| Unclassified|         2|\n",
            "+--------------------+-------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svzU4FcCGgA0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "de76aa3d-b2a2-4c55-afb9-d57176c73445"
      },
      "source": [
        "#transactions which have emoticons & words of same category\n",
        "all_matched = word_count.join(emoji_count, [word_count.story_id_word == emoji_count.story_id_emoji, word_count.word_category == emoji_count.emoji_category], how='inner')\n",
        "\n",
        "#counting all instances (words or emojis) of that category\n",
        "all_matched=all_matched.withColumn('total_count', all_matched.emoji_count + all_matched.word_count)\n",
        "\n",
        "all_matched.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+-------------+----------+--------------------+--------------+-----------+-----------+\n",
            "|       story_id_word|word_category|word_count|      story_id_emoji|emoji_category|emoji_count|total_count|\n",
            "+--------------------+-------------+----------+--------------------+--------------+-----------+-----------+\n",
            "|52334aa8d56b6bac5...| Unclassified|         1|52334aa8d56b6bac5...|  Unclassified|          1|          2|\n",
            "|52acb446d56b6bac5...| Unclassified|         2|52acb446d56b6bac5...|  Unclassified|          1|          3|\n",
            "|53545753d546b8434...| Unclassified|         2|53545753d546b8434...|  Unclassified|          1|          3|\n",
            "|536d31ecd546b8434...| Unclassified|         1|536d31ecd546b8434...|  Unclassified|          1|          2|\n",
            "|53719fbdd546b8434...| Unclassified|         1|53719fbdd546b8434...|  Unclassified|          1|          2|\n",
            "|539daa167d0b0354e...| Unclassified|         2|539daa167d0b0354e...|  Unclassified|          1|          3|\n",
            "|53bd7ee47d0b0354e...| Unclassified|         6|53bd7ee47d0b0354e...|  Unclassified|          1|          7|\n",
            "|53dd1dcb7d0b0354e...|     Activity|         1|53dd1dcb7d0b0354e...|      Activity|          1|          2|\n",
            "|53ea7d7c7d0b0354e...| Unclassified|         3|53ea7d7c7d0b0354e...|  Unclassified|          1|          4|\n",
            "|540267c37d0b0354e...| Unclassified|         1|540267c37d0b0354e...|  Unclassified|          1|          2|\n",
            "|543eea7c7d0b0354e...| Unclassified|         1|543eea7c7d0b0354e...|  Unclassified|          1|          2|\n",
            "|5442c4c87d0b0354e...| Unclassified|         1|5442c4c87d0b0354e...|  Unclassified|          1|          2|\n",
            "|544e70907d0b0354e...| Unclassified|         3|544e70907d0b0354e...|  Unclassified|          1|          4|\n",
            "|545fc0267d0b0354e...| Unclassified|         2|545fc0267d0b0354e...|  Unclassified|          1|          3|\n",
            "|547957350cfa78941...| Unclassified|         1|547957350cfa78941...|  Unclassified|          1|          2|\n",
            "|5486752efe5f22c0b...| Unclassified|         6|5486752efe5f22c0b...|  Unclassified|          1|          7|\n",
            "|548f1aa3ca134c404...| Unclassified|         2|548f1aa3ca134c404...|  Unclassified|          1|          3|\n",
            "|54922e9eca134c404...| Unclassified|         1|54922e9eca134c404...|  Unclassified|          3|          4|\n",
            "|54a6d494ca134c404...| Unclassified|         6|54a6d494ca134c404...|  Unclassified|          1|          7|\n",
            "|54c55e0eca134c404...| Unclassified|         2|54c55e0eca134c404...|  Unclassified|          1|          3|\n",
            "+--------------------+-------------+----------+--------------------+--------------+-----------+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtnCRBaFKVHx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "efac07e0-1ca4-4fd3-f5ea-e3f7b1e337b2"
      },
      "source": [
        "#merging word & emoji counts into category_count\n",
        "category_count=emoji_count.unionAll(word_count)\n",
        "#merging matched transactions (emjoi & word) with category_count\n",
        "all_matched=all_matched.select('story_id_word','word_category','total_count')\n",
        "transaction_category_count=all_matched.unionAll(category_count)\n",
        "transaction_category_count.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+-------------+-----------+\n",
            "|       story_id_word|word_category|total_count|\n",
            "+--------------------+-------------+-----------+\n",
            "|52334aa8d56b6bac5...| Unclassified|          2|\n",
            "|52acb446d56b6bac5...| Unclassified|          3|\n",
            "|53545753d546b8434...| Unclassified|          3|\n",
            "|536d31ecd546b8434...| Unclassified|          2|\n",
            "|53719fbdd546b8434...| Unclassified|          2|\n",
            "|539daa167d0b0354e...| Unclassified|          3|\n",
            "|53bd7ee47d0b0354e...| Unclassified|          7|\n",
            "|53dd1dcb7d0b0354e...|     Activity|          2|\n",
            "|53ea7d7c7d0b0354e...| Unclassified|          4|\n",
            "|540267c37d0b0354e...| Unclassified|          2|\n",
            "|543eea7c7d0b0354e...| Unclassified|          2|\n",
            "|5442c4c87d0b0354e...| Unclassified|          2|\n",
            "|544e70907d0b0354e...| Unclassified|          4|\n",
            "|545fc0267d0b0354e...| Unclassified|          3|\n",
            "|547957350cfa78941...| Unclassified|          2|\n",
            "|5486752efe5f22c0b...| Unclassified|          7|\n",
            "|548f1aa3ca134c404...| Unclassified|          3|\n",
            "|54922e9eca134c404...| Unclassified|          4|\n",
            "|54a6d494ca134c404...| Unclassified|          7|\n",
            "|54c55e0eca134c404...| Unclassified|          3|\n",
            "+--------------------+-------------+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBx6n3eBWQtr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "b8a9b922-09ba-4fc9-b1d5-000d1b5f4fcc"
      },
      "source": [
        "#removing unclassified and finding the category with maximum counts\n",
        "w = Window.partitionBy('story_id_word')\n",
        "max_category=transaction_category_count.filter(transaction_category_count.word_category!= \"Unclassified\").withColumn('max_total_count', f.max('total_count').over(w))\\\n",
        "    .where(f.col('total_count') == f.col('max_total_count'))\\\n",
        "    .drop('total_count')\n",
        "max_category=max_category.withColumnRenamed('story_id_word','story_id')\n",
        "max_category.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+---------------+---------------+\n",
            "|            story_id|  word_category|max_total_count|\n",
            "+--------------------+---------------+---------------+\n",
            "|532c62a0d546b8434...|Illegal/Sarcasm|              1|\n",
            "|5340bbc0d546b8434...|           Food|              1|\n",
            "|53567ecdd546b8434...|         People|              1|\n",
            "|53657200d546b8434...|       Activity|              1|\n",
            "|5387e6d9d546b8434...|         People|              4|\n",
            "|538cf297d546b8434...|          Event|              1|\n",
            "|53dba6467d0b0354e...|Illegal/Sarcasm|              1|\n",
            "|53dba6467d0b0354e...|         People|              1|\n",
            "|53e14a017d0b0354e...|        Utility|              1|\n",
            "|541ee5b17d0b0354e...| Transportation|              1|\n",
            "|5431ca3a7d0b0354e...|         People|              1|\n",
            "|5431ca3a7d0b0354e...|           Food|              1|\n",
            "|543cf38b7d0b0354e...|           Food|              2|\n",
            "|544489757d0b0354e...|Illegal/Sarcasm|              1|\n",
            "|545d45867d0b0354e...|         People|              1|\n",
            "|54674b937d0b0354e...| Transportation|              1|\n",
            "|548b8fd2ca134c404...|           Food|              1|\n",
            "|54955592ca134c404...|           Food|              1|\n",
            "|54a341bbca134c404...|          Event|              1|\n",
            "|550ac739cd03c9af2...|        Utility|              1|\n",
            "+--------------------+---------------+---------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCp1ut5apkxx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "3b6ea99a-9483-4a57-b387-9486c3221515"
      },
      "source": [
        "#getting the final classification of spend\n",
        "final_classification=input_parq.join(max_category, on='story_id', how='left')\n",
        "final_classification=final_classification.fillna({'word_category':'Unclassified'})\n",
        "final_classification=final_classification.drop_duplicates(subset=['story_id'])\n",
        "final_classification=final_classification.withColumnRenamed('word_category','spend_category')\n",
        "final_classification=final_classification.drop(final_classification.max_total_count)\n",
        "final_classification.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+-------+-------+----------------+-------------------+--------------------+-----------+---------------+\n",
            "|            story_id|  user1|  user2|transaction_type|           datetime|         description|is_business| spend_category|\n",
            "+--------------------+-------+-------+----------------+-------------------+--------------------+-----------+---------------+\n",
            "|529ae977d56b6bac5...| 442653| 546146|         payment|2013-12-01 15:47:01|     Drinks and shiz|      false|   Unclassified|\n",
            "|52be390bd56b6bac5...| 310867| 293503|         payment|2013-12-28 10:35:55|                 Can|      false|   Unclassified|\n",
            "|5303ace7f02e4d984...| 523875| 620678|         payment|2014-02-19 02:56:38|                Skis|      false|   Unclassified|\n",
            "|532c62a0d546b8434...| 493604| 779561|         payment|2014-03-21 23:02:40|     Suga momma ❤️😘|      false|Illegal/Sarcasm|\n",
            "|53361fcbd546b8434...|1052066| 748914|          charge|2014-03-29 08:20:11|            Brackets|      false|   Unclassified|\n",
            "|5340bbc0d546b8434...| 515235| 479212|         payment|2014-04-06 09:28:15|     Mmhmm chocolate|      false|           Food|\n",
            "|53567ecdd546b8434...| 480959|1181469|         payment|2014-04-22 21:38:03|For being the mad...|      false|         People|\n",
            "|53657200d546b8434...| 781376|1142071|         payment|2014-05-04 05:47:28|                Gift|      false|       Activity|\n",
            "|537fee82d546b8434...| 400995| 890523|          charge|2014-05-24 07:57:38|              Wings!|      false|   Unclassified|\n",
            "|5387e6d9d546b8434...| 347634|1563885|         payment|2014-05-30 09:03:03|   BOARDY BABY😄😄😄|      false|         People|\n",
            "|538cf297d546b8434...| 646297| 640960|         payment|2014-06-03 04:54:30|                  👑|      false|          Event|\n",
            "|53b089797d0b0354e...| 658029| 917934|          charge|2014-06-30 04:47:37|               May🌸|      false|   Unclassified|\n",
            "|53dba6467d0b0354e...| 688897| 145618|         payment|2014-08-01 21:37:58|  Cool as a cucumber|      false|Illegal/Sarcasm|\n",
            "|53e14a017d0b0354e...| 891407| 891401|         payment|2014-08-06 04:17:51|         August rent|      false|        Utility|\n",
            "|540906217d0b0354e...| 294531| 282640|          charge|2014-09-05 07:38:57|               Rolls|      false|   Unclassified|\n",
            "|541ee5b17d0b0354e...|1511424| 491210|          charge|2014-09-21 21:50:25|We went a \"little...|      false| Transportation|\n",
            "|5431ca3a7d0b0354e...|1147383|1014801|         payment|2014-10-06 05:46:18| Beef tongues and 💩|      false|         People|\n",
            "|543cf38b7d0b0354e...| 382391| 529244|         payment|2014-10-14 16:57:31|     Vino and cheese|      false|           Food|\n",
            "|544489757d0b0354e...| 755092|2626577|         payment|2014-10-20 11:03:01|                 sex|      false|Illegal/Sarcasm|\n",
            "|545d45867d0b0354e...|1357997| 388253|         payment|2014-11-08 06:19:50|for being a good ...|      false|         People|\n",
            "+--------------------+-------+-------+----------------+-------------------+--------------------+-----------+---------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOd-TeoDrU5W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "531d4631-b056-4db7-99e4-7f2f6803885b"
      },
      "source": [
        "final_classification.select('story_id').distinct().count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3scWGjYt69D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "bb8589fd-0fd3-403c-d32b-a68fc48d0856"
      },
      "source": [
        "final_classification.count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXRX3P6wAJGk",
        "colab_type": "text"
      },
      "source": [
        "## Analyzing Emoji Transactions\n",
        "What is the percent of emoji only transactions? Which are the top 5 most popular emoji? Which are the top three most popular emoji categories?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6nLcCN5AQ2h",
        "colab_type": "text"
      },
      "source": [
        "### Percentage of emoji only transactions\n",
        "\n",
        "It's worth noting that all descriptions MUST have something entered in them. TO calculate the percentage of emoji only transactions we will:\n",
        "\n",
        "- Create a new column that removes all emojis from the description\n",
        "- Count the number of blank rows in the new column\n",
        "- Divide by the total number of rows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsnAMzctCQXr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# change \"drive\" to \"gdrive\" for windows\n",
        "path='/content/drive/My Drive/Colab Notebooks/Bigdata_Venmo/VenmoSample.snappy.parquet'\n",
        "inputFile= spark\\\n",
        "                  .read\\\n",
        "                  .option('header','true')\\\n",
        "                  .option('inferSchema','true')\\\n",
        "                  .parquet(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h71xLz9PAShV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Percentage_df = inputFile.withColumn('clean_text',regexp_replace('description',\"\\\\p{So}+\",''))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WhmChwjAU7K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "a4acf024-7394-4def-e65e-fb55b788b6bd"
      },
      "source": [
        "# test \n",
        "Percentage_df.select([c for c in Percentage_df.columns if c in ['description','clean_text']]).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+\n",
            "|         description|          clean_text|\n",
            "+--------------------+--------------------+\n",
            "|                Uber|                Uber|\n",
            "|              Costco|              Costco|\n",
            "|        Sweaty balls|        Sweaty balls|\n",
            "|                  🎥|                    |\n",
            "|                   ⚡|                    |\n",
            "|          Chipotlaid|          Chipotlaid|\n",
            "|     kitchen counter|     kitchen counter|\n",
            "|                Food|                Food|\n",
            "|               Zaxby|               Zaxby|\n",
            "|           Fan sucks|           Fan sucks|\n",
            "|                  👠|                    |\n",
            "|         Thanks babe|         Thanks babe|\n",
            "|                  🍺|                    |\n",
            "|                   ⛽|                    |\n",
            "|Hey man  it's bee...|Hey man  it's bee...|\n",
            "|         Getaway car|         Getaway car|\n",
            "|     🔮 gypsy things|        gypsy things|\n",
            "|                  🔴|                    |\n",
            "|           Furniture|           Furniture|\n",
            "|bed bath mostly b...|bed bath mostly b...|\n",
            "+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyM-TGqrAYLn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "783b8549-548c-4e46-a0ba-1682af308b9c"
      },
      "source": [
        "Emoji_rows = Percentage_df.filter((Percentage_df[\"clean_text\"] == \"\") | Percentage_df[\"clean_text\"].isNull() | isnan(Percentage_df[\"clean_text\"])).count()\n",
        "Total_rows = Percentage_df.count()\n",
        "print(Emoji_rows *100/ Total_rows)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "19.603179300497093\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwUCuU4nAcLZ",
        "colab_type": "text"
      },
      "source": [
        "Emoji only transactions account for 19.6% of all transactions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pgy9eijYAeYt",
        "colab_type": "text"
      },
      "source": [
        "### Top 5 most popular emojis\n",
        "\n",
        "We begin by creating a new column that extracts only the emoji characters. Afterwards, we will extract that column as a list and obtain the counter for each individual emoji on each row."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBsirCVMAgUQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Emojis_df = inputFile.withColumn('Emojis',regexp_extract(col('description'),\"\\\\p{So}+\",0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Or10REYvAgbd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "5188781c-debd-4379-efa5-988a80e376af"
      },
      "source": [
        "Emojis_df.select([c for c in Emojis_df.columns if c in ['description','Emojis']]).show(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------------+------+\n",
            "|    description|Emojis|\n",
            "+---------------+------+\n",
            "|           Uber|      |\n",
            "|         Costco|      |\n",
            "|   Sweaty balls|      |\n",
            "|             🎥|    🎥|\n",
            "|              ⚡|     ⚡|\n",
            "|     Chipotlaid|      |\n",
            "|kitchen counter|      |\n",
            "|           Food|      |\n",
            "|          Zaxby|      |\n",
            "|      Fan sucks|      |\n",
            "+---------------+------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgLZ4vmwBpVe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "e440a7f9-2a1a-4578-d343-fb540d93208c"
      },
      "source": [
        "# Delete transactions that didn't contain emojis \n",
        "Emojis_only_df = Emojis_df.filter(Emojis_df.Emojis != \"\")\n",
        "# and check\n",
        "Emojis_only_df.select([c for c in Emojis_only_df.columns if c in ['description','Emojis']]).show(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+------+\n",
            "|         description|Emojis|\n",
            "+--------------------+------+\n",
            "|                  🎥|    🎥|\n",
            "|                   ⚡|     ⚡|\n",
            "|                  👠|    👠|\n",
            "|                  🍺|    🍺|\n",
            "|                   ⛽|     ⛽|\n",
            "|     🔮 gypsy things|    🔮|\n",
            "|                  🔴|    🔴|\n",
            "|  DAT CPK THO 💁🏻🍕|    💁|\n",
            "|Bdayyy🎉💞🎉 sund...|🎉💞🎉|\n",
            "|                ✌❤🏈|  ✌❤🏈|\n",
            "+--------------------+------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWeBtNGEBpad",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create function to separate split emojis in a string to arrays\n",
        "def emoji_splitter(text):\n",
        "    new_string = \"\"\n",
        "    for char in text:\n",
        "        if char in emoji.UNICODE_EMOJI:\n",
        "            new_string += \" {} \".format(char)\n",
        "        else:\n",
        "            new_string += char\n",
        "    return [v for v in map(lambda x: x.strip(), new_string.split(\" \")) if v != \"\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8A04YjNkBpgT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# UDF-ize for Spark\n",
        "emoji_array_udf = udf(lambda z: emoji_splitter(z), StringType())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHTLvXdmBpef",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "ed6873ff-c7f8-4af1-966d-0c4ee582bb00"
      },
      "source": [
        "# Create a new column that splits the emojis\n",
        "Emojis_only_df = Emojis_only_df.withColumn(\"split_emojis\",emoji_array_udf(Emojis_only_df.Emojis))\n",
        "# Let's see how they look like now\n",
        "Emojis_only_df.select([c for c in Emojis_only_df.columns if c in ['description','Emojis','split_emojis']]).show(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+------+------------+\n",
            "|         description|Emojis|split_emojis|\n",
            "+--------------------+------+------------+\n",
            "|                  🎥|    🎥|        [🎥]|\n",
            "|                   ⚡|     ⚡|         [⚡]|\n",
            "|                  👠|    👠|        [👠]|\n",
            "|                  🍺|    🍺|        [🍺]|\n",
            "|                   ⛽|     ⛽|         [⛽]|\n",
            "|     🔮 gypsy things|    🔮|        [🔮]|\n",
            "|                  🔴|    🔴|        [🔴]|\n",
            "|  DAT CPK THO 💁🏻🍕|    💁|        [💁]|\n",
            "|Bdayyy🎉💞🎉 sund...|🎉💞🎉|[🎉, 💞, 🎉]|\n",
            "|                ✌❤🏈|  ✌❤🏈|  [✌, ❤, 🏈]|\n",
            "+--------------------+------+------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8fkS9K1BpYa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "1b55d905-e735-43f5-9098-8eaf66f48e51"
      },
      "source": [
        "# Split, explode and count!\n",
        "Emojis_only_df.withColumn(\n",
        "    \"split_emojis\", \n",
        "    explode(split(regexp_replace(col(\"split_emojis\"), \"(^\\[)|(\\]$)\", \"\"), \", \")))\\\n",
        "    .groupBy('split_emojis')\\\n",
        "    .count()\\\n",
        "    .sort('count', ascending=False)\\\n",
        "    .show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------+------+\n",
            "|split_emojis| count|\n",
            "+------------+------+\n",
            "|          🍕|201122|\n",
            "|          🍻|128647|\n",
            "|          💸|117532|\n",
            "|          🍷|100520|\n",
            "|          🎉| 81835|\n",
            "+------------+------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTPAEDRbCY_E",
        "colab_type": "text"
      },
      "source": [
        "### Top three most popular emoji categories\n",
        "We will begin by associating each emoji with it's respective classification; afterwards, we will apply the counter similar to the previous part."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TZfjQ_CCawh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Following the last step, let's modify the dataframe to include a column for individual emojis\n",
        "Emojis_only_df_mod = Emojis_only_df.withColumn(\n",
        "    \"individual_emojis\", \n",
        "    explode(split(regexp_replace(col(\"split_emojis\"), \"(^\\[)|(\\]$)\", \"\"), \", \")))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xo4mPLz4Cazf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "64cf68d4-1658-4a3a-a731-b201738ce427"
      },
      "source": [
        "# Let's extract the column of interest\n",
        "individual_emojis_column = Emojis_only_df_mod.select(\"individual_emojis\")\n",
        "# and check\n",
        "individual_emojis_column.show(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------------+\n",
            "|individual_emojis|\n",
            "+-----------------+\n",
            "|               🎥|\n",
            "|                ⚡|\n",
            "|               👠|\n",
            "|               🍺|\n",
            "|                ⛽|\n",
            "|               🔮|\n",
            "|               🔴|\n",
            "|               💁|\n",
            "|               🎉|\n",
            "|               💞|\n",
            "+-----------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_gR36G5Ca2f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To classify the emojis into categories, let's import the classification dictionary and create a function with conditions\n",
        "# change \"drive\" to \"gdrive\" for windows\n",
        "emoji_path ='/content/drive/My Drive/Colab Notebooks/Bigdata_Venmo/Venmo_Emoji_Classification_Dictionary.csv'\n",
        "Venmo_Emoji_Dict = pd.read_csv(emoji_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KeG8Ql1CgIe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating the function\n",
        "def category_classifier(text): \n",
        "  if text in Venmo_Emoji_Dict['Event'].dropna(how='all').tolist():\n",
        "    return \"Event\"\n",
        "  elif text in Venmo_Emoji_Dict['Travel'].dropna(how='all').tolist():\n",
        "    return \"Travel\"\n",
        "  elif text in Venmo_Emoji_Dict['Food'].dropna(how='all').tolist():\n",
        "    return \"Food\"\n",
        "  elif text in Venmo_Emoji_Dict['Activity'].dropna(how='all').tolist():\n",
        "    return \"Activity\"\n",
        "  elif text in Venmo_Emoji_Dict['Transportation'].dropna(how='all').tolist():\n",
        "    return \"Transportation\"\n",
        "  elif text in Venmo_Emoji_Dict['People'].dropna(how='all').tolist():\n",
        "    return \"People\"\n",
        "  elif text in Venmo_Emoji_Dict['Utility'].dropna(how='all').tolist():\n",
        "    return \"Utility\"\n",
        "  else:\n",
        "    return \"Unclassified\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MSf-xfRChfB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# UDF-ize for Spark\n",
        "category_classifier_udf = udf(lambda z: category_classifier(z), StringType())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzrPKIHeChnI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# WARNING ONLY AGGREGATING FOR A SUBSET 100k / 4M + \n",
        "# Let's take a small subset for testing\n",
        "individual_emojis_column_sample = individual_emojis_column.limit(100000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6Ehy1oPCjYC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a new column that categorizes the emojis\n",
        "category_classifier_df = individual_emojis_column_sample.withColumn(\n",
        "    \"category\",\n",
        "    category_classifier_udf(individual_emojis_column_sample.individual_emojis))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVpsST-qCjas",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "8d53e97f-90fd-4e39-9b85-2b00caa5ba90"
      },
      "source": [
        "category_classifier_df.select([c for c in category_classifier_df.columns if c in ['individual_emojis','category']]).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------------+--------------+\n",
            "|individual_emojis|      category|\n",
            "+-----------------+--------------+\n",
            "|               🎥|         Event|\n",
            "|                ⚡|       Utility|\n",
            "|               👠|  Unclassified|\n",
            "|               🍺|          Food|\n",
            "|                ⛽|Transportation|\n",
            "|               🔮|  Unclassified|\n",
            "|               🔴|  Unclassified|\n",
            "|               💁|        People|\n",
            "|               🎉|      Activity|\n",
            "|               💞|  Unclassified|\n",
            "|               🎉|      Activity|\n",
            "|                ✌|        People|\n",
            "|                ❤|  Unclassified|\n",
            "|               🏈|      Activity|\n",
            "|               🎫|      Activity|\n",
            "|               👰|         Event|\n",
            "|               👰|         Event|\n",
            "|               🍕|          Food|\n",
            "|               🍕|          Food|\n",
            "|               🍕|          Food|\n",
            "+-----------------+--------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04GplFoBCnXM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "b0549768-21fc-4e7f-dd3b-6cdd72f77b5a"
      },
      "source": [
        "category_classifier_df.groupBy('category').count().sort('count', ascending=False).show(4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------+-----+\n",
            "|    category|count|\n",
            "+------------+-----+\n",
            "|        Food|31270|\n",
            "|Unclassified|28654|\n",
            "|      People|17079|\n",
            "|    Activity| 7250|\n",
            "+------------+-----+\n",
            "only showing top 4 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niopWwmtv6xS",
        "colab_type": "text"
      },
      "source": [
        "## Analyzing User Spending Profile\n",
        "For each user, create a variable to indicate their spending behavior profile. For example, if a user has made 10 transactions, where 5 of them are food and the other 5 are activity, then the user’s spending profile will be 50% food and 50% activity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQ1tVM-vv5z6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create dataframe from classified transaction data\n",
        "spend_data = final_classification.limit(10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4AO-F7Tz9K_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "be1a2628-69da-40cd-9876-06eed1866e38"
      },
      "source": [
        "spend_data.select([c for c in spend_data.columns if c in ['user1','description','spend_category']]).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+--------------------+---------------+\n",
            "|  user1|         description| spend_category|\n",
            "+-------+--------------------+---------------+\n",
            "| 442653|     Drinks and shiz|   Unclassified|\n",
            "| 310867|                 Can|   Unclassified|\n",
            "| 523875|                Skis|   Unclassified|\n",
            "| 493604|     Suga momma ❤️😘|Illegal/Sarcasm|\n",
            "|1052066|            Brackets|   Unclassified|\n",
            "| 515235|     Mmhmm chocolate|           Food|\n",
            "| 480959|For being the mad...|         People|\n",
            "| 781376|                Gift|       Activity|\n",
            "| 400995|              Wings!|   Unclassified|\n",
            "| 347634|   BOARDY BABY😄😄😄|         People|\n",
            "| 646297|                  👑|          Event|\n",
            "| 658029|               May🌸|   Unclassified|\n",
            "| 688897|  Cool as a cucumber|Illegal/Sarcasm|\n",
            "| 891407|         August rent|        Utility|\n",
            "| 294531|               Rolls|   Unclassified|\n",
            "|1511424|We went a \"little...| Transportation|\n",
            "|1147383| Beef tongues and 💩|         People|\n",
            "| 382391|     Vino and cheese|           Food|\n",
            "| 755092|                 sex|Illegal/Sarcasm|\n",
            "|1357997|for being a good ...|         People|\n",
            "+-------+--------------------+---------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0ghtWCQ0boq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "48b634f9-5294-4779-cb5b-4de402e15e85"
      },
      "source": [
        "# Check levels \n",
        "levels = [x for x in chain(*spend_data.select('spend_category').distinct().collect())]\n",
        "levels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Unclassified',\n",
              " 'Illegal/Sarcasm',\n",
              " 'Food',\n",
              " 'People',\n",
              " 'Activity',\n",
              " 'Event',\n",
              " 'Utility',\n",
              " 'Transportation',\n",
              " 'Travel',\n",
              " 'Cash']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4arVEL2H0r5g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "91dbdb65-e7ef-41a1-f3db-233e531458fe"
      },
      "source": [
        "# Check count of each category for each user\n",
        "pivoted = spend_data.groupBy(\"user1\").pivot('spend_category', levels).count()\n",
        "pivoted = pivoted.na.fill(0)\n",
        "pivoted.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+------------+---------------+----+------+--------+-----+-------+--------------+------+----+\n",
            "|  user1|Unclassified|Illegal/Sarcasm|Food|People|Activity|Event|Utility|Transportation|Travel|Cash|\n",
            "+-------+------------+---------------+----+------+--------+-----+-------+--------------+------+----+\n",
            "| 442653|           1|              0|   0|     0|       0|    0|      0|             0|     0|   0|\n",
            "| 310867|           1|              0|   0|     0|       0|    0|      0|             0|     0|   0|\n",
            "| 523875|           1|              0|   0|     0|       0|    0|      0|             0|     0|   0|\n",
            "| 493604|           0|              1|   0|     0|       0|    0|      0|             0|     0|   0|\n",
            "|1052066|           1|              0|   0|     0|       0|    0|      0|             0|     0|   0|\n",
            "| 515235|           0|              0|   1|     0|       0|    0|      0|             0|     0|   0|\n",
            "| 480959|           0|              0|   0|     1|       0|    0|      0|             0|     0|   0|\n",
            "| 781376|           0|              0|   0|     0|       1|    0|      0|             0|     0|   0|\n",
            "| 400995|           1|              0|   0|     0|       0|    0|      0|             0|     0|   0|\n",
            "| 347634|           0|              0|   0|     1|       0|    0|      0|             0|     0|   0|\n",
            "| 646297|           0|              0|   0|     0|       0|    1|      0|             0|     0|   0|\n",
            "| 658029|           1|              0|   0|     0|       0|    0|      0|             0|     0|   0|\n",
            "| 688897|           0|              1|   0|     0|       0|    0|      0|             0|     0|   0|\n",
            "| 891407|           0|              0|   0|     0|       0|    0|      1|             0|     0|   0|\n",
            "| 294531|           1|              0|   0|     0|       0|    0|      0|             0|     0|   0|\n",
            "|1511424|           0|              0|   0|     0|       0|    0|      0|             1|     0|   0|\n",
            "|1147383|           0|              0|   0|     1|       0|    0|      0|             0|     0|   0|\n",
            "| 382391|           1|              0|   1|     0|       0|    0|      0|             0|     0|   0|\n",
            "| 755092|           0|              1|   0|     0|       0|    0|      0|             0|     0|   0|\n",
            "|1357997|           0|              0|   0|     1|       0|    0|      0|             0|     0|   0|\n",
            "+-------+------------+---------------+----+------+--------+-----+-------+--------------+------+----+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNqoeibU5h7k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a column to sum \n",
        "pivoted = pivoted.withColumn('Sum', col(\"Food\") + col(\"Event\") + col(\"Travel\") + col(\"Activity\") + col(\"Utility\") + col(\"Unclassified\") + col(\"People\") + col(\"Transportation\")+col(\"Illegal/Sarcasm\")+col(\"Cash\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3BFNWL35nNY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " #then change counts to proportion \n",
        "revised_pivot = pivoted.withColumn('Food', (col(\"Food\")*100)/(col('Sum')))\\\n",
        ".withColumn('Event', (col(\"Event\")*100)/(col('Sum')))\\\n",
        ".withColumn('Travel', (col(\"Travel\")*100)/(col('Sum')))\\\n",
        ".withColumn('Activity', (col(\"Activity\")*100)/(col('Sum')))\\\n",
        ".withColumn('Utility', (col(\"Utility\")*100)/(col('Sum')))\\\n",
        ".withColumn('Unclassified', (col(\"Unclassified\")*100)/(col('Sum')))\\\n",
        ".withColumn('People', (col(\"People\")*100)/(col('Sum')))\\\n",
        ".withColumn('Transportation', (col(\"Transportation\")*100)/(col('Sum')))\\\n",
        ".withColumn('Illegal/Sarcasm', (col(\"Illegal/Sarcasm\")*100)/(col('Sum')))\\\n",
        ".withColumn('Cash', (col(\"Cash\")*100)/(col('Sum')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhC4eIpX5r5M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "79363bc5-5da5-4d6c-cca0-8c048d6cd63d"
      },
      "source": [
        "# View\n",
        "revised_pivot.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+------------+---------------+-----+------+--------+-----+-------+--------------+------+----+---+\n",
            "|  user1|Unclassified|Illegal/Sarcasm| Food|People|Activity|Event|Utility|Transportation|Travel|Cash|Sum|\n",
            "+-------+------------+---------------+-----+------+--------+-----+-------+--------------+------+----+---+\n",
            "| 442653|       100.0|            0.0|  0.0|   0.0|     0.0|  0.0|    0.0|           0.0|   0.0| 0.0|  1|\n",
            "| 310867|       100.0|            0.0|  0.0|   0.0|     0.0|  0.0|    0.0|           0.0|   0.0| 0.0|  1|\n",
            "| 523875|       100.0|            0.0|  0.0|   0.0|     0.0|  0.0|    0.0|           0.0|   0.0| 0.0|  1|\n",
            "| 493604|         0.0|          100.0|  0.0|   0.0|     0.0|  0.0|    0.0|           0.0|   0.0| 0.0|  1|\n",
            "|1052066|       100.0|            0.0|  0.0|   0.0|     0.0|  0.0|    0.0|           0.0|   0.0| 0.0|  1|\n",
            "| 515235|         0.0|            0.0|100.0|   0.0|     0.0|  0.0|    0.0|           0.0|   0.0| 0.0|  1|\n",
            "| 480959|         0.0|            0.0|  0.0| 100.0|     0.0|  0.0|    0.0|           0.0|   0.0| 0.0|  1|\n",
            "| 781376|         0.0|            0.0|  0.0|   0.0|   100.0|  0.0|    0.0|           0.0|   0.0| 0.0|  1|\n",
            "| 400995|       100.0|            0.0|  0.0|   0.0|     0.0|  0.0|    0.0|           0.0|   0.0| 0.0|  1|\n",
            "| 347634|         0.0|            0.0|  0.0| 100.0|     0.0|  0.0|    0.0|           0.0|   0.0| 0.0|  1|\n",
            "| 646297|         0.0|            0.0|  0.0|   0.0|     0.0|100.0|    0.0|           0.0|   0.0| 0.0|  1|\n",
            "| 658029|       100.0|            0.0|  0.0|   0.0|     0.0|  0.0|    0.0|           0.0|   0.0| 0.0|  1|\n",
            "| 688897|         0.0|          100.0|  0.0|   0.0|     0.0|  0.0|    0.0|           0.0|   0.0| 0.0|  1|\n",
            "| 891407|         0.0|            0.0|  0.0|   0.0|     0.0|  0.0|  100.0|           0.0|   0.0| 0.0|  1|\n",
            "| 294531|       100.0|            0.0|  0.0|   0.0|     0.0|  0.0|    0.0|           0.0|   0.0| 0.0|  1|\n",
            "|1511424|         0.0|            0.0|  0.0|   0.0|     0.0|  0.0|    0.0|         100.0|   0.0| 0.0|  1|\n",
            "|1147383|         0.0|            0.0|  0.0| 100.0|     0.0|  0.0|    0.0|           0.0|   0.0| 0.0|  1|\n",
            "| 382391|        50.0|            0.0| 50.0|   0.0|     0.0|  0.0|    0.0|           0.0|   0.0| 0.0|  2|\n",
            "| 755092|         0.0|          100.0|  0.0|   0.0|     0.0|  0.0|    0.0|           0.0|   0.0| 0.0|  1|\n",
            "|1357997|         0.0|            0.0|  0.0| 100.0|     0.0|  0.0|    0.0|           0.0|   0.0| 0.0|  1|\n",
            "+-------+------------+---------------+-----+------+--------+-----+-------+--------------+------+----+---+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2o6UQpHE7azS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating a variable for the spending profile of users\n",
        "user_spending_behavior = revised_pivot.withColumn('variable',\n",
        "                            f.concat(f.col('Utility'),f.lit(\"% Utility, \"),f.col('Transportation'),f.lit(\"% Transportation, \"),\n",
        "                                           f.col('Unclassified'),f.lit(\"% Unclassified, \"),f.col('Illegal/Sarcasm'),f.lit(\"% Illegal/Sarcasm, \"),\n",
        "                                           f.col('Food'),f.lit(\"% Food, \"),f.col('People'),f.lit(\"% People and \"),\n",
        "                                           f.col('Activity'),f.lit(\"% Activity, \"),f.col('Event'),f.lit(\"% Event and \"),\n",
        "                                           f.col('Travel'),f.lit(\"% Travel and \"),f.col('Cash'),f.lit(\"% Cash\")))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oABuLcC67a3G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 499
        },
        "outputId": "f8db65ff-2067-41e6-b4ae-9b8ed99076b3"
      },
      "source": [
        "# View \n",
        "user_spending_behavior.select([c for c in user_spending_behavior.columns if c in ['user1','variable']]).show(20, False)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+----------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|user1  |variable                                                                                                                                                        |\n",
            "+-------+----------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|442653 |0.0% Utility, 0.0% Transportation, 100.0% Unclassified, 0.0% Illegal/Sarcasm, 0.0% Food, 0.0% People and 0.0% Activity, 0.0% Event and 0.0% Travel and 0.0% Cash|\n",
            "|310867 |0.0% Utility, 0.0% Transportation, 100.0% Unclassified, 0.0% Illegal/Sarcasm, 0.0% Food, 0.0% People and 0.0% Activity, 0.0% Event and 0.0% Travel and 0.0% Cash|\n",
            "|523875 |0.0% Utility, 0.0% Transportation, 100.0% Unclassified, 0.0% Illegal/Sarcasm, 0.0% Food, 0.0% People and 0.0% Activity, 0.0% Event and 0.0% Travel and 0.0% Cash|\n",
            "|493604 |0.0% Utility, 0.0% Transportation, 0.0% Unclassified, 100.0% Illegal/Sarcasm, 0.0% Food, 0.0% People and 0.0% Activity, 0.0% Event and 0.0% Travel and 0.0% Cash|\n",
            "|1052066|0.0% Utility, 0.0% Transportation, 100.0% Unclassified, 0.0% Illegal/Sarcasm, 0.0% Food, 0.0% People and 0.0% Activity, 0.0% Event and 0.0% Travel and 0.0% Cash|\n",
            "|515235 |0.0% Utility, 0.0% Transportation, 0.0% Unclassified, 0.0% Illegal/Sarcasm, 100.0% Food, 0.0% People and 0.0% Activity, 0.0% Event and 0.0% Travel and 0.0% Cash|\n",
            "|480959 |0.0% Utility, 0.0% Transportation, 0.0% Unclassified, 0.0% Illegal/Sarcasm, 0.0% Food, 100.0% People and 0.0% Activity, 0.0% Event and 0.0% Travel and 0.0% Cash|\n",
            "|781376 |0.0% Utility, 0.0% Transportation, 0.0% Unclassified, 0.0% Illegal/Sarcasm, 0.0% Food, 0.0% People and 100.0% Activity, 0.0% Event and 0.0% Travel and 0.0% Cash|\n",
            "|400995 |0.0% Utility, 0.0% Transportation, 100.0% Unclassified, 0.0% Illegal/Sarcasm, 0.0% Food, 0.0% People and 0.0% Activity, 0.0% Event and 0.0% Travel and 0.0% Cash|\n",
            "|347634 |0.0% Utility, 0.0% Transportation, 0.0% Unclassified, 0.0% Illegal/Sarcasm, 0.0% Food, 100.0% People and 0.0% Activity, 0.0% Event and 0.0% Travel and 0.0% Cash|\n",
            "|646297 |0.0% Utility, 0.0% Transportation, 0.0% Unclassified, 0.0% Illegal/Sarcasm, 0.0% Food, 0.0% People and 0.0% Activity, 100.0% Event and 0.0% Travel and 0.0% Cash|\n",
            "|658029 |0.0% Utility, 0.0% Transportation, 100.0% Unclassified, 0.0% Illegal/Sarcasm, 0.0% Food, 0.0% People and 0.0% Activity, 0.0% Event and 0.0% Travel and 0.0% Cash|\n",
            "|688897 |0.0% Utility, 0.0% Transportation, 0.0% Unclassified, 100.0% Illegal/Sarcasm, 0.0% Food, 0.0% People and 0.0% Activity, 0.0% Event and 0.0% Travel and 0.0% Cash|\n",
            "|891407 |100.0% Utility, 0.0% Transportation, 0.0% Unclassified, 0.0% Illegal/Sarcasm, 0.0% Food, 0.0% People and 0.0% Activity, 0.0% Event and 0.0% Travel and 0.0% Cash|\n",
            "|294531 |0.0% Utility, 0.0% Transportation, 100.0% Unclassified, 0.0% Illegal/Sarcasm, 0.0% Food, 0.0% People and 0.0% Activity, 0.0% Event and 0.0% Travel and 0.0% Cash|\n",
            "|1511424|0.0% Utility, 100.0% Transportation, 0.0% Unclassified, 0.0% Illegal/Sarcasm, 0.0% Food, 0.0% People and 0.0% Activity, 0.0% Event and 0.0% Travel and 0.0% Cash|\n",
            "|1147383|0.0% Utility, 0.0% Transportation, 0.0% Unclassified, 0.0% Illegal/Sarcasm, 0.0% Food, 100.0% People and 0.0% Activity, 0.0% Event and 0.0% Travel and 0.0% Cash|\n",
            "|382391 |0.0% Utility, 0.0% Transportation, 50.0% Unclassified, 0.0% Illegal/Sarcasm, 50.0% Food, 0.0% People and 0.0% Activity, 0.0% Event and 0.0% Travel and 0.0% Cash|\n",
            "|755092 |0.0% Utility, 0.0% Transportation, 0.0% Unclassified, 100.0% Illegal/Sarcasm, 0.0% Food, 0.0% People and 0.0% Activity, 0.0% Event and 0.0% Travel and 0.0% Cash|\n",
            "|1357997|0.0% Utility, 0.0% Transportation, 0.0% Unclassified, 0.0% Illegal/Sarcasm, 0.0% Food, 100.0% People and 0.0% Activity, 0.0% Event and 0.0% Travel and 0.0% Cash|\n",
            "+-------+----------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0XV-UZK5alz",
        "colab_type": "text"
      },
      "source": [
        "## Building User's Dyamic Spending Profile\n",
        "In the previous question, you got a static spending profile. However, life and social networks are evolving over time. Therefore, let’s explore how a user’s spending profile is evolving over her lifetime in Venmo. First of all, you need to analyze a user’s transactions in monthly intervals, starting from 0 (indicating their first transaction only ) up to 12.\n",
        "\n",
        "For example, assume a user’s first transaction was a pizza emoji. Then, her user profile at 0 would be 100% food. Now, by the end of her first month in Venmo, she has transacted 4 times, 2 of them are food and 2 are activity related. Her speding profile in 1 month is 50% food and 50% activity. Following this logic, you need to create a user’s profile up to 12 months.\n",
        "\n",
        "If you do this right, you will create a dynamic spending profile for each user. However, this is meaningless to plot. Let’s plot instead the spending profile of the average user. To do this, for each time point, you need to compute the average and standard deviation of each spending\n",
        "category across all users. Therefore, in your y-axis, you will have time in months (from 0 up to 12). In your x-axis, for each time point, plot the average for each category surrounded by its confidence interval (+-2 * standard deviation). What do you observe? Does the spending profile of the average customer stabilize after some point in time?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwO0f5IUi4dX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create dataframe from classified transaction data\n",
        "dynamic_spend_data = final_classification"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBQQmQFi6K0o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "first_transaction=dynamic_spend_data.groupBy(\"user1\").agg(min(\"datetime\"))\n",
        "first_transaction=first_transaction.withColumnRenamed('min(datetime)', 'first_transaction_datetime')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3t0wqdx-8C-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spend_data_first_date=dynamic_spend_data.join(first_transaction, on=['user1'], how='inner')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nD8ooELBBiU0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Calculate difference between first_transaction_datetime and datetime for each user in months in pyspark\n",
        "  \n",
        "spend_timeperiod=spend_data_first_date.withColumn(\"time_period_months\", months_between(col(\"datetime\"),col(\"first_transaction_datetime\")))\n",
        "\n",
        "spend_timeperiod=spend_timeperiod.select(\"*\", round(col('time_period_months'),0))\n",
        "spend_timeperiod=spend_timeperiod.withColumnRenamed('round(time_period_months, 0)','timeperiod')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDHHNtx3EQ2N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 499
        },
        "outputId": "1ab31106-e171-435b-9ea1-c26337618d9a"
      },
      "source": [
        "spend_timeperiod.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+--------------------+-------+----------------+-------------------+--------------------+-----------+--------------+--------------------------+------------------+----------+\n",
            "| user1|            story_id|  user2|transaction_type|           datetime|         description|is_business|spend_category|first_transaction_datetime|time_period_months|timeperiod|\n",
            "+------+--------------------+-------+----------------+-------------------+--------------------+-----------+--------------+--------------------------+------------------+----------+\n",
            "|118628|54ff7e12cd03c9af2...| 443608|         payment|2015-03-11 06:28:18|               Token|      false|  Unclassified|       2015-03-11 06:28:18|               0.0|       0.0|\n",
            "|126753|56f6e980cd03c9af2...| 846421|         payment|2016-03-27 02:56:48|                   ⛳|      false|      Activity|       2016-03-27 02:56:48|               0.0|       0.0|\n",
            "|137147|5797597123e064eac...|1106650|          charge|2016-07-26 19:37:05|              Costco|      false|          Food|       2016-07-26 19:37:05|               0.0|       0.0|\n",
            "|150285|5621bd07cd03c9af2...|1064458|         payment|2015-10-17 10:14:15|Kayaking refund. ...|      false|  Unclassified|       2015-10-17 10:14:15|               0.0|       0.0|\n",
            "|166578|544155527d0b0354e...|1654912|         payment|2014-10-18 00:43:44|               Stuff|      false|  Unclassified|       2014-10-18 00:43:44|               0.0|       0.0|\n",
            "|174215|532b0be2d546b8434...| 174233|         payment|2014-03-20 22:40:15|             Madness|      false|      Activity|       2014-03-20 22:40:15|               0.0|       0.0|\n",
            "|176501|5601798bcd03c9af2...|1092929|         payment|2015-09-22 22:53:47|     Utilities  late|      false|  Unclassified|       2015-09-22 22:53:47|               0.0|       0.0|\n",
            "|179838|5388f690d546b8434...| 966175|         payment|2014-05-31 04:22:20|       Lake house!!!|      false|      Activity|       2014-05-31 04:22:20|               0.0|       0.0|\n",
            "|188058|53a664d77d0b0354e...|1569018|         payment|2014-06-22 12:08:39|            🍹🍹🍹🍹|      false|          Food|       2014-06-22 12:08:39|               0.0|       0.0|\n",
            "|214552|56e76416cd03c9af2...| 382056|         payment|2016-03-15 08:23:33|        Bloy madness|      false|      Activity|       2016-03-15 08:23:33|               0.0|       0.0|\n",
            "|225635|56bd7179cd03c9af2...| 333984|          charge|2016-02-12 13:45:29|Prom tickets (lim...|      false|          Cash|       2016-02-12 13:45:29|               0.0|       0.0|\n",
            "|244729|5384cee2d546b8434...| 505128|         payment|2014-05-28 00:44:00|    🚙 the euro whip|      false|  Unclassified|       2014-05-28 00:44:00|               0.0|       0.0|\n",
            "|261285|535d6027d546b8434...| 630062|         payment|2014-04-28 02:53:11|It's Britney pitc...|      false|        People|       2014-04-28 02:53:11|               0.0|       0.0|\n",
            "|265265|56262bbacd03c9af2...|1836209|         payment|2015-10-20 18:55:38|           Cuse game|      false|      Activity|       2015-10-20 18:55:38|               0.0|       0.0|\n",
            "|281332|563d7d40cd03c9af2...| 472092|         payment|2015-11-07 12:25:36|              3 pins|      false|  Unclassified|       2015-11-07 12:25:36|               0.0|       0.0|\n",
            "|287497|578c048323e064eac...| 251573|         payment|2016-07-18 05:19:47|    Cava 🍲 (not 🍾)|      false|          Food|       2016-07-18 05:19:47|               0.0|       0.0|\n",
            "|293958|5650cbe0cd03c9af2...| 411945|         payment|2015-11-22 03:54:08|              Target|      false|          Food|       2015-11-22 03:54:08|               0.0|       0.0|\n",
            "|313680|56eb9eebcd03c9af2...|1057194|         payment|2016-03-18 13:23:39|               Vedas|      false|  Unclassified|       2016-03-18 13:23:39|               0.0|       0.0|\n",
            "|335634|5623483ccd03c9af2...| 944224|         payment|2015-10-18 14:20:28|           Beaumonts|      false|  Unclassified|       2015-10-18 14:20:28|               0.0|       0.0|\n",
            "|337862|53e691827d0b0354e...| 337721|         payment|2014-08-10 04:24:17|                  🍱|      false|          Food|       2014-08-10 04:24:17|               0.0|       0.0|\n",
            "+------+--------------------+-------+----------------+-------------------+--------------------+-----------+--------------+--------------------------+------------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAFhpCrdD3LS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dynamic_spend_profile=spend_timeperiod.select('user1','timeperiod','spend_category')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_q8_-BZQGOzv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "d5ae4e8f-e2cc-4280-940b-1d671231777b"
      },
      "source": [
        "# Check levels \n",
        "\n",
        "levels = [x for x in chain(*dynamic_spend_profile.select('spend_category').distinct().collect())]\n",
        "levels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Food',\n",
              " 'Event',\n",
              " 'Travel',\n",
              " 'Activity',\n",
              " 'Utility',\n",
              " 'Cash',\n",
              " 'Illegal/Sarcasm',\n",
              " 'Unclassified',\n",
              " 'People',\n",
              " 'Transportation']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cK-4x7PHApc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "b042c31e-5370-4669-9852-c49a97a6595b"
      },
      "source": [
        "# Check count of each category for each user for each time period\n",
        "pivoted = dynamic_spend_profile.groupBy(\"user1\",\"timeperiod\").pivot('spend_category', levels).count()\n",
        "pivoted = pivoted.na.fill(0)\n",
        "pivoted.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+----------+----+-----+------+--------+-------+----+---------------+------------+------+--------------+\n",
            "| user1|timeperiod|Food|Event|Travel|Activity|Utility|Cash|Illegal/Sarcasm|Unclassified|People|Transportation|\n",
            "+------+----------+----+-----+------+--------+-------+----+---------------+------------+------+--------------+\n",
            "|118628|       0.0|   0|    0|     0|       0|      0|   0|              0|           1|     0|             0|\n",
            "|126753|       0.0|   0|    0|     0|       1|      0|   0|              0|           0|     0|             0|\n",
            "|137147|       0.0|   1|    0|     0|       0|      0|   0|              0|           0|     0|             0|\n",
            "|150285|       0.0|   0|    0|     0|       0|      0|   0|              0|           1|     0|             0|\n",
            "|166578|       0.0|   0|    0|     0|       0|      0|   0|              0|           1|     0|             0|\n",
            "|174215|       0.0|   0|    0|     0|       1|      0|   0|              0|           0|     0|             0|\n",
            "|176501|       0.0|   0|    0|     0|       0|      0|   0|              0|           1|     0|             0|\n",
            "|179838|       0.0|   0|    0|     0|       1|      0|   0|              0|           0|     0|             0|\n",
            "|188058|       0.0|   1|    0|     0|       0|      0|   0|              0|           0|     0|             0|\n",
            "|214552|       0.0|   0|    0|     0|       1|      0|   0|              0|           0|     0|             0|\n",
            "|225635|       0.0|   0|    0|     0|       0|      0|   1|              0|           0|     0|             0|\n",
            "|244729|       0.0|   0|    0|     0|       0|      0|   0|              0|           1|     0|             0|\n",
            "|261285|       0.0|   0|    0|     0|       0|      0|   0|              0|           0|     1|             0|\n",
            "|265265|       0.0|   0|    0|     0|       1|      0|   0|              0|           0|     0|             0|\n",
            "|281332|       0.0|   0|    0|     0|       0|      0|   0|              0|           1|     0|             0|\n",
            "|287497|       0.0|   1|    0|     0|       0|      0|   0|              0|           0|     0|             0|\n",
            "|293958|       0.0|   1|    0|     0|       0|      0|   0|              0|           0|     0|             0|\n",
            "|313680|       0.0|   0|    0|     0|       0|      0|   0|              0|           1|     0|             0|\n",
            "|335634|       0.0|   0|    0|     0|       0|      0|   0|              0|           1|     0|             0|\n",
            "|337862|       0.0|   1|    0|     0|       0|      0|   0|              0|           0|     0|             0|\n",
            "+------+----------+----+-----+------+--------+-------+----+---------------+------------+------+--------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwBYZnxZIQ28",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "243524eb-6708-4be4-a356-225695b5bec1"
      },
      "source": [
        "pivoted.filter(pivoted.timeperiod<=12).orderBy(\"user1\",\"timeperiod\").show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+----------+----+-----+------+--------+-------+----+---------------+------------+------+--------------+\n",
            "|user1|timeperiod|Food|Event|Travel|Activity|Utility|Cash|Illegal/Sarcasm|Unclassified|People|Transportation|\n",
            "+-----+----------+----+-----+------+--------+-------+----+---------------+------------+------+--------------+\n",
            "|  164|       0.0|   0|    0|     0|       0|      0|   0|              0|           1|     0|             0|\n",
            "|  669|       0.0|   0|    0|     0|       0|      1|   0|              0|           0|     0|             0|\n",
            "| 1649|       0.0|   1|    0|     0|       0|      0|   0|              0|           0|     0|             0|\n",
            "| 3664|       0.0|   0|    0|     0|       0|      0|   0|              0|           1|     0|             0|\n",
            "| 4262|       0.0|   1|    0|     0|       0|      0|   0|              0|           0|     0|             0|\n",
            "| 4536|       0.0|   1|    0|     0|       0|      0|   0|              0|           0|     0|             0|\n",
            "| 4541|       0.0|   0|    0|     0|       0|      0|   0|              0|           1|     0|             0|\n",
            "| 6680|       0.0|   1|    0|     0|       0|      0|   0|              0|           0|     0|             0|\n",
            "|21768|       0.0|   1|    0|     0|       0|      0|   0|              0|           0|     0|             0|\n",
            "|21958|       0.0|   0|    0|     0|       0|      0|   1|              0|           0|     0|             0|\n",
            "|22792|       0.0|   1|    0|     0|       0|      0|   0|              0|           0|     0|             0|\n",
            "|25535|       0.0|   0|    0|     0|       0|      0|   0|              0|           1|     0|             0|\n",
            "|28429|       0.0|   0|    0|     0|       0|      0|   0|              0|           1|     0|             0|\n",
            "|29489|       0.0|   1|    0|     0|       0|      0|   0|              0|           0|     0|             0|\n",
            "|29639|       0.0|   0|    0|     0|       0|      1|   0|              0|           0|     0|             0|\n",
            "|30056|       0.0|   0|    0|     0|       0|      0|   0|              0|           1|     0|             0|\n",
            "|33757|       0.0|   0|    0|     0|       0|      0|   0|              0|           1|     0|             0|\n",
            "|33871|       0.0|   0|    0|     0|       0|      0|   0|              0|           1|     0|             0|\n",
            "|34026|       0.0|   0|    0|     0|       0|      0|   0|              0|           1|     0|             0|\n",
            "|34116|       0.0|   0|    0|     0|       0|      0|   0|              0|           1|     0|             0|\n",
            "+-----+----------+----+-----+------+--------+-------+----+---------------+------------+------+--------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sd5szqwcIk0M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pivoted=pivoted.filter(pivoted.timeperiod<=12).orderBy(\"user1\",\"timeperiod\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mlt78vsYKGEE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a column to sum \n",
        "pivoted = pivoted.withColumn('Sum', col(\"Food\") + col(\"Event\") + col(\"Travel\") + col(\"Activity\") + col(\"Utility\") + col(\"Unclassified\") + col(\"People\") + col(\"Transportation\")+col(\"Illegal/Sarcasm\")+col(\"Cash\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xsx6bRNkKXc7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#then change counts to proportion \n",
        "revised_pivot = pivoted.withColumn('Food', (col(\"Food\")*100)/(col('Sum')))\\\n",
        ".withColumn('Event', (col(\"Event\")*100)/(col('Sum')))\\\n",
        ".withColumn('Travel', (col(\"Travel\")*100)/(col('Sum')))\\\n",
        ".withColumn('Activity', (col(\"Activity\")*100)/(col('Sum')))\\\n",
        ".withColumn('Utility', (col(\"Utility\")*100)/(col('Sum')))\\\n",
        ".withColumn('Unclassified', (col(\"Unclassified\")*100)/(col('Sum')))\\\n",
        ".withColumn('People', (col(\"People\")*100)/(col('Sum')))\\\n",
        ".withColumn('Transportation', (col(\"Transportation\")*100)/(col('Sum')))\\\n",
        ".withColumn('Illegal/Sarcasm', (col(\"Illegal/Sarcasm\")*100)/(col('Sum')))\\\n",
        ".withColumn('Cash', (col(\"Cash\")*100)/(col('Sum')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qk7deMjRMeK_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "user_dynamic_spend=revised_pivot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YTJR2_kawhr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "792fda12-3f09-40e0-cbba-74777969fe25"
      },
      "source": [
        "user_dynamic_spend.groupBy('timeperiod').avg('Food',\n",
        " 'Event',\n",
        " 'Travel',\n",
        " 'Activity',\n",
        " 'Utility',\n",
        " 'Cash',\n",
        " 'Illegal/Sarcasm',\n",
        " 'Unclassified',\n",
        " 'People',\n",
        " 'Transportation').orderBy(\"timeperiod\").show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+------------------+------------------+------------------+------------------+------------------+------------------+--------------------+------------------+------------------+-------------------+\n",
            "|timeperiod|         avg(Food)|        avg(Event)|       avg(Travel)|     avg(Activity)|      avg(Utility)|         avg(Cash)|avg(Illegal/Sarcasm)| avg(Unclassified)|       avg(People)|avg(Transportation)|\n",
            "+----------+------------------+------------------+------------------+------------------+------------------+------------------+--------------------+------------------+------------------+-------------------+\n",
            "|       0.0|19.355762933857235|1.9144973804846104|1.6975687622789783| 8.114358218729535| 7.979289456450557|0.6119024230517355|  2.9653732809430253| 42.33894073346431| 10.19155206286837| 4.8307547478716435|\n",
            "|       1.0|23.711340206185568| 4.123711340206185|               0.0|  8.24742268041237| 6.185567010309279|1.0309278350515463|  1.0309278350515463| 41.23711340206186|10.309278350515465|  4.123711340206185|\n",
            "|       2.0| 26.31578947368421|3.1578947368421053|1.0526315789473684|5.2631578947368425| 6.315789473684211|1.0526315789473684|  4.2105263157894735| 38.94736842105263| 8.421052631578947| 5.2631578947368425|\n",
            "|       3.0|23.076923076923077| 1.098901098901099| 2.197802197802198| 6.593406593406593| 8.791208791208792| 1.098901098901099|  3.2967032967032965| 38.46153846153846|  9.89010989010989| 5.4945054945054945|\n",
            "|       4.0|            15.625|1.0416666666666667|1.0416666666666667| 4.166666666666667| 7.291666666666667|               0.0|                 0.0|51.041666666666664|              6.25| 13.541666666666666|\n",
            "|       5.0|              20.0| 4.615384615384615|1.5384615384615385|12.307692307692308|  9.23076923076923|               0.0|  1.5384615384615385| 43.07692307692308| 4.615384615384615|  3.076923076923077|\n",
            "|       6.0|16.666666666666668|               0.0|               0.0| 5.555555555555555| 6.944444444444445|2.7777777777777777|   4.166666666666667| 47.22222222222222| 8.333333333333334|  8.333333333333334|\n",
            "|       7.0|20.967741935483872|1.6129032258064515|1.6129032258064515|  9.67741935483871| 6.451612903225806|               0.0|   3.225806451612903| 33.87096774193548|12.903225806451612|   9.67741935483871|\n",
            "|       8.0| 19.29824561403509| 3.508771929824561|               0.0| 7.017543859649122|5.2631578947368425|               0.0|                 0.0|  54.3859649122807|5.2631578947368425| 5.2631578947368425|\n",
            "|       9.0|13.636363636363637| 2.272727272727273| 2.272727272727273|               0.0| 6.818181818181818|               0.0|  11.363636363636363| 47.72727272727273| 6.818181818181818|  9.090909090909092|\n",
            "|      10.0|12.820512820512821|               0.0| 5.128205128205129|7.6923076923076925| 17.94871794871795|               0.0|                 0.0|28.205128205128204| 17.94871794871795| 10.256410256410257|\n",
            "|      11.0| 35.13513513513514|               0.0| 8.108108108108109| 10.81081081081081|2.7027027027027026|               0.0|                 0.0|24.324324324324323|13.513513513513514|  5.405405405405405|\n",
            "|      12.0|22.727272727272727|               0.0|               0.0| 6.818181818181818| 6.818181818181818| 4.545454545454546|   2.272727272727273| 36.36363636363637| 6.818181818181818| 13.636363636363637|\n",
            "+----------+------------------+------------------+------------------+------------------+------------------+------------------+--------------------+------------------+------------------+-------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGeILlWGfiyj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "avg_spend_timeperiod=user_dynamic_spend.groupBy('timeperiod').avg('Food',\n",
        " 'Event',\n",
        " 'Travel',\n",
        " 'Activity',\n",
        " 'Utility',\n",
        " 'Cash',\n",
        " 'Illegal/Sarcasm',\n",
        " 'Unclassified',\n",
        " 'People',\n",
        " 'Transportation').orderBy(\"timeperiod\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j59eJWBQdLgY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "d25b3b81-329f-48a1-8191-edda6cb54cc5"
      },
      "source": [
        "user_dynamic_spend.groupBy('timeperiod').agg(stddev('Food'),\n",
        " stddev('Event'),stddev('Travel'),\n",
        " stddev('Activity'),\n",
        " stddev('Utility'),\n",
        " stddev('Cash'),\n",
        "stddev('Illegal/Sarcasm'),\n",
        "stddev('Unclassified'),\n",
        " stddev('People'),\n",
        " stddev('Transportation')\n",
        " ).orderBy(\"timeperiod\").show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+------------------+------------------+-------------------+---------------------+--------------------+------------------+----------------------------+-------------------------+-------------------+---------------------------+\n",
            "|timeperiod| stddev_samp(Food)|stddev_samp(Event)|stddev_samp(Travel)|stddev_samp(Activity)|stddev_samp(Utility)| stddev_samp(Cash)|stddev_samp(Illegal/Sarcasm)|stddev_samp(Unclassified)|stddev_samp(People)|stddev_samp(Transportation)|\n",
            "+----------+------------------+------------------+-------------------+---------------------+--------------------+------------------+----------------------------+-------------------------+-------------------+---------------------------+\n",
            "|       0.0|39.492181881946394|13.701719507126509|  12.91220590440474|    27.30208082893058|  27.089953943644236| 7.798529499131703|          16.960172501298246|        49.39508470452833|  30.24726263464328|          21.43339795493046|\n",
            "|       1.0| 42.75218287869605|19.987109247774534|                0.0|   27.651488404723594|   24.21446274823307|10.153461651336189|          10.153461651336192|        49.48185130173913| 30.565967194176334|         19.987109247774526|\n",
            "|       2.0|44.268345916331384|17.580402404729135| 10.259783520851538|   22.448148540588733|  24.453717969020552|10.259783520851542|          20.189472049435526|       49.021785956044766| 27.917652063748477|          22.44814854058873|\n",
            "|       3.0| 42.36592728681616|10.482848367219182| 14.742395237484898|   24.954170447254356|  28.473580925918164|10.482848367219182|          17.953950048508055|       48.919959047022836|  30.01830943106059|         22.913544575873132|\n",
            "|       4.0| 36.49981975442157|10.206207261596576| 10.206207261596573|   20.087527770480484|  26.136450315368364|               0.0|                         0.0|        50.25156015732859| 24.333213169614375|          34.39642474280894|\n",
            "|       5.0| 40.31128874149275| 21.14510308831302| 12.403473458920846|    33.10821417947284|  29.171245061901942|               0.0|          12.403473458920846|       49.903753519997025| 21.145103088313018|         17.403580533459643|\n",
            "|       6.0| 37.52933125204008|               0.0|                0.0|   23.066889195376994|    25.5992346325338| 16.54887956057706|          20.122862059478244|        50.27311947224928| 27.832496965309158|         27.832496965309165|\n",
            "|       7.0| 41.04015136821201|12.700012700019048| 12.700012700019046|   29.806355463524554|  24.767560367065308|               0.0|          17.812704394997528|       47.713449274508264|  33.79723030529027|          29.80635546352455|\n",
            "|       8.0|39.814733862999184|18.563715383027592|                0.0|   25.771309648570565|  22.528177844479153|               0.0|                         0.0|         50.2500015586229| 22.528177844479146|         22.528177844479146|\n",
            "|       9.0| 34.71417571787742|15.075567228888177| 15.075567228888177|                  0.0|  25.497170592935273|               0.0|          32.103822064055045|       50.525776813638046|  25.49717059293527|          29.08033634511526|\n",
            "|      10.0| 33.86884284293705|               0.0| 22.345586503095895|    26.99527623995085|   38.87764119844715|               0.0|                         0.0|         45.5880752454893|  38.87764119844715|         30.735474060472818|\n",
            "|      11.0|48.397751418246095|               0.0| 27.672473069203008|    31.48000938676783|  16.439898730535724|               0.0|                         0.0|        43.49588362008401|  34.65834966066909|         22.924343513512564|\n",
            "|      12.0|42.391510578572124|               0.0|                0.0|    25.49717059293527|   25.49717059293527|21.070705494148548|           15.07556722888818|        48.66070995624795|  25.49717059293527|          34.71417571787742|\n",
            "+----------+------------------+------------------+-------------------+---------------------+--------------------+------------------+----------------------------+-------------------------+-------------------+---------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAv8WzgIgt_R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stddev_spend_timeperiod=user_dynamic_spend.groupBy('timeperiod').agg(stddev('Food'),\n",
        " stddev('Event'),stddev('Travel'),\n",
        " stddev('Activity'),\n",
        " stddev('Utility'),\n",
        " stddev('Cash'),\n",
        "stddev('Illegal/Sarcasm'),\n",
        "stddev('Unclassified'),\n",
        " stddev('People'),\n",
        " stddev('Transportation')\n",
        " ).orderBy(\"timeperiod\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UFV39bUMw7r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "avg_spend =avg_spend_timeperiod.toPandas()\n",
        "std_spend = stddev_spend_timeperiod.toPandas()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wUkgp5JMoZ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def plot_avg(y,x,err):\n",
        "    plt.plot(y,x, 'or')\n",
        "    plt.plot( y,x+err, '-', color='gray')\n",
        "    plt.plot( y,x-err, '-', color='gray')\n",
        "    plt.xlabel(\"Months\")\n",
        "    plt.ylabel(\"Std against the mean\")\n",
        "    plt.title(\"Spending profile of avg customer across 12 months\")\n",
        "    plt.fill_between(y,x+err,x-err, facecolor ='grey',alpha=0.30)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3d5rasxMwFI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_avg(avg_spend['timeperiod'],avg_spend['avg(Activity)'],std_spend['stddev_samp(Activity)'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0ixxWbICF61",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_avg(avg_spend['timeperiod'],avg_spend['avg(Food)'],std_spend['stddev_samp(Food)'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkUIki04CJgB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_avg(avg_spend['timeperiod'],avg_spend['avg(Event)'],std_spend['stddev_samp(Event)'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24xfAKB8CLwL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_avg(avg_spend['timeperiod'],avg_spend['avg(Cash)'],std_spend['stddev_samp(Cash)'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtIZ4mfqCMZW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_avg(avg_spend['timeperiod'],avg_spend['avg(Illegal/Sarcasm)'],std_spend['stddev_samp(Illegal/Sarcasm)'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_3xHJ_yCMm4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_avg(avg_spend['timeperiod'],avg_spend['avg(People)'],std_spend['stddev_samp(People)'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPJUhgFXCMiU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_avg(avg_spend['timeperiod'],avg_spend['avg(Transportation)'],std_spend['stddev_samp(Transportation)'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyqPZkZdzXdy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_avg(avg_spend['timeperiod'],avg_spend['avg(Travel)'],std_spend['stddev_samp(Travel)'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uxjV9GTzZuG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_avg(avg_spend['timeperiod'],avg_spend['avg(Unclassified)'],std_spend['stddev_samp(Unclassified)'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHAIXDVdzbod",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_avg(avg_spend['timeperiod'],avg_spend['avg(Utility)'],std_spend['stddev_samp(Utility)'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43-JtE1_Gq7-",
        "colab_type": "text"
      },
      "source": [
        "# Social Network Analytics\n",
        "Let's now look at a user's social network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMHn6VyzGtSp",
        "colab_type": "text"
      },
      "source": [
        "## Analyzing User's Social Network \n",
        "Write a script to find a user’s friends and friends of friends ( Friend definition: A user’s friend is someone who has transacted with the user , either sending money to the user or receiving money from the user). Describe your algorithm and calculate its computational complexity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaJ-0LCJoUrV",
        "colab_type": "text"
      },
      "source": [
        "We have built a SQL algorithm to find friends and friends-of-friends for users in this table. It works in the following way:\n",
        "\n",
        "Step 1 : Degree 1 - all unique pairs of users in the data\n",
        "\n",
        "Step 2:  Degree 2 - 3 cases of friends of friends when user 1 and user 2 have mutual friend\n",
        "\n",
        "Step 3: Minimize the degree to remove dependency. For instance - \n",
        "\n",
        "       a - b , b - c , a - c  are 3 user1 - user2 pairs. Here, a - c will be assigned both degrees 1 and 2 , but we choose 1.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5PMJyJoXJvl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0dc190e9-0b43-4374-8415-f6dac4a5c878"
      },
      "source": [
        "#creating table\n",
        "#sampleParq = spark.createDataFrame(input_parq.take(5000))\n",
        "input_parq.createOrReplaceTempView(\"df_view\")\n",
        "spark.sql(\"drop table user_network\")\n",
        "spark.sql(\"create table user_network as select * from df_view\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLcq2omHulWl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "55a747bb-6a61-4148-af8a-d98a4cb44b75"
      },
      "source": [
        "\n",
        "spark.sql(\"\"\"\n",
        "create table friends as \n",
        "select user,user_friend,min(degree) as degree \n",
        "from \n",
        "(select distinct user,user_friend,degree from \n",
        "(select distinct a.user1 as user, b.user2 as user_friend,1 as degree from user_network a,user_network b where a.user2 = b.user1 \n",
        "and a.user1 = b.user2)\n",
        "union \n",
        "(select distinct a.user1 as user, b.user2 as user_friend,2 as degree from user_network a inner join user_network b on a.user2 = b.user1 \n",
        "and a.user1 <> b.user1)\n",
        "union \n",
        "(select distinct a.user2 as user, b.user2 as user_friend,2 as degree from user_network a inner join user_network b on a.user1 = b.user1)\n",
        "union \n",
        "(select distinct a.user2 as user, b.user1 as user_friend,2 as degree from user_network a inner join user_network b on a.user1 = b.user2)\n",
        ") p where user <> user_friend\n",
        "group by user,user_friend \n",
        "\"\"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jWnRNPI7WQz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "outputId": "08a2e9b9-7b42-42ba-d00d-5e5375851aa9"
      },
      "source": [
        "spark.sql(\"select * from friends where degree = 2 limit 10\").show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+-----------+------+\n",
            "|   user|user_friend|degree|\n",
            "+-------+-----------+------+\n",
            "|1176716|    3454184|     2|\n",
            "| 604900|    3066522|     2|\n",
            "|5878270|    5491686|     2|\n",
            "|2587227|     384692|     2|\n",
            "|1655842|     861019|     2|\n",
            "| 574093|    3032548|     2|\n",
            "|4613243|    2816253|     2|\n",
            "|2089296|    2251624|     2|\n",
            "|2269757|    2306248|     2|\n",
            "| 732137|     715531|     2|\n",
            "+-------+-----------+------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAWk3c7mpi91",
        "colab_type": "text"
      },
      "source": [
        "This algorithm is extremely complex computationally. To calculate the complexity of SQL query , I am calculating the following:\n",
        "\n",
        "i) No. of join conditions - 6\n",
        "ii) No. of unions - 3\n",
        "iii) Group by - 1\n",
        "\n",
        "Thus, 10 is the no. of operations in more than 7mn records dataset. Thus, it is almost infeasible to perform this algorithm on Spark. SQL\n",
        "Therefore, we should use GraphX for this type of analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gt2JYTDaS4MV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Finding Friends\n",
        "input_parq.createOrReplaceTempView(\"df5\")\n",
        "spark.sql(\"\"\"\n",
        "select mean(no_fr) from\n",
        "(select user,sum(friends) no_fr from\n",
        "(select * from\n",
        "(select user1 user,count(distinct user2) friends from df5 group by 1) \n",
        "union\n",
        "(select user2 user, count(distinct user1) friends from df5) group by 1)\n",
        "group by 1) p\n",
        "\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDeslhVGGviz",
        "colab_type": "text"
      },
      "source": [
        "## Building User's Dynamic Social Network Profile\n",
        "Now, that you have the list of each user’s friends and friends of friends, you are in position to calculate many social network variables. Use the dynamic analysis from before, and calculate the following social network metrics across a user’s lifetime in Venmo (from 0 up to 12\n",
        "months).\n",
        "\n",
        "i) Number of friends and number of friends of friends [very easy, 2pts].<br><br>\n",
        "ii) Clustering coefficient of a user's network [easy, 3 pts]. ( Hint : the easiest way to calculate this is to program it yourselves. Alternatively, you can use “networkx” python package. The latter approach will slow down your script significantly).<br><br>\n",
        "iii) Calculate the page rank of each user (hard, 5 pts). ( Hint : First of all, you need to use GraphX to do this. Moreover, notice that page rank is a global social network metric. If you go ahead and calculate the page rank for each user at each of her lifetime points, you will soon realize it will be a dead end. Can you think of a smart way to do this? )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCaWor-SwIJq",
        "colab_type": "text"
      },
      "source": [
        "i) Using the above algorithm to find friends and friends-of-friends for every lifetime point from 0 - 12:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qm2teekX_D1D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "62e40d1a-e763-4438-bd23-4d7b74524185"
      },
      "source": [
        "input_parq.createOrReplaceTempView(\"df_view\")\n",
        "#spark.sql(\"drop table timeperiod_nw\")\n",
        "spark.sql(\"\"\"create table timeperiod_nw as select user1,user2,CAST( MONTHS_BETWEEN(datetime, \n",
        "FIRST_VALUE(datetime) OVER (PARTITION BY user1 ORDER BY datetime)) as INT) as timperiod\n",
        "from df_view\n",
        "\"\"\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdkfwoIWAiES",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "outputId": "c1fa0db2-a1ad-4f3b-eaf0-7efe0a91ee5e"
      },
      "source": [
        "spark.sql(\"select * from timeperiod_nw limit 10\").show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+-------+---------+\n",
            "| user1|  user2|timperiod|\n",
            "+------+-------+---------+\n",
            "| 73362|1754162|        0|\n",
            "|133739| 284618|        0|\n",
            "|140174|1982815|        0|\n",
            "|158508| 185417|        0|\n",
            "|183620| 958979|        0|\n",
            "|192675| 186066|        0|\n",
            "|197072|3803746|        0|\n",
            "|204836|1225124|        0|\n",
            "|204963|5509299|        0|\n",
            "|215019|2499164|        0|\n",
            "+------+-------+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlKPO5LDxevT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a7c0e66c-1eb4-42b0-e11f-209a1180efb9"
      },
      "source": [
        "#time intervals\n",
        "a = [0,1,2,3,4,5,6,7,8,9,10,11,12]\n",
        "#defining schema\n",
        "#schema = users.schema\n",
        "for i in a:\n",
        "  connquery = f\"\"\"(select distinct user,user_friend,timperiod, degree \n",
        "from \n",
        "(select user,user_friend,timperiod,degree from \n",
        "(select distinct a.user1 as user, b.user2 as user_friend,a.timperiod,1 as degree from timeperiod_nw a,timeperiod_nw b where a.user2 = b.user1 and a.user1 = b.user2 and a.timperiod = {i}\n",
        "and b.timperiod  = {i})\n",
        "union \n",
        "(select distinct a.user1 as user, b.user2 as user_friend,a.timperiod,2 as degree from timeperiod_nw a inner join timeperiod_nw b on a.user2 = b.user1 and a.user1 <> b.user1 and a.timperiod = {i}\n",
        "and b.timperiod  = {i})\n",
        "union \n",
        "(select distinct a.user2 as user, b.user2 as user_friend,a.timperiod,2 as degree from timeperiod_nw a inner join timeperiod_nw b on a.user1 = b.user1 and a.timperiod = {i}\n",
        "and b.timperiod  = {i})\n",
        "union \n",
        "(select distinct a.user2 as user, b.user1 as user_friend,a.timperiod,2 as degree from timeperiod_nw a inner join timeperiod_nw b on a.user1 = b.user2 and a.timperiod = {i}\n",
        "and b.timperiod  = {i})\n",
        ") p \n",
        ")\"\"\" \n",
        "  if i==0:\n",
        "    df = spark.sql(connquery)\n",
        "    dff = df\n",
        "  else:\n",
        "    df = spark.sql(connquery)\n",
        "    dff = dff.union(df)\n",
        "\n",
        "connections = dff.drop_duplicates()\n",
        "connections.createOrReplaceTempView(\"connections\")\n",
        "\n",
        "#creating final table\n",
        "spark.sql(\"drop table network\")\n",
        "spark.sql(\"create table network as select * from connections\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5bJPV4fCIBb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d813e1e0-b6f1-4f3a-d7c6-b68422479ca2"
      },
      "source": [
        "#Calculating friends and friends of friends\n",
        "spark.sql(\"\"\" create table network_number as\n",
        "select user,timperiod,\n",
        "sum(case when degree = 1 then 1 else 0 end) as friends,\n",
        "sum(case when degree = 2 then 1 else 0 end) as fof \n",
        "from network group by user,timperiod\n",
        "\"\"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmbjDyfOOd37",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "a10d4b8d-0334-471d-9c29-32667bc63fc7"
      },
      "source": [
        "input_parq.createOrReplaceTempView(\"df6\")\n",
        "user1_f = spark.sql(\"\"\"select user1 user,count(distinct user2) friends from df6 group by 1\"\"\")\n",
        "user2_f = spark.sql(\"\"\"select user2 user,count(distinct user1) friends from df6 group by 1\"\"\")\n",
        "user1_f.createOrReplaceTempView(\"df61\")\n",
        "user2_f.createOrReplaceTempView(\"df62\")\n",
        "user_all = spark.sql(\"\"\"\n",
        "select user,sum(friends) total_friends from\n",
        "(select * from df61 union\n",
        "select * from df62 ) as r group by 1\n",
        "\"\"\")\n",
        "user_all.createOrReplaceTempView(\"df7\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------------+\n",
            "|max(total_friends)|\n",
            "+------------------+\n",
            "|                15|\n",
            "+------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTDOIwGopP5z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spark.sql(\"select * from network_number limit 10\").show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJ32AVU32sN1",
        "colab_type": "text"
      },
      "source": [
        "ii) Finding clustering coefficient"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4bjpNhy2rej",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c3c76af9-3ffe-4d88-915d-2536363bdda1"
      },
      "source": [
        "#creating a unified table \n",
        "spark.sql(\"\"\"create table connections_all as\n",
        "(\n",
        "select a.user, a.user_friend, b.user_friend as mutual_friend, a.timperiod\n",
        "from network as a\n",
        "left join network as b\n",
        "on a.user_friend = b.user\n",
        "where a.degree = 1\n",
        "order by user, user_friend\n",
        ")\"\"\")\n",
        "#"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpQhlHsW9Y-J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "outputId": "6c5e3f61-ad6a-4375-ca49-5e0263fdf229"
      },
      "source": [
        "spark.sql(\"\"\"select count(*) from network\"\"\").show()\n",
        "spark.sql(\"\"\"select degree,count(user,user_friend) as friends\n",
        "from network where timperiod = 0 group by degree\"\"\").show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+\n",
            "|count(1)|\n",
            "+--------+\n",
            "|   52065|\n",
            "+--------+\n",
            "\n",
            "+------+-------+\n",
            "|degree|friends|\n",
            "+------+-------+\n",
            "|     1|    196|\n",
            "|     2|  51096|\n",
            "+------+-------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSqqSGoduofP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "outputId": "015dc2ab-6160-4679-9c82-2c3d11cfa2f3"
      },
      "source": [
        "spark.sql(\"select count(*) from connections_all where (user != user_friend) or (user_friends != mutual_friend) or (user != )\").show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+-----------+-------------+---------+\n",
            "|   user|user_friend|mutual_friend|timperiod|\n",
            "+-------+-----------+-------------+---------+\n",
            "|  36957|      36957|        36957|        1|\n",
            "|  84249|      84249|         3664|        1|\n",
            "|  84249|      84249|        84249|        1|\n",
            "|4164806|    4164806|      4164806|        1|\n",
            "|4164806|    4164806|      3558442|        1|\n",
            "|1566754|    1566754|      2287931|        1|\n",
            "|1566754|    1566754|      1566754|        1|\n",
            "| 422164|     422164|       676937|        1|\n",
            "| 422164|     422164|       422164|        1|\n",
            "|1037500|    1037500|      1003356|        1|\n",
            "+-------+-----------+-------------+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEVSKJkVRBJs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql.functions import array, collect_list, flatten, udf, desc, asc\n",
        "from pyspark.sql.types import StringType, ArrayType, IntegerType, FloatType, MapType\n",
        "\n",
        "mutual_friend = collect_list(\"mutual_friend\").alias(\"mutual_friend\")\n",
        "user_friend = collect_list(\"user_friend\").alias(\"user_friend\")\n",
        "users_df = spark.sql('select user, user_friend, mutual_friend from connections_all where timperiod = \"%month\"')\n",
        "user_network = users_df.groupBy(\"user\",\"user_friend\").agg(mutual_friend).orderBy(asc(\"user\"), asc('user_friend'))\n",
        "user_network_coalesced = user_network.groupBy('user').agg(mutual_friend,user_friend).orderBy(asc('user'))\n",
        "\n",
        "append_udf = udf(lambda x,y: {a:b for a,b in zip(x,y)}, MapType(IntegerType(),ArrayType(IntegerType())))\n",
        "# append_udf = udf(lambda x,y: {a:b for a,b in zip(x,y)}, StringType())\n",
        "all_conn = user_network_coalesced.withColumn('friends_dict', append_udf('user_friend','mutual_friend')).select('user','friends_dict')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeLa-xNYRPoN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = user_network_coalesced.toPandas()\n",
        "df[['user_friend','mutual_friend']].head().apply(lambda x: coeff(x['user_friend'],x['mutual_friend']), axis = 1)\n",
        "def coeff(x,y):\n",
        "  '''\n",
        "  formula for clustering coeff:  number of connections among friends/(k*(k-1)/2)\n",
        "  '''\n",
        "#   friends = x.keys()\n",
        "  k = len(x)\n",
        "  den = k*(k-1)/2\n",
        "  if den == 0:\n",
        "    den = 1\n",
        "  num = []\n",
        "  for friend, mutual in zip(x,y):\n",
        "    x_conns = [a+friend for a in x if a in mutual and a+friend not in num]\n",
        "    num = num+x_conns\n",
        "  return (len(num)/den)\n",
        "clustering_coeff_udf=udf(lambda x,y: coeff(x,y), FloatType())\n",
        "clustering_coeff = user_network_coalesced.withColumn('ClusteringCoef',clustering_coeff_udf('user_friend','mutual_friend'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIIQgxGORdaK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clustering_coeff.createOrReplaceTempView('clustering_coeff')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxAM8xoave-R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "bc97badf-1b96-4bdd-9048-8169b4f1b8a7"
      },
      "source": [
        "spark.sql(\"create table clustering_coeff as select * from clustering_coeff\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTEM0Um9x5kc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spark.sql(\"select * from clustering_coeff limit 10\").show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpmiQeEP0H2X",
        "colab_type": "text"
      },
      "source": [
        "iii) Implementing PageRank using GraphX"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZ_y2DLRydEl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Creating list of all users from the data with unique identifier\n",
        "from pyspark.sql import SQLContext, Row\n",
        "#sqlContext = SQLContext(sc)\n",
        "input_parq.createOrReplaceTempView(\"tempview_pr\")\n",
        "spark.sql(\n",
        "\"\"\"create table users1 as select distinct user from \n",
        "(select distinct user1 as user from tempview_pr)  \n",
        "  union (select distinct user2 as user from tempview_pr) \"\"\")\n",
        "spark.sql(\n",
        "\"\"\"create table unique_users as select row_number() over (order by user) id, user from users1 \"\"\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-vqVNX54yjP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "96ec889b-c190-4523-8bb1-4f3040a9e907"
      },
      "source": [
        "#input_parq.createOrReplaceTempView(\"tempview_pr\")\n",
        "from pyspark.sql import SQLContext, Row\n",
        "sqlContext = SQLContext(spark)\n",
        "df_nique =  sqlContext.sql(\"select distinct user from users1\")\n",
        "df_e = input_parq.select('user1', 'user2','transaction_type')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.sql.dataframe.DataFrame"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3eXAHUDC_uTy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rename_columns(df, columns):\n",
        "    if isinstance(columns, dict):\n",
        "        for old_name, new_name in columns.items():\n",
        "            df = df.withColumnRenamed(old_name, new_name)\n",
        "        return df\n",
        "    else:\n",
        "        raise ValueError(\"'columns' should be a dict, like {'old_name_1':'new_name_1', 'old_name_2':'new_name_2'}\")\n",
        "#df_e = rename_columns(df_e, {'user1': 'src', 'user2': 'dst'})\n",
        "#df_nique = rename_columns(df_nique, {'id': 'id'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dkyx9ITO8Xrd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install graphframes\n",
        "from graphframes import *\n",
        "g = GraphFrame(df_nique, df_e)\n",
        "display(g.degrees)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSRiFQGAGy5S",
        "colab_type": "text"
      },
      "source": [
        "# Predictive Analytics with MLlib\n",
        "One of the biggest questions in Customer Relationship Management (CRM) is whether you can predict in advance how many times your customers will transact.Here, we will investigate how the different set of metrics that you have created above (text and social) can help us predict the total number of transactions a user will have by the end of their first year in Venmo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqlZ_SXUwBYq",
        "colab_type": "text"
      },
      "source": [
        "## User's total transactions in the first year\n",
        "First, create your dependent variable Y , i.e. the total number of transactions at lifetime point 12. In other words, for every user, you need to count how many transactions s/he had committed during her/his twelve months in Venmo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2J7MB-Ht2hJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "Y_user_timeperiod=spend_timeperiod.groupBy('user1','timeperiod').agg(countDistinct(\"story_id\").alias('transaction_timeperiodwise'),max('datetime').alias('recent_transaction_month'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNT7GEWAR6j9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 941
        },
        "outputId": "a63f0f84-da66-49b6-aacf-28d0953bad07"
      },
      "source": [
        "Y_user_timeperiod.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+----------+--------------------------+------------------------+\n",
            "| user1|timeperiod|transaction_timeperiodwise|recent_transaction_month|\n",
            "+------+----------+--------------------------+------------------------+\n",
            "|118628|       0.0|                         1|     2015-03-11 06:28:18|\n",
            "|126753|       0.0|                         1|     2016-03-27 02:56:48|\n",
            "|137147|       0.0|                         1|     2016-07-26 19:37:05|\n",
            "|150285|       0.0|                         1|     2015-10-17 10:14:15|\n",
            "|166578|       0.0|                         1|     2014-10-18 00:43:44|\n",
            "|174215|       0.0|                         1|     2014-03-20 22:40:15|\n",
            "|176501|       0.0|                         1|     2015-09-22 22:53:47|\n",
            "|179838|       0.0|                         1|     2014-05-31 04:22:20|\n",
            "|188058|       0.0|                         1|     2014-06-22 12:08:39|\n",
            "|214552|       0.0|                         1|     2016-03-15 08:23:33|\n",
            "|225635|       0.0|                         1|     2016-02-12 13:45:29|\n",
            "|244729|       0.0|                         1|     2014-05-28 00:44:00|\n",
            "|261285|       0.0|                         1|     2014-04-28 02:53:11|\n",
            "|265265|       0.0|                         1|     2015-10-20 18:55:38|\n",
            "|281332|       0.0|                         1|     2015-11-07 12:25:36|\n",
            "|287497|       0.0|                         1|     2016-07-18 05:19:47|\n",
            "|293958|       0.0|                         1|     2015-11-22 03:54:08|\n",
            "|313680|       0.0|                         1|     2016-03-18 13:23:39|\n",
            "|335634|       0.0|                         1|     2015-10-18 14:20:28|\n",
            "|337862|       0.0|                         1|     2014-08-10 04:24:17|\n",
            "+------+----------+--------------------------+------------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+------+----------+--------------------------+------------------------+\n",
            "| user1|timeperiod|transaction_timeperiodwise|recent_transaction_month|\n",
            "+------+----------+--------------------------+------------------------+\n",
            "|118628|       0.0|                         1|     2015-03-11 06:28:18|\n",
            "|126753|       0.0|                         1|     2016-03-27 02:56:48|\n",
            "|137147|       0.0|                         1|     2016-07-26 19:37:05|\n",
            "|150285|       0.0|                         1|     2015-10-17 10:14:15|\n",
            "|166578|       0.0|                         1|     2014-10-18 00:43:44|\n",
            "|174215|       0.0|                         1|     2014-03-20 22:40:15|\n",
            "|176501|       0.0|                         1|     2015-09-22 22:53:47|\n",
            "|179838|       0.0|                         1|     2014-05-31 04:22:20|\n",
            "|188058|       0.0|                         1|     2014-06-22 12:08:39|\n",
            "|214552|       0.0|                         1|     2016-03-15 08:23:33|\n",
            "|225635|       0.0|                         1|     2016-02-12 13:45:29|\n",
            "|244729|       0.0|                         1|     2014-05-28 00:44:00|\n",
            "|261285|       0.0|                         1|     2014-04-28 02:53:11|\n",
            "|265265|       0.0|                         1|     2015-10-20 18:55:38|\n",
            "|281332|       0.0|                         1|     2015-11-07 12:25:36|\n",
            "|287497|       0.0|                         1|     2016-07-18 05:19:47|\n",
            "|293958|       0.0|                         1|     2015-11-22 03:54:08|\n",
            "|313680|       0.0|                         1|     2016-03-18 13:23:39|\n",
            "|335634|       0.0|                         1|     2015-10-18 14:20:28|\n",
            "|337862|       0.0|                         1|     2014-08-10 04:24:17|\n",
            "+------+----------+--------------------------+------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MH9M5yNbD-7Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "w = Window.partitionBy('user1')\n",
        "\n",
        "Y_user_timeperiod=Y_user_timeperiod.filter(Y_user_timeperiod.timeperiod<=12).withColumn('Y_totatransactions_12month', f.sum('transaction_timeperiodwise').over(w))\n",
        "\n",
        "Y_user_timeperiod.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XONyrmAhoDj",
        "colab_type": "text"
      },
      "source": [
        "## Recency Frequency Model\n",
        "Create the recency and frequency variables. In CRM, this predictive framework is known as RFM . Here, you don’t have monetary amounts, so we will focus on just RF . Recency refers to the last time a user was active, and frequency is how often a user uses Venmo in a month. You need to compute these metrics across a user’s lifetime in Venmo (from 0 up to 12).\n",
        "\n",
        "For example, if a user has used Venmo twice during her first month in Venmo with the second time being on day x, then her recency in month 1 is “30-x” and her frequency is 30/2=15."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdrMkYt8mTzZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "recency_frequency_tp=Y_user_timeperiod.withColumn(\"frequency\", 30/Y_user_timeperiod.transaction_timeperiodwise)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PY0eALYHoipV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "recency_frequency_tp=recency_frequency_tp.withColumn(\"recency\", 31-f.dayofmonth(f.col(\"recent_transaction_month\")))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3UL6jZKrvD3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "801e0ce4-04cf-4875-db93-f0a3656bfb96"
      },
      "source": [
        "recency_frequency_tp.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+----------+--------------------------+------------------------+--------------------------+---------+-------+\n",
            "| user1|timeperiod|transaction_timeperiodwise|recent_transaction_month|Y_totatransactions_12month|frequency|recency|\n",
            "+------+----------+--------------------------+------------------------+--------------------------+---------+-------+\n",
            "|118628|       0.0|                         1|     2015-03-11 06:28:18|                         1|     30.0|     20|\n",
            "|126753|       0.0|                         1|     2016-03-27 02:56:48|                         1|     30.0|      4|\n",
            "|137147|       0.0|                         1|     2016-07-26 19:37:05|                         1|     30.0|      5|\n",
            "|150285|       0.0|                         1|     2015-10-17 10:14:15|                         1|     30.0|     14|\n",
            "|166578|       0.0|                         1|     2014-10-18 00:43:44|                         1|     30.0|     13|\n",
            "|174215|       0.0|                         1|     2014-03-20 22:40:15|                         1|     30.0|     11|\n",
            "|176501|       0.0|                         1|     2015-09-22 22:53:47|                         1|     30.0|      9|\n",
            "|179838|       0.0|                         1|     2014-05-31 04:22:20|                         1|     30.0|      0|\n",
            "|188058|       0.0|                         1|     2014-06-22 12:08:39|                         1|     30.0|      9|\n",
            "|214552|       0.0|                         1|     2016-03-15 08:23:33|                         1|     30.0|     16|\n",
            "|225635|       0.0|                         1|     2016-02-12 13:45:29|                         1|     30.0|     19|\n",
            "|244729|       0.0|                         1|     2014-05-28 00:44:00|                         1|     30.0|      3|\n",
            "|261285|       0.0|                         1|     2014-04-28 02:53:11|                         1|     30.0|      3|\n",
            "|265265|       0.0|                         1|     2015-10-20 18:55:38|                         1|     30.0|     11|\n",
            "|281332|       0.0|                         1|     2015-11-07 12:25:36|                         1|     30.0|     24|\n",
            "|287497|       0.0|                         1|     2016-07-18 05:19:47|                         1|     30.0|     13|\n",
            "|293958|       0.0|                         1|     2015-11-22 03:54:08|                         1|     30.0|      9|\n",
            "|313680|       0.0|                         1|     2016-03-18 13:23:39|                         1|     30.0|     13|\n",
            "|335634|       0.0|                         1|     2015-10-18 14:20:28|                         1|     30.0|     13|\n",
            "|337862|       0.0|                         1|     2014-08-10 04:24:17|                         1|     30.0|     21|\n",
            "+------+----------+--------------------------+------------------------+--------------------------+---------+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LKFM9nPwgnM",
        "colab_type": "text"
      },
      "source": [
        "## Predicting user's total transactions from RF model\n",
        "For each user’s lifetime point, regress recency and frequency on Y. Plot the MSE for each lifetime point. In other words, your x-axis will be lifetime in months (0-12), and your y-axis will be the MSE. ( Hint : Don’t forget to split your data into train and test sets)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jR-Yak6PUNKb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "modeldata=recency_frequency_tp.join(user_dynamic_spend, on=['user1','timeperiod'], how='inner')\n",
        "modeldata_all=modeldata.select( 'user1','timeperiod','Y_totatransactions_12month','frequency','recency','Food','Event','Travel','Activity','Utility','Cash','Illegal/Sarcasm','Unclassified','People','Transportation')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvH2aoNZkIC_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "modeldata=modeldata_all.select('Y_totatransactions_12month','frequency','recency','timeperiod').limit(10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3AdjPUnUwZP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "modeldata_t0=modeldata.filter(modeldata.timeperiod==0)\n",
        "modeldata_t1=modeldata.filter(modeldata.timeperiod==1)\n",
        "modeldata_t2=modeldata.filter(modeldata.timeperiod==2)\n",
        "modeldata_t3=modeldata.filter(modeldata.timeperiod==3)\n",
        "modeldata_t4=modeldata.filter(modeldata.timeperiod==4)\n",
        "modeldata_t5=modeldata.filter(modeldata.timeperiod==5)\n",
        "modeldata_t6=modeldata.filter(modeldata.timeperiod==6)\n",
        "modeldata_t7=modeldata.filter(modeldata.timeperiod==7)\n",
        "modeldata_t8=modeldata.filter(modeldata.timeperiod==8)\n",
        "modeldata_t9=modeldata.filter(modeldata.timeperiod==9)\n",
        "modeldata_t10=modeldata.filter(modeldata.timeperiod==10)\n",
        "modeldata_t11=modeldata.filter(modeldata.timeperiod==11)\n",
        "modeldata_t12=modeldata.filter(modeldata.timeperiod==12)\n",
        "models_lifetime=[modeldata_t0,modeldata_t1,modeldata_t2,modeldata_t3,modeldata_t4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRku0IMUptX3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"model_timeperiod\",  IntegerType(), True), StructField(\"MSE\", IntegerType(), True)])\n",
        "model_MSE=spark.createDataFrame([], schema)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPM91Fm1mYNn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "def lin_reg_time(model_data,model_time):\n",
        "  model=model_data\n",
        "  model_timeperiod = model_time\n",
        "  model_data=model.select('recency','frequency','Y_totatransactions_12month')\n",
        "  vectorAssembler = VectorAssembler(inputCols = ['recency','frequency'], outputCol = 'features')\n",
        "  model_vector =  vectorAssembler.transform(model_data)\n",
        "  model_vector=model_vector.select(['features', 'Y_totatransactions_12month'])\n",
        "  splits = model_vector.randomSplit([0.7, 0.3])\n",
        "  train_df = splits[0]\n",
        "  test_df = splits[1]\n",
        "  lr = LinearRegression(featuresCol = 'features', labelCol='Y_totatransactions_12month')\n",
        "  lr_model = lr.fit(train_df)\n",
        "  test_result = lr_model.evaluate(test_df)\n",
        "  print(\"Root Mean Squared Error (RMSE) on test data = %g\" % test_result.rootMeanSquaredError)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAyVzaZZztSI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lin_reg_time(modeldata_t0,0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uX8vAbt5zuG9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lin_reg_time(modeldata_t1,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fp8lyV-9zuOq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lin_reg_time(modeldata_t2,2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5ZQ3fm6zuUX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lin_reg_time(modeldata_t3,3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "En3lbcnczuYJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lin_reg_time(modeldata_t4,4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiQvU3Zxzug3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lin_reg_time(modeldata_t5,5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50E3-KtOzulA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lin_reg_time(modeldata_t6,6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsHuQ-WpzueB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lin_reg_time(modeldata_t7,7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4O2OSzuzub-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lin_reg_time(modeldata_t8,8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-J1YeZsTz8E_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lin_reg_time(modeldata_t9,9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXI8T8A2z9uq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lin_reg_time(modeldata_t10,10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCZ4wJ-5z_IB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lin_reg_time(modeldata_t11,11)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrZbtljP0Afg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lin_reg_time(modeldata_t12,12)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuW_XjhwmGLa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mse(x,y):\n",
        "  plt.plot(x,y)\n",
        "  plt.xlabel(\"Timeperiod\")\n",
        "  plt.ylabel(\"MSE\")\n",
        "  plt.title(\"MSE across timeperiods\")\n",
        "  plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxQwt_JLHAH6",
        "colab_type": "text"
      },
      "source": [
        "## Predicting user's total transactions from Recency,Frequency & Spending profile\n",
        "For each user’s lifetime point, regress recency, frequency AND her spending behavior profile on Y. Plot the MSE for each lifetime point like above. Did you get any improvement?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TnkqIuPVc5q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "bd1609f6-d80f-4129-911c-37d33f4d2b08"
      },
      "source": [
        "modeldata=modeldata_all.select('timeperiod','Y_totatransactions_12month','frequency','recency','Food','Event','Travel','Activity','Utility','Cash','Illegal/Sarcasm','Unclassified','People','Transportation')\n",
        "modeldata_t0=modeldata.filter(modeldata.timeperiod==0)\n",
        "modeldata_t1=modeldata.filter(modeldata.timeperiod==1)\n",
        "modeldata_t2=modeldata.filter(modeldata.timeperiod==2)\n",
        "modeldata_t3=modeldata.filter(modeldata.timeperiod==3)\n",
        "modeldata_t4=modeldata.filter(modeldata.timeperiod==4)\n",
        "modeldata_t5=modeldata.filter(modeldata.timeperiod==5)\n",
        "modeldata_t6=modeldata.filter(modeldata.timeperiod==6)\n",
        "modeldata_t7=modeldata.filter(modeldata.timeperiod==7)\n",
        "modeldata_t8=modeldata.filter(modeldata.timeperiod==8)\n",
        "modeldata_t9=modeldata.filter(modeldata.timeperiod==9)\n",
        "modeldata_t10=modeldata.filter(modeldata.timeperiod==10)\n",
        "modeldata_t11=modeldata.filter(modeldata.timeperiod==11)\n",
        "modeldata_t12=modeldata.filter(modeldata.timeperiod==12)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-0ad4d0a7c5ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodeldata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodeldata_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'timeperiod'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Y_totatransactions_12month'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'frequency'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'recency'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Food'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Event'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Travel'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Activity'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Utility'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Cash'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Illegal/Sarcasm'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Unclassified'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'People'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Transportation'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodeldata_t0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodeldata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodeldata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeperiod\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodeldata_t1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodeldata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodeldata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeperiod\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodeldata_t2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodeldata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodeldata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeperiod\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodeldata_t3\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodeldata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodeldata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeperiod\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'modeldata_all' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWwwQL3A7E_r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "def lin_reg_time(model_data):\n",
        "  model=model_data\n",
        "  model_data=model.select('Y_totatransactions_12month','frequency','recency','Food','Event','Travel','Activity','Utility','Cash','Illegal/Sarcasm','Unclassified','People','Transportation')\n",
        "  vectorAssembler = VectorAssembler(inputCols = ['frequency','recency','Food','Event','Travel','Activity','Utility','Cash','Illegal/Sarcasm','Unclassified','People','Transportation'], outputCol = 'features')\n",
        "  model_vector =  vectorAssembler.transform(model_data)\n",
        "  model_vector=model_vector.select(['features', 'Y_totatransactions_12month'])\n",
        "  splits = model_vector.randomSplit([0.7, 0.3])\n",
        "  train_df = splits[0]\n",
        "  test_df = splits[1]\n",
        "  lr = LinearRegression(featuresCol = 'features', labelCol='Y_totatransactions_12month')\n",
        "  lr_model = lr.fit(train_df)\n",
        "  test_result = lr_model.evaluate(test_df)\n",
        " \n",
        "  print(\"Root Mean Squared Error (RMSE) on test data = %g\" % test_result.rootMeanSquaredError)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gc2VCFht7IEh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lin_reg_time(modeldata_t0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VaTrE4n7Las",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lin_reg_time(modeldata_t1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekY-XNVZ7MKV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lin_reg_time(modeldata_t2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArD1uzCG7MqY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lin_reg_time(modeldata_t3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhozFX2V7NDG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lin_reg_time(modeldata_t4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QW5yhSbX7NgB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lin_reg_time(modeldata_t5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1Wqg1417SZR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lin_reg_time(modeldata_t6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCA-JEBW7T0e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lin_reg_time(modeldata_t7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mInyz4zY7VOX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lin_reg_time(modeldata_t8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79bTHnog7WnO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lin_reg_time(modeldata_t9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yr8qxEf-7X16",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lin_reg_time(modeldata_t10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4z2PTa4p7ZH1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lin_reg_time(modeldata_t11)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_uFPQ6P7aUx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lin_reg_time(modeldata_t12)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VygX2oiHBgq",
        "colab_type": "text"
      },
      "source": [
        "## Predicting user's total transactions from user's Social Network metrics\n",
        "For each user’s lifetime point, regress her social network metrics on Y. Plot the MSE for each lifetime point like above. What do you observe? How do social network metrics compare with the RF framework? What are the most informative predictors?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OryJkWhGHC1e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = spark.sql(\"select user,timperiod,avg(degree) as degree_metric from network group by user,timperiod\")\n",
        "dict_path='/content/drive/My Drive/Big Data/Venmo Project/modeldata_Venmo.csv'\n",
        "modeldata = spark\\\n",
        "                  .read\\\n",
        "                  .option('header','true')\\\n",
        "                  .option('inferSchema','true')\\\n",
        "                  .csv(dict_path)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAyujx9PV6YY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = rename_columns(a,{'user':'user1'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brBwBMeOHECf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "outputId": "00ffe924-d62b-4d53-f83b-cd737c7fdb6e"
      },
      "source": [
        "modeldata.show()\n",
        "model_pd = modeldata.toPandas()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+------+----------+--------------------------+---------+-------+-----+-----+------+--------+-------+----+---------------+------------+------+--------------+\n",
            "|_c0| user1|timeperiod|Y_totatransactions_12month|frequency|recency| Food|Event|Travel|Activity|Utility|Cash|Illegal/Sarcasm|Unclassified|People|Transportation|\n",
            "+---+------+----------+--------------------------+---------+-------+-----+-----+------+--------+-------+----+---------------+------------+------+--------------+\n",
            "|  0| 65457|       0.0|                         1|     30.0|     28|  0.0|  0.0|   0.0|     0.0|    0.0| 0.0|            0.0|         0.0| 100.0|           0.0|\n",
            "|  1|118859|       0.0|                         1|     30.0|     13|  0.0|  0.0|   0.0|     0.0|    0.0| 0.0|            0.0|       100.0|   0.0|           0.0|\n",
            "|  2|121101|       0.0|                         1|     30.0|     21|100.0|  0.0|   0.0|     0.0|    0.0| 0.0|            0.0|         0.0|   0.0|           0.0|\n",
            "|  3|139055|       0.0|                         1|     30.0|      1|  0.0|  0.0|   0.0|     0.0|    0.0| 0.0|            0.0|       100.0|   0.0|           0.0|\n",
            "|  4|141759|       0.0|                         1|     30.0|     12|  0.0|  0.0|   0.0|     0.0|    0.0| 0.0|            0.0|       100.0|   0.0|           0.0|\n",
            "|  5|159959|       0.0|                         1|     30.0|     25|  0.0|  0.0|   0.0|     0.0|    0.0| 0.0|            0.0|       100.0|   0.0|           0.0|\n",
            "|  6|183431|       0.0|                         1|     30.0|      4|  0.0|  0.0|   0.0|     0.0|    0.0| 0.0|            0.0|         0.0| 100.0|           0.0|\n",
            "|  7|191035|       0.0|                         1|     30.0|     14|  0.0|  0.0|   0.0|     0.0|  100.0| 0.0|            0.0|         0.0|   0.0|           0.0|\n",
            "|  8|196536|       0.0|                         1|     30.0|      7|  0.0|  0.0|   0.0|     0.0|    0.0| 0.0|            0.0|         0.0| 100.0|           0.0|\n",
            "|  9|198329|       0.0|                         1|     30.0|     17|100.0|  0.0|   0.0|     0.0|    0.0| 0.0|            0.0|         0.0|   0.0|           0.0|\n",
            "| 10|199693|       8.0|                         2|     30.0|     18|  0.0|  0.0|   0.0|     0.0|    0.0| 0.0|            0.0|       100.0|   0.0|           0.0|\n",
            "| 11|201979|       0.0|                         1|     30.0|     12|  0.0|  0.0|   0.0|     0.0|    0.0| 0.0|            0.0|       100.0|   0.0|           0.0|\n",
            "| 12|204190|       0.0|                         1|     30.0|      7|  0.0|  0.0|   0.0|     0.0|    0.0| 0.0|            0.0|         0.0|   0.0|         100.0|\n",
            "| 13|205337|       0.0|                         1|     30.0|     28|100.0|  0.0|   0.0|     0.0|    0.0| 0.0|            0.0|         0.0|   0.0|           0.0|\n",
            "| 14|223103|       0.0|                         1|     30.0|     13|  0.0|  0.0|   0.0|     0.0|  100.0| 0.0|            0.0|         0.0|   0.0|           0.0|\n",
            "| 15|240103|       0.0|                         1|     30.0|     17|  0.0|  0.0|   0.0|     0.0|    0.0| 0.0|            0.0|       100.0|   0.0|           0.0|\n",
            "| 16|264048|       0.0|                         1|     30.0|      9|  0.0|  0.0|   0.0|     0.0|  100.0| 0.0|            0.0|         0.0|   0.0|           0.0|\n",
            "| 17|269351|       0.0|                         1|     30.0|     11|100.0|  0.0|   0.0|     0.0|    0.0| 0.0|            0.0|         0.0|   0.0|           0.0|\n",
            "| 18|279789|       0.0|                         1|     30.0|     13|  0.0|  0.0|   0.0|   100.0|    0.0| 0.0|            0.0|         0.0|   0.0|           0.0|\n",
            "| 19|280405|       0.0|                         1|     30.0|      0|  0.0|  0.0|   0.0|     0.0|    0.0| 0.0|            0.0|       100.0|   0.0|           0.0|\n",
            "+---+------+----------+--------------------------+---------+-------+-----+-----+------+--------+-------+----+---------------+------------+------+--------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFZKJELViMU-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "outputId": "4367df4c-f44e-44e0-c624-37289180d410"
      },
      "source": [
        "model_pd.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>_c0</th>\n",
              "      <th>user1</th>\n",
              "      <th>timeperiod</th>\n",
              "      <th>Y_totatransactions_12month</th>\n",
              "      <th>frequency</th>\n",
              "      <th>recency</th>\n",
              "      <th>Food</th>\n",
              "      <th>Event</th>\n",
              "      <th>Travel</th>\n",
              "      <th>Activity</th>\n",
              "      <th>Utility</th>\n",
              "      <th>Cash</th>\n",
              "      <th>Illegal/Sarcasm</th>\n",
              "      <th>Unclassified</th>\n",
              "      <th>People</th>\n",
              "      <th>Transportation</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>65457</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>30.0</td>\n",
              "      <td>28</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>118859</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>30.0</td>\n",
              "      <td>13</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>121101</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>30.0</td>\n",
              "      <td>21</td>\n",
              "      <td>100.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>139055</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>30.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>141759</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>30.0</td>\n",
              "      <td>12</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   _c0   user1  timeperiod  ...  Unclassified  People  Transportation\n",
              "0    0   65457         0.0  ...           0.0   100.0             0.0\n",
              "1    1  118859         0.0  ...         100.0     0.0             0.0\n",
              "2    2  121101         0.0  ...           0.0     0.0             0.0\n",
              "3    3  139055         0.0  ...         100.0     0.0             0.0\n",
              "4    4  141759         0.0  ...         100.0     0.0             0.0\n",
              "\n",
              "[5 rows x 16 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1IonpoXFTvq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model11 = model11.rename(columns={\"timeperiod\": \"timperiod\"})\n",
        "\n",
        "modeldata = modeldata.rename(columns={\"timeperiod\": \"timperiod\"})\n",
        "ds_full = pd.merge(modeldata, model11, on=['user1','timperiod'], how='left')[[\"user1\",'timperiod','degree_metric','Y_totatransactions_12month']]\n",
        "ds1  = ds_full[ds_full['timperiod']==0]\n",
        "ds2  = ds_full[ds_full['timperiod']==2]\n",
        "ds3  = ds_full[ds_full['timperiod']==3]\n",
        "ds4  = ds_full[ds_full['timperiod']==4]\n",
        "ds5  = ds_full[ds_full['timperiod']==5]\n",
        "ds6  = ds_full[ds_full['timperiod']==6]\n",
        "ds7  = ds_full[ds_full['timperiod']==7]\n",
        "ds8  = ds_full[ds_full['timperiod']==8]\n",
        "ds9  = ds_full[ds_full['timperiod']==9]\n",
        "ds10  = ds_full[ds_full['timperiod']==10]\n",
        "ds11  = ds_full[ds_full['timperiod']==11]\n",
        "ds12  = ds_full[ds_full['timperiod']==12]\n",
        "ds =[ds1,ds2,ds3,ds4,ds5,ds6,ds7,ds8,ds9,ds10,ds11,ds12]\n",
        "model_timeperiod=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97LMx1hZmlPq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#running regression\n",
        "mse = []\n",
        "ts = 0\n",
        "for ds_t in ds:\n",
        "  x = ds1[['user1','timperiod','degree_metric']]\n",
        "  y = ds1[['Y_totatransactions_12month']]    \n",
        "  X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=0)\n",
        "  regressor = LinearRegression()  \n",
        "  regressor.fit(X_train, y_train) #training the algorithm\n",
        "  y_pred = regressor.predict(X_test)\n",
        "  mse[ts] = metrics.mean_squared_error(y_test, y_pred)\n",
        "  ts = ts + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkrzzHz9HF6F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "0e8ea5c5-200d-4899-92e2-a4ff013c69de"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(timeperiod,mse.values,data = mse)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fc86d9163c8>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RSRLw7t3wbd",
        "colab_type": "text"
      },
      "source": [
        "##  Predicting user's total transactions from Spending profile & Social Network metrics\n",
        "For each user’s lifetime point, regress her social network metrics and the\n",
        "spending behavior of her social network on Y. Plot the MSE for each lifetime point like\n",
        "above. Does the spending behavior of her social network add any predictive benefit\n",
        "compared to Q10?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqORWJce87Y2",
        "colab_type": "text"
      },
      "source": [
        "For this part we are just cosnidering - \n",
        "\n",
        "1. Degree as the social network metric\n",
        "2. direct 'Friend' as the social network entity\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxlV-paaQq7T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#in python\n",
        "modeldata = pd.read_csv(\"modeldata_Venmo.csv\")\n",
        "modeldata['timeperiod'] = modeldata['timeperiod'].apply(np.int64)\n",
        "model11 = pd.read_csv(\"model_q11.csv\")\n",
        "ds_full = pd.merge(modeldata, model11, on=['user1','timeperiod'], how='inner')\n",
        "ds_full.rename(column = {'user2' = 'user1'})   #functioned define\n",
        "\n",
        "nw_spend = pd.merge(ds_full, recency_frequency_tp, on=['user','timeperiod'], how='inner')\n",
        "#timeperiod datasets\n",
        "nw_spend1  = nw_spend[nw_spend['timperiod']==0]\n",
        "nw_spend2  = nw_spend[nw_spend['timperiod']==2]\n",
        "nw_spend3  = nw_spend[nw_spend['timperiod']==3]\n",
        "nw_spend4  = nw_spend[nw_spend['timperiod']==4]\n",
        "nw_spend5  = nw_spend[nw_spend['timperiod']==5]\n",
        "nw_spend6  = nw_spend[nw_spend['timperiod']==6]\n",
        "nw_spend7  = nw_spend[nw_spend['timperiod']==7]\n",
        "nw_spend8  = nw_spend[nw_spend['timperiod']==8]\n",
        "nw_spend9  = nw_spend[nw_spend['timperiod']==9]\n",
        "nw_spend10  = nw_spend[nw_spend['timperiod']==10]\n",
        "nw_spend11  = nw_spend[nw_spend['timperiod']==11]\n",
        "nw_spend12  = nw_spend[nw_spend['timperiod']==12]\n",
        "\n",
        "nw_spend_all = [nw_spend1  ,\tnw_spend2 , \tnw_spend3  ,\tnw_spend4  ,\tnw_spend5 , \tnw_spend6 , \tnw_spend7 , \tnw_spend8  ,\tnw_spend9  \tnw_spend10  ,\tnw_spend11 , \tnw_spend12  \n",
        "]\n",
        "mse = []\n",
        "ts = 0\n",
        "for nw in nw_spend_all:\n",
        "  x = nw_spend_all[['degree_metric','frequency','recency','Food','Event','Travel','Activity','Utility','Cash','Illegal/Sarcasm','Unclassified','People','Transportation']]\n",
        "  y = nw_spend_all[['Y_totatransactions_12month']]    \n",
        "  X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=0)\n",
        "  regressor = LinearRegression()  \n",
        "  regressor.fit(X_train, y_train) #training the algorithm\n",
        "  y_pred = regressor.predict(X_test)\n",
        "  mse[ts] = metrics.mean_squared_error(y_test, y_pred)\n",
        "  ts = ts + 1\n",
        "  plt.plot(timeperiod,mse.values,data = mse)\n",
        "\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}